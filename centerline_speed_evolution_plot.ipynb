{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb5505a",
   "metadata": {},
   "source": [
    "# Grab glacier centerline speeds and the plot the speed evolution time series\n",
    "\n",
    "_Last modified by jukesliu@u.boisestate.edu on 2022-05-02._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78ea77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet # pip install ordered-set\n",
    "import cmocean\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LogNorm\n",
    "import datetime\n",
    "\n",
    "from additional_functions import mytomd, unique_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a4fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/' # SET WORKING DIRECTORY\n",
    "# the root folder that holds centerline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017297ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6621358210688768, 1.1809038467853512, 1.96913473157004, 2.6019051815086787, 3.4028123782370128, 3.968339390980005, 4.487107416696439, 5.059990864284815, 5.800090100751969, 6.280106596531697, 6.764581908855604, 7.502383884003746, 8.585703615784784, 9.277921319256537, 9.961512766630737, 10.532482627862397, 11.133169686797553, 12.659516601584729, 13.58041624731048, 14.51964045447002, 15.117263301936958, 15.965774636580152, 17.173794530096174, 17.655295102578453, 18.200551279194777, 18.8973339580466, 19.35440784028735, 20.06910451852252, 20.97977811303844, 21.561178724745403, 22.211724307818333, 23.04869915438657, 23.8292214713847, 24.90490075727111, 25.919781117759506, 26.71240617924131, 27.34837656823172, 28.03341304380494, 28.751095878644183, 29.54666033803075]\n"
     ]
    }
   ],
   "source": [
    "# grab reference centerline distances from a data file:\n",
    "sorted_df = pd.read_csv(basepath+'ASF_autoRIFT/centerline_data_n.csv', # PATH TO THE SAR CSV\n",
    "                    usecols=[2,3,4,5,6,7,8,9,10,11]) # may need to adjust column\n",
    "dists = list(OrderedSet(sorted_df.dist_km))[1:]\n",
    "dists.sort()\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca487753",
   "metadata": {},
   "source": [
    "# 1A) Read in individual centerline profiles and combine\n",
    "\n",
    "When speeds along centerline are stored in a folder containing a CSV file with the data for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e41de95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "      <th>dist_km</th>\n",
       "      <th>vmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>0.662136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>1.180904</td>\n",
       "      <td>21.669880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>1.969135</td>\n",
       "      <td>19.900368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>2.601905</td>\n",
       "      <td>22.848957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>3.402812</td>\n",
       "      <td>19.935375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ds1        ds2   dist_km       vmag\n",
       "0 2013-06-10 2013-06-17  0.662136   0.000000\n",
       "1 2013-06-10 2013-06-17  1.180904  21.669880\n",
       "2 2013-06-10 2013-06-17  1.969135  19.900368\n",
       "3 2013-06-10 2013-06-17  2.601905  22.848957\n",
       "4 2013-06-10 2013-06-17  3.402812  19.935375"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfilespath = basepath+'/centerline_data_figure/' # ENTER PATH TO THE FOLDER CONTAINING THE PROFILES\n",
    "\n",
    "interp_dfs = []\n",
    "for file in os.listdir(cfilespath):\n",
    "    if file.startswith('profile') and file.endswith('.csv') and 'S' not in file:\n",
    "        ds1 = file[8:16]; ds2 = file[17:25]\n",
    "        profile_df = pd.read_csv(cfilespath+file, names=['dist_km','vmag_md']) # read in data\n",
    "        \n",
    "        # correct distance values if not correct\n",
    "        maxdist = np.nanmax(profile_df.dist_km) # grab the maximum distance value \n",
    "        if  maxdist < 29.54666: # distance should go out to 29.54 km\n",
    "            newdistkm = np.flip(np.array(profile_df.dist_km * 29.54666/maxdist)) # rescale to 29.54 km\n",
    "            profile_df.dist_km = newdistkm # replace in dataframe\n",
    "        \n",
    "#         downsample to SAR dist values\n",
    "        sample_indices = np.round(np.linspace(0,len(profile_df)-1,len(dists))) # grab sampling indexes for dists\n",
    "        distances = dists\n",
    "#         sample_indices = np.round(np.linspace(0,len(profile_df)-1,63)) # grab full profile\n",
    "#         distances = np.flip(profile_df.dist_km[sample_indices])\n",
    "        \n",
    "        v_interp = np.flip(profile_df.vmag_md[sample_indices]) # grab down-sampled spped values\n",
    "    \n",
    "        # fill in ds1 and ds2 columns\n",
    "        ds1s = np.full(np.size(v_interp),ds1) \n",
    "        ds2s = np.full(np.size(v_interp),ds2)\n",
    "\n",
    "        # enter into dataframe\n",
    "        interp_df = pd.DataFrame(list(zip(ds1s,ds2s,distances,v_interp)),columns=['ds1','ds2','dist_km','vmag'])\n",
    "        \n",
    "        # calculate datetimes\n",
    "        interp_df['ds1'] = pd.to_datetime(ds1s, format='%Y%m%d')\n",
    "        interp_df['ds2'] = pd.to_datetime(ds2s, format='%Y%m%d')\n",
    "        interp_dfs.append(interp_df)\n",
    "\n",
    "# enter into one dataframe\n",
    "interp_total = pd.concat(interp_dfs).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total = interp_total.drop_duplicates()\n",
    "interp_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f48921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds1</th>\n",
       "      <th>ds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-06-10</td>\n",
       "      <td>2013-06-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-06-17</td>\n",
       "      <td>2013-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>2013-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-28</td>\n",
       "      <td>2013-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-08-13</td>\n",
       "      <td>2013-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2021-08-15</td>\n",
       "      <td>2021-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>2021-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2021-09-07</td>\n",
       "      <td>2021-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2021-09-19</td>\n",
       "      <td>2021-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2021-09-22</td>\n",
       "      <td>2021-10-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds1        ds2\n",
       "0   2013-06-10 2013-06-17\n",
       "1   2013-06-17 2013-07-12\n",
       "2   2013-07-12 2013-07-28\n",
       "3   2013-07-28 2013-08-13\n",
       "4   2013-08-13 2013-10-07\n",
       "..         ...        ...\n",
       "153 2021-08-15 2021-08-30\n",
       "154 2021-08-30 2021-09-07\n",
       "155 2021-09-07 2021-09-19\n",
       "156 2021-09-19 2021-09-22\n",
       "157 2021-09-22 2021-10-04\n",
       "\n",
       "[158 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the unique dates\n",
    "df2 = unique_date_df(interp_total,'ds1','ds2')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df765cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize speed grid\n",
    "speed_grid = np.zeros((len(df2), len(distances)))\n",
    "\n",
    "# fill in speed grid with distances\n",
    "rown = 0\n",
    "for idx, row in df2.iterrows():\n",
    "    # grab the dates\n",
    "    d1 = row.ds1; d2 = row.ds2\n",
    "    \n",
    "    # grab the part of the df matching those dates\n",
    "    date_df = interp_total[interp_total.ds1 == d1]\n",
    "    date_df = date_df[date_df.ds2 == d2]\n",
    "    date_df.reset_index(drop=True, inplace=True)\n",
    "    date_df = date_df.drop_duplicates(subset=['dist_km'],keep='last') # drop duplications\n",
    "    \n",
    "    # append into row of speed grid\n",
    "    speed_grid[rown,:] = list(date_df.vmag) # add speed along centerline to speed_grid\n",
    "    rown += 1\n",
    "print(speed_grid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3c14e",
   "metadata": {},
   "source": [
    "### Fill in temporal gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa7433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_ends = np.array(list((zip(df2.ds1, df2.ds2)))).flatten() # intersperse ds1 and ds2\n",
    "\n",
    "fill_dfs = []\n",
    "counter = 0\n",
    "# identify temporal gaps\n",
    "for i in np.arange(0, len(date_ends),2):\n",
    "    if i+2 < len(date_ends):\n",
    "        # grab the two dates for that speed_grid()\n",
    "        date1_start = date_ends[i]\n",
    "        date1_end = date_ends[i+1]\n",
    "        date2_start = date_ends[i+2]\n",
    "        print(str(date1_start)[0:10],str(date1_end)[0:10]) # print the date start and end\n",
    "    \n",
    "        if not date1_end == date2_start: # if the end date and the next start date don't match\n",
    "            print()\n",
    "            print('Gap between', date1_end, 'and', date2_start) # Gap found\n",
    "            print()\n",
    "\n",
    "            # OPTION 1: fill in all gaps gap with Nans\n",
    "            ds1s = np.full(np.size(distances),date1_end) \n",
    "            ds2s = np.full(np.size(distances),date2_start)\n",
    "            nans = np.empty(len(distances)); nans[:] = np.nan # create list of nans to fill\n",
    "            fill_df = pd.DataFrame(list(zip(ds1s, ds2s,distances,nans)),columns=interp_total.columns)\n",
    "            fill_dfs.append(fill_df)\n",
    "            \n",
    "#             # OPTION 2: fill in the gaps with SAR data\n",
    "#             sorted_df = pd.read_csv(basepath+'ASF_autoRIFT/centerline_data_n.csv', # PATH TO THE SAR CSV\n",
    "#                                     usecols=[2,3,4,5,6,7,8,9,10,11]) # may need to adjust column\n",
    "#             # convert dates to datetime objects using pd\n",
    "#             sorted_df['mid_date'] = pd.to_datetime(sorted_df.mid_date, format='%Y%m%d')\n",
    "#             sorted_df['ds1'] = pd.to_datetime(sorted_df.ds1, format='%Y-%m-%d')\n",
    "#             sorted_df['ds2'] = pd.to_datetime(sorted_df.ds2, format='%Y-%m-%d')\n",
    "#             sorted_df['vmag'] = mytomd(sorted_df.vmag) # convert velocities to m/day\n",
    "#             sorted_df['v_error'] = mytomd(sorted_df.v_error)\n",
    "#             sorted_df = sorted_df.drop_duplicates(subset=['ds1','lat'],keep='first') # drop overlapping dates\n",
    "\n",
    "#             # grab each unique date pair\n",
    "#             dates_df = unique_date_df(sorted_df,'ds1','ds2')\n",
    "#             gap_df = dates_df[(dates_df.ds1 >= date1_end) & (dates_df.ds2 <= date2_start)] # find those in gap\n",
    "#             if len(gap_df) > 0: # fill in gaps with SAR data\n",
    "#                 print('Fill with:')\n",
    "#                 for idx, row in gap_df.iterrows():\n",
    "#                     print(row.ds1, row.ds2)\n",
    "#                     df = sorted_df[(sorted_df.ds1 == row.ds1) & (sorted_df.ds2 == row.ds2)]\n",
    "#                     fill_df = df[[\"ds1\", \"ds2\",\"dist_km\", \"vmag\"]]\n",
    "#                     fill_dfs.append(fill_df)\n",
    "#             else: # if no optical data found, fill in with nans\n",
    "#                 print(\"Fill with nans\")\n",
    "#                 ds1s = np.full(np.size(dists),date1_end) \n",
    "#                 ds2s = np.full(np.size(dists),date2_start)\n",
    "#                 nans = np.empty(len(dists)); nans[:] = np.nan # create list of nans to fill\n",
    "#                 fill_df = pd.DataFrame(list(zip(ds1s, ds2s, dists,nans)),columns=interp_total.columns)\n",
    "#                 fill_dfs.append(fill_df)\n",
    "                \n",
    "                \n",
    "            counter+=1  # count the gaps      \n",
    "print(counter, 'gaps')\n",
    "\n",
    "# add gap-filling data back into the dataframe\n",
    "interp_total_filled = pd.concat([interp_total,pd.concat(fill_dfs)]).sort_values(by=['ds1','ds2','dist_km'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e08c8",
   "metadata": {},
   "source": [
    "# 1B) Read in combined centerline data file\n",
    "\n",
    "When speeds along centerline for all dates have been combined into one large file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvpath = basepath+'ASF_autoRIFT/centerline_data_n.csv' # ENTER PATH TO THE CSV FILE WITH ALL DATA COMBINED\n",
    "csvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/its_live/notebooks/'\n",
    "csvpath += 'ITS_LIVE_vx_vy_optical_centerline_n.csv'\n",
    "\n",
    "# read into a dataframe\n",
    "sorted_df = pd.read_csv(csvpath, usecols=[1,2,3,4,6,7,8,9,12,13]) # ADJUST COLUMNS AS NEEDED, DROP MID_DATE\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to datetime objects using pd\n",
    "sorted_df['ds1'] = pd.to_datetime(sorted_df.ds1, format='%Y-%m-%d')\n",
    "sorted_df['ds2'] = pd.to_datetime(sorted_df.ds2, format='%Y-%m-%d')\n",
    "\n",
    "# convert velocities to m/day\n",
    "sorted_df['vmag'] = mytomd(sorted_df.vmag)\n",
    "sorted_df['v_error'] = mytomd(sorted_df.v_error)\n",
    "\n",
    "# drop overlapping dates!\n",
    "sorted_df = sorted_df.drop_duplicates(subset=['ds1','lat'],keep='first') \n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab each unique date pair\n",
    "df2 = unique_date_df(sorted_df,'ds1','ds2')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize speed grid\n",
    "speed_grid = np.zeros((len(df2), len(dists)))\n",
    "print(speed_grid.shape)\n",
    "\n",
    "# fill in speed grid with speed values for each date pair\n",
    "rown = 0\n",
    "for idx, row in df2.iterrows():\n",
    "    # grab the dates\n",
    "    d1 = row.ds1; d2 = row.ds2\n",
    "    \n",
    "    # grab the part of the df matching those dates\n",
    "    date_df = sorted_df[sorted_df.ds1 == d1]\n",
    "    date_df = date_df[date_df.ds2 == d2]\n",
    "    date_df.reset_index(drop=True, inplace=True)\n",
    "    date_df = date_df.drop_duplicates(subset='dist_km',keep='first')\n",
    "\n",
    "    # append into row of speed grid\n",
    "    speed_grid[rown,:] = list(date_df.vmag) # add speed along centerline to speed_grid   \n",
    "    rown += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92cdec6",
   "metadata": {},
   "source": [
    "### Fill in temporal gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaffc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# intersperse ds1 and ds2\n",
    "date_ends = np.array(list((zip(df2.ds1, df2.ds2)))).flatten()\n",
    "\n",
    "fill_dfs = []\n",
    "counter = 0\n",
    "# identify number of gaps in to fill in data\n",
    "for i in np.arange(0, len(date_ends),2):\n",
    "    if i+2 < len(date_ends):\n",
    "        # grab the two dates for that speed_grid()\n",
    "        date1_start = date_ends[i]\n",
    "        date1_end = date_ends[i+1]\n",
    "        date2_start = date_ends[i+2]\n",
    "        print(str(date1_start)[0:10],str(date1_end)[0:10]) # print the date start and end\n",
    "        \n",
    "        if not date1_end == date2_start: # if the end date and the next start date don't match\n",
    "            print()\n",
    "            print('Gap between', str(date1_end)[0:10], 'and', str(date2_start)[0:10])\n",
    "            print()\n",
    "            \n",
    "#             # OPTION 1: fill in gaps with Nans\n",
    "#             ds1s = np.full(np.size(dists),date1_end) \n",
    "#             ds2s = np.full(np.size(dists),date2_start)\n",
    "#             nans = np.empty(len(dists)); nans[:] = np.nan # create list of nans to fill\n",
    "#             fill_df = pd.DataFrame(list(zip(nans, nans, nans, nans, nans, nans, dists, nans, ds1s, ds2s)),\n",
    "#                                    columns=sorted_df.columns)\n",
    "#             fill_dfs.append(fill_df)\n",
    "            \n",
    "            # OPTION 2: fill in gaps with optical data\n",
    "            for file in os.listdir(cfilespath): # path to optical data\n",
    "                if file.startswith('profile') and file.endswith('.csv') and 'S' not in file:\n",
    "                    dstr1 = file[8:16]; dstr2 = file[17:25]\n",
    "                    ds1 = pd.to_datetime(dstr1,format='%Y%m%d')\n",
    "                    ds2 = pd.to_datetime(dstr2,format='%Y%m%d')\n",
    "                    \n",
    "                    # find optical data between the gap dates\n",
    "                    if (ds1 >= date1_end) and (ds2 <= date2_start): \n",
    "                        print('Fill with optical data :',dstr1, dstr2)\n",
    "                        profile_df = pd.read_csv(cfilespath+file, names=['dist_km','vmag_md'])\n",
    "                        \n",
    "                        # down-sample the optical data\n",
    "                        maxdist = np.nanmax(profile_df.dist_km)\n",
    "                        if  maxdist < 29.54666: # distance should go out to 29.54 km\n",
    "                            newdistkm = np.flip(np.array(profile_df.dist_km * 29.54666/maxdist)) # rescale to 29.54 km\n",
    "                            profile_df.dist_km = newdistkm # replace in dataframe\n",
    "                        sample_indices = np.round(np.linspace(0,len(profile_df)-1,40))\n",
    "                        v_interp = np.flip(profile_df.vmag_md[sample_indices])\n",
    "\n",
    "                        # fill in a ds1 and ds2 columns\n",
    "                        ds1s = np.full(np.size(v_interp),ds1) \n",
    "                        ds2s = np.full(np.size(v_interp),ds2)\n",
    "                        nans = np.empty(np.size(v_interp)); nans[:] = np.nan # create list of nans to fill\n",
    "\n",
    "                        # enter into dataframe\n",
    "                        fill_df = pd.DataFrame(list(zip(nans, nans, nans, nans, v_interp, nans, dists, nans, ds1s, ds2s)),\n",
    "                                           columns=sorted_df.columns)\n",
    "                        fill_dfs.append(fill_df)\n",
    "            counter+=1\n",
    "print(counter, 'gaps')\n",
    "\n",
    "# return the filled df\n",
    "interp_total_filled = pd.concat([sorted_df,pd.concat(fill_dfs)]).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL ALL REMAINING GAPS IF OPTICAL DATA ARE INPUT\n",
    "df2 = pd.DataFrame(list(OrderedSet(zip(interp_total_filled.ds1, interp_total_filled.ds2))),\n",
    "                   columns=['ds1','ds2'])\n",
    "date_ends = np.array(list((zip(df2.ds1, df2.ds2)))).flatten()\n",
    "\n",
    "fill_dfs = []\n",
    "counter = 0\n",
    "# identify number of gaps in to fill in data\n",
    "for i in np.arange(0, len(date_ends),2):\n",
    "    if i+2 < len(date_ends):\n",
    "        # grab the two dates for that speed_grid()\n",
    "        date1_start = date_ends[i]\n",
    "        date1_end = date_ends[i+1]\n",
    "        date2_start = date_ends[i+2]\n",
    "        print(date1_start, date1_end)\n",
    "        \n",
    "        if not date1_end == date2_start: # if the end date and the next start date don't match\n",
    "            print()\n",
    "            print('Gap between', date1_end, 'and', date2_start)\n",
    "            print()\n",
    "            \n",
    "            # fill in a the gaps with nans\n",
    "            ds1s = np.full(np.size(dists),date1_end) \n",
    "            ds2s = np.full(np.size(dists),date2_start)\n",
    "            nans = np.empty(len(dists)); nans[:] = np.nan # create list of nans to fill\n",
    "            fill_df = pd.DataFrame(list(zip(nans, nans, nans, nans, nans, nans, dists, nans, ds1s, ds2s)),\n",
    "                                   columns=sorted_df.columns)\n",
    "            fill_dfs.append(fill_df)\n",
    "            counter+=1  \n",
    "print(counter, 'gaps')\n",
    "\n",
    "interp_total_filled = pd.concat([interp_total_filled,pd.concat(fill_dfs)]).sort_values(by=['ds1','ds2','dist_km'])\n",
    "interp_total_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259358b",
   "metadata": {},
   "source": [
    "## 2) Plot speed evolution time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab ordered dates as a dataeframe\n",
    "df2_filled = unique_date_df(interp_total_filled,'ds1','ds2')\n",
    "df2_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create speed grid with gap-filled data\n",
    "speed_grid_filled = np.zeros((len(df2_filled), len(distances)))\n",
    "\n",
    "# speed grid with gap-filled data\n",
    "rown = 0\n",
    "for idx, row in df2_filled.iterrows():\n",
    "    d1 = row.ds1; d2 = row.ds2 # grab the dates\n",
    "    \n",
    "    # grab the part of the df matching those dates\n",
    "    date_df = interp_total_filled[interp_total_filled.ds1 == d1]\n",
    "    date_df = date_df[date_df.ds2 == d2]\n",
    "    date_df.reset_index(drop=True, inplace=True)\n",
    "    date_df = date_df.drop_duplicates(subset=['dist_km'],keep='last') # drop duplications\n",
    "    \n",
    "    # append into row of speed grid\n",
    "    speed_grid_filled[rown,:] = list(date_df.vmag) # add speed along centerline to speed_grid\n",
    "    rown += 1\n",
    "print(speed_grid_filled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac292559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the grid spacing based on the time differences\n",
    "spacing = np.round(np.diff(df2_filled.ds1)/np.min(np.diff(df2_filled.ds1)))\n",
    "spacing = np.append(spacing, spacing[-1]) # add last spacing\n",
    "spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ee3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaced grid\n",
    "speed_spaced = np.repeat(speed_grid_filled,spacing.astype(int),0) # along rows\n",
    "# speed_spaced[speed_spaced == 0] = np.NaN\n",
    "speed_spaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ba98a",
   "metadata": {},
   "source": [
    "# Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64881955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input to properly generate axis labels for the plot:\n",
    "# if grid tick spacing is unknown, run the cell below with the tick labels turned off to see tick spacing\n",
    "\n",
    "# create centerline (x-axis) labels \n",
    "x = np.linspace(dists[0], dists[-1], speed_grid.shape[1]+1)\n",
    "x_labels = np.insert(x[::10],0,0).astype(int) # INPUT GRID TICK SPACING FOR X (e.g. 10 or 5)\n",
    "print(x_labels)\n",
    "\n",
    "# create y-axis labels\n",
    "t = pd.date_range(start='2013-06-10',end='2021-09-22',periods=len(speed_spaced)) # INPUT START AND END DATE\n",
    "y = np.insert(t[::500],0,t[0]) # INPUT GRID TICK SPACING (e.g. 250, 200, 500, or 20)\n",
    "y_labels = [ystr[:7] for ystr in y.astype(str)] # grab the first 7 digits YYYY-MM for each timestamp in y\n",
    "print(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfb52c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the properly-spaced speed evolution map with imshow\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "ax.set_facecolor('black')\n",
    "grid = plt.imshow(speed_spaced,aspect=0.03, # 0.03 for full 2013-2022\n",
    "                  cmap=cmocean.cm.thermal,\n",
    "#                  vmin=0, vmax=7) # linear coloring\n",
    "                  norm=LogNorm(vmin=1, vmax=25)) # log norm coloring\n",
    "fig.colorbar(grid, orientation=\"vertical\",label=\"Surface speed (m/day)\")\n",
    "\n",
    "# set tick labels\n",
    "ax.set_xticklabels(x_labels); plt.xlabel('Distance from terminus (km)')\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "# plt.savefig(basepath+\"figures/Optical_filled_with_SAR.png\",dpi=300) # save figure\n",
    "# plt.title('SAR and Optical') # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec319c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
