{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc750979",
   "metadata": {},
   "source": [
    "# Code to map velocities using autoRIFT with adjustable parameters\n",
    "\n",
    "_Last modified on 2022-09-07 by jukesliu@u.boisestate.edu._\n",
    "\n",
    "Majority of the code is borrowed from testGeogrid_ISCE.py and testautoRIFT_ISCE.py from the autoRIFT GitHub repository (https://github.com/nasa-jpl/autoRIFT/). Requires ISCE and autoRIFT to be installed.\n",
    "\n",
    "Change the kernel to __newautoriftenv__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2308ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "import isce\n",
    "from contrib.geo_autoRIFT.geogrid import Geogrid, GeogridOptical\n",
    "from iscesys.Component.ProductManager import ProductManager as PM\n",
    "from isceobj.Orbit.Orbit import Orbit\n",
    "from osgeo import gdal, osr\n",
    "import struct\n",
    "import re\n",
    "from datetime import date\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request\n",
    "from components.contrib.geo_autoRIFT.autoRIFT import autoRIFT_ISCE\n",
    "import isceobj\n",
    "import time\n",
    "import subprocess\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "\n",
    "from autorift_utilities import *\n",
    "\n",
    "gdal.AllRegister() # register all GDAL drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fb391",
   "metadata": {},
   "source": [
    "## 1) Resample DEM and other autoRIFT/geogrid input rasters to the desired chip size\n",
    "\n",
    "Recommended chip size is >= 16*pixel_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076bfa2",
   "metadata": {},
   "source": [
    "#### Customizable parameters for geogrid:\n",
    "\n",
    "    dhdx, dhdy:              x/y local surface slope maps (unitless)\n",
    "    vx,vy:                   x/y reference velocity maps (in units of m/yr)\n",
    "    srx, sry:                x/y velocity search range limit maps (in units of m/yr)\n",
    "    csminx, csminy:          x/y chip size minimum maps (in units of m; constant ratio between x and y)\n",
    "    csmaxx, csmaxy:          x/y chip size maximum maps (in units of m; constant ratio between x and y)\n",
    "    ssm:                     stable surface mask (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cfddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ENTER CHIP SIZE, DEM INFO, AND REFERENCE VELOCITY INFO ########### \n",
    "CHIPSIZE_M = 100 # enter in desired grid size in meters (default is 32 pixels)\n",
    "\n",
    "# enter in the path to your best DEM over the region\n",
    "dempath = '/Users/jukesliu/Documents/TURNER/DATA/ICE_THICKNESS/surface/DEMs_previous/'\n",
    "demname = 'IfSAR_5m_DSM_clipped.tif'\n",
    "\n",
    "# path to the reference files for geogrid (vx, vy, ssm)\n",
    "refvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/forAutoRIFT/' \n",
    "vx_fname = 'vx_cropped.tif' # name of reference vx file\n",
    "vy_fname = 'vy_cropped.tif' # name of reference vy file\n",
    "\n",
    "sr_scaling = 16 # multiply by vx and vy to generate search range limits\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5009da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and grab info from your DEM\n",
    "refdem = rio.open(dempath+demname) # open DEM using rasterio\n",
    "elev = refdem.read(1) # read in the first and only band (elevations)\n",
    "\n",
    "# grab the x and y grid values from the DEM:\n",
    "dem_x = np.linspace(refdem.bounds.left, refdem.bounds.right, num=np.shape(elev)[1])\n",
    "dem_y = np.linspace(refdem.bounds.top, refdem.bounds.bottom, num=np.shape(elev)[0])\n",
    "\n",
    "# grab the resampled x and y grid values from the DEM\n",
    "new_x = np.arange(refdem.bounds.left, refdem.bounds.right, CHIPSIZE_M)\n",
    "new_y = np.arange(refdem.bounds.top, refdem.bounds.bottom, -CHIPSIZE_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6318997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IfSAR_100m_DSM_clipped.tif  already exists.\n"
     ]
    }
   ],
   "source": [
    "dem_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(dempath+dem_outfile): # if the resampled DEM does not exist already\n",
    "    # Create thew new x and y grid values using DEM bounds and the chipsize\n",
    "    dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "    print(dem_resamp.shape)\n",
    "    \n",
    "    # Resample to your new DEM bounds\n",
    "    f = interp2d(dem_x, dem_y, elev) # create DEM interpolation object\n",
    "    dem_resamp = f(new_x,new_y) # resample the NIR data to the DSM coordinates\n",
    "    dem_resamp = np.flipud(dem_resamp) # flip up down\n",
    "    print(\"Resampled to new dimensions:\",dem_resamp.shape)\n",
    "    \n",
    "    # Display the two DEMs as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(elev, cmap='Greys_r', vmin=0)\n",
    "    ax1.set_title('Original DEM: '+str(refdem.transform[0])+' m') # original spatial resolution\n",
    "    fig.colorbar(im1, ax=ax1,label='Elevation [m]')\n",
    "\n",
    "    im2 = ax2.imshow(dem_resamp, cmap='Greys_r', vmin=0)\n",
    "    ax2.set_title('Resampled DEM: '+str(CHIPSIZE_M)+' m') # new spatial resolution\n",
    "    fig.colorbar(im2, ax=ax2,label='Elevation [m]')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the resampled DEM to georeferenced tif file\n",
    "    print(\"Save resampled DEM to\", dempath+dem_outfile)\n",
    "    rio_write(dempath+dem_outfile, dem_resamp, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    # load the empty grid\n",
    "    dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "    print(dem_outfile, ' already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f272d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling):\n",
    "    \n",
    "    # GRAB DEM INFO\n",
    "    refdem = rio.open(dempath+demname) # open DEM using rasterio\n",
    "    elev = refdem.read(1) # read in the first and only band (elevations)\n",
    "\n",
    "    # grab the x and y grid values from the DEM:\n",
    "    dem_x = np.linspace(refdem.bounds.left, refdem.bounds.right, num=np.shape(elev)[1])\n",
    "    dem_y = np.linspace(refdem.bounds.top, refdem.bounds.bottom, num=np.shape(elev)[0])\n",
    "\n",
    "    # grab the resampled x and y grid values from the DEM\n",
    "    new_x = np.arange(refdem.bounds.left, refdem.bounds.right, CHIPSIZE_M)\n",
    "    new_y = np.arange(refdem.bounds.top, refdem.bounds.bottom, -CHIPSIZE_M)\n",
    "    \n",
    "    # RESAMPLE THE DEM\n",
    "    dem_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped.tif' # generate new filename\n",
    "    if not os.path.exists(dempath+dem_outfile): # if the resampled DEM does not exist already\n",
    "        # Create thew new x and y grid values using DEM bounds and the chipsize\n",
    "        dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "        print(dem_resamp.shape)\n",
    "\n",
    "        # Resample to your new DEM bounds\n",
    "        f = interp2d(dem_x, dem_y, elev) # create DEM interpolation object\n",
    "        dem_resamp = f(new_x,new_y) # resample the NIR data to the DSM coordinates\n",
    "        dem_resamp = np.flipud(dem_resamp) # flip up down\n",
    "        print(\"Resampled to new dimensions:\",dem_resamp.shape)\n",
    "\n",
    "        # Display the two DEMs as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(elev, cmap='Greys_r', vmin=0)\n",
    "        ax1.set_title('Original DEM: '+str(refdem.transform[0])+' m') # original spatial resolution\n",
    "        fig.colorbar(im1, ax=ax1,label='Elevation [m]')\n",
    "\n",
    "        im2 = ax2.imshow(dem_resamp, cmap='Greys_r', vmin=0)\n",
    "        ax2.set_title('Resampled DEM: '+str(CHIPSIZE_M)+' m') # new spatial resolution\n",
    "        fig.colorbar(im2, ax=ax2,label='Elevation [m]')\n",
    "        plt.show()\n",
    "\n",
    "        # Save the resampled DEM to georeferenced tif file\n",
    "        print(\"Save resampled DEM to\", dempath+dem_outfile)\n",
    "        rio_write(dempath+dem_outfile, dem_resamp, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        # load the existing resampled DEM\n",
    "        dem_r = rio.open(dempath+dem_outfile) # open DEM using rasterio\n",
    "        dem_resamp = dem_r.read(1) # read in the first and only band (elevations)\n",
    "        print(dem_outfile, ' already exists.')\n",
    "    \n",
    "    # CREATE DHDX, DHDY\n",
    "    dhdx_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdx.tif' # generate new filename\n",
    "    dhdy_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdy.tif' # generate new filename\n",
    "    if not os.path.exists(dempath+dhdx_outfile) or not os.path.exists(dempath+dhdy_outfile): # if either is missing\n",
    "        # Produce dhdx and dhdy maps from resampled DEM\n",
    "        dhdx = np.gradient(dem_resamp, axis=1)/CHIPSIZE_M\n",
    "        dhdy = np.gradient(dem_resamp, axis=0)/CHIPSIZE_M\n",
    "\n",
    "        # Filter out borders with high gradient values\n",
    "        grad_thresh = 5\n",
    "        dhdx[abs(dhdx) > grad_thresh] = 0; dhdy[abs(dhdy) > grad_thresh] = 0\n",
    "\n",
    "        # absolute value of the max gradient values expected:\n",
    "        dhmax = 1\n",
    "\n",
    "        # Display the two DEMs as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(dhdx, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "        ax1.set_title('dhdx') # surface slope x\n",
    "        fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "        im2 = ax2.imshow(dhdy, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "        ax2.set_title('dhdy') # surface slope y\n",
    "        fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # Save the gradient maps to tif files\n",
    "        print(\"Save surface slope maps to\", dempath)\n",
    "        rio_write(dempath+dhdx_outfile, dhdx, refdem, CHIPSIZE_M) # dhdx\n",
    "        rio_write(dempath+dhdy_outfile, dhdy, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        print(dhdy_outfile, 'and', dhdx_outfile, 'already exist.')\n",
    "\n",
    "    # VX, VY, SRX, SRY\n",
    "    vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    if not os.path.exists(refvpath+vx_outfile) or not os.path.exists(refvpath+vy_outfile): # if either vx, vy missing\n",
    "        # open the files with rasterio\n",
    "        vx_reader = rio.open(refvpath+vx_fname); vx0 = vx_reader.read(1)\n",
    "        vy_reader = rio.open(refvpath+vy_fname); vy0 = vy_reader.read(1)\n",
    "        vx_x = np.linspace(vx_reader.bounds.left, vx_reader.bounds.right, num=np.shape(vx0)[1])\n",
    "        vx_y = np.linspace(vx_reader.bounds.top, vx_reader.bounds.bottom, num=np.shape(vx0)[0])\n",
    "        vy_x = np.linspace(vy_reader.bounds.left, vy_reader.bounds.right, num=np.shape(vy0)[1])\n",
    "        vy_y = np.linspace(vy_reader.bounds.top, vy_reader.bounds.bottom, num=np.shape(vy0)[0])\n",
    "\n",
    "        # Resample to the DEM grid\n",
    "        fx = interp2d(vx_x, vx_y, vx0)\n",
    "        fy = interp2d(vy_x, vy_y, vy0)\n",
    "        vx_resamp = np.flipud(fx(new_x,new_y)) \n",
    "        vy_resamp = np.flipud(fy(new_x,new_y)) # flip up down\n",
    "\n",
    "        # Display the two velocity files as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(vx_resamp, cmap='Greys_r'); ax1.set_title('vx'); fig.colorbar(im1, ax=ax1)\n",
    "        im2 = ax2.imshow(vy_resamp, cmap='Greys_r'); ax2.set_title('vy'); fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # CALCULATE SEARCH RANGE LIMITS MULTIPLY VX AND VY BY SOME NUMBER\n",
    "        srx_resamp = vx_resamp*sr_scaling; sry_resamp = vy_resamp*sr_scaling\n",
    "\n",
    "        # Display the two search range files as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(srx_resamp, cmap='Greys_r'); ax1.set_title('srx'); fig.colorbar(im1, ax=ax1)\n",
    "        im2 = ax2.imshow(sry_resamp, cmap='Greys_r'); ax2.set_title('sry'); fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # save the reference velocity and search range maps\n",
    "        rio_write(refvpath+vx_outfile, vx_resamp, refdem, CHIPSIZE_M) # vx\n",
    "        rio_write(refvpath+vy_outfile, vy_resamp, refdem, CHIPSIZE_M) # vy\n",
    "        rio_write(refvpath+srx_outfile, srx_resamp, refdem, CHIPSIZE_M) # srx\n",
    "        rio_write(refvpath+sry_outfile, sry_resamp, refdem, CHIPSIZE_M) # sry\n",
    "    else:\n",
    "        print(vx_outfile, ',', vy_outfile, ',', srx_outfile, ',', sry_outfile, 'already exist.')  \n",
    "    \n",
    "    # MASKS\n",
    "    ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    tg_outfile = 'TG_mask_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    tg_mt_outfile = 'TG_mask_MT_'+str(CHIPSIZE_M)+'m.tif'\n",
    "    tg_nt_outfile = 'TG_mask_NT_'+str(CHIPSIZE_M)+'m.tif'\n",
    "    tg_st_outfile = 'TG_mask_ST_'+str(CHIPSIZE_M)+'m.tif'\n",
    "\n",
    "    if True: # overwrite all\n",
    "        # read it in, process (resample, mask, etc.) and resave\n",
    "        ssmreader = rio.open(refvpath+'ice_mask_200mbuffer.tif')\n",
    "        ssm = ssmreader.read(1)\n",
    "        ssm[ssm > 0] = 1; #ssm[ssm < 0.0] = 0; # make binary\n",
    "        ssm = ssm < 1 # find all stable areas (where.tif = 0)\n",
    "\n",
    "        # do the same for Turner Glacier mask\n",
    "        tgreader = rio.open(refvpath+'TG_mask.tif'); tg_mask = tgreader.read(1)\n",
    "        tg_mask[tg_mask > 0] = 1; tg_mask = tg_mask > 0\n",
    "        \n",
    "        # and all the regional masks\n",
    "        tg_mt_reader = rio.open(refvpath+'TG_mask_MT.tif'); tg_mt = tg_mt_reader.read(1)\n",
    "        tg_mt[tg_mt > 0] = 1; tg_mt = tg_mt > 0\n",
    "        tg_nt_reader = rio.open(refvpath+'TG_mask_NT.tif'); tg_nt = tg_nt_reader.read(1)\n",
    "        tg_nt[tg_nt > 0] = 1; tg_nt = tg_nt > 0\n",
    "        tg_st_reader = rio.open(refvpath+'TG_mask_ST.tif'); tg_st = tg_st_reader.read(1)\n",
    "        tg_st[tg_st > 0] = 1; tg_st = tg_st > 0\n",
    "        \n",
    "#         fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(10,5)); ax1.imshow(tg_mt); ax1.set_title('main truk')\n",
    "#         ax2.imshow(tg_nt); ax2.set_title('N tributary'); ax3.imshow(tg_st); ax3.set_title('S tributary')\n",
    "#         plt.show()\n",
    "\n",
    "        # grab x and y-values\n",
    "        ssm_x = np.linspace(ssmreader.bounds.left, ssmreader.bounds.right, num=np.shape(ssm)[1])\n",
    "        ssm_y = np.linspace(ssmreader.bounds.top, ssmreader.bounds.bottom, num=np.shape(ssm)[0])\n",
    "        tg_x = np.linspace(tgreader.bounds.left, tgreader.bounds.right, num=np.shape(tg_mask)[1])\n",
    "        tg_y = np.linspace(tgreader.bounds.top, tgreader.bounds.bottom, num=np.shape(tg_mask)[0])\n",
    "        tg_mtx = np.linspace(tg_mt_reader.bounds.left, tg_mt_reader.bounds.right, num=np.shape(tg_mt)[1])\n",
    "        tg_mty = np.linspace(tg_mt_reader.bounds.top, tg_mt_reader.bounds.bottom, num=np.shape(tg_mt)[0])\n",
    "\n",
    "        # Resample to the DEM grid\n",
    "        f_ssm = interp2d(ssm_x, ssm_y, ssm)\n",
    "        f_tg = interp2d(tg_x, tg_y, tg_mask)\n",
    "        f_tg_mt = interp2d(tg_mtx, tg_mty, tg_mt)\n",
    "        f_tg_nt = interp2d(tg_mtx, tg_mty, tg_nt)\n",
    "        f_tg_st = interp2d(tg_mtx, tg_mty, tg_st)\n",
    "        \n",
    "        ssm_resamp = np.flipud(f_ssm(new_x,new_y))\n",
    "        tg_resamp = np.flipud(f_tg(new_x, new_y))\n",
    "        tg_mt_resamp = np.flipud(f_tg_mt(new_x, new_y))\n",
    "        tg_nt_resamp = np.flipud(f_tg_nt(new_x, new_y))\n",
    "        tg_st_resamp = np.flipud(f_tg_st(new_x, new_y))\n",
    "        \n",
    "        # plot\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ssm_im = ax.imshow(ssm_resamp,cmap='gray',vmin=0)\n",
    "        ax.set_title('Stable Surface Mask')\n",
    "        fig.colorbar(ssm_im, ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "        # export\n",
    "        rio_write(refvpath+ssm_outfile, ssm_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_outfile, tg_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_mt_outfile, tg_mt_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_nt_outfile, tg_nt_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_st_outfile, tg_st_resamp, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        print(ssm_outfile,'and',tg_outfile,'already exist.')\n",
    "    \n",
    "    return dem_outfile, dhdx_outfile, dhdy_outfile, vx_outfile, vy_outfile, srx_outfile, sry_outfile, ssm_outfile, tg_outfile\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf17146",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_geogrid_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ONE CHIPSIZE:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_geogrid_inputs\u001b[49m(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_geogrid_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# ONE CHIPSIZE:\n",
    "generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling)\n",
    "\n",
    "# # MANY CHIPSIZES:\n",
    "# for CHIPSIZE_M in [100,150,160,200,300, 350, 400]:\n",
    "#     generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling)\n",
    "#     print(); print()\n",
    "\n",
    "# COULD MAKE AN EQUIVALENT LOOP FOR MAY SEARCH RANGE SCALING FACTORS - FIND OPTIMAL PARAMATERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b4d2a",
   "metadata": {},
   "source": [
    "## Generate geogrid outputs one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ccaa0",
   "metadata": {},
   "source": [
    "### dhdx, dhdy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c786b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IfSAR_200m_DSM_clipped_dhdy.tif and IfSAR_200m_DSM_clipped_dhdx.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "dhdx_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdx.tif' # generate new filename\n",
    "dhdy_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdy.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(dempath+dhdx_outfile) or not os.path.exists(dempath+dhdy_outfile): # if either is missing\n",
    "    # Produce dhdx and dhdy maps from resampled DEM\n",
    "    dhdx = np.gradient(dem_resamp, axis=1)/CHIPSIZE_M\n",
    "    dhdy = np.gradient(dem_resamp, axis=0)/CHIPSIZE_M\n",
    "\n",
    "    # Filter out borders with high gradient values\n",
    "    grad_thresh = 5\n",
    "    dhdx[abs(dhdx) > grad_thresh] = 0; dhdy[abs(dhdy) > grad_thresh] = 0\n",
    "\n",
    "    # absolute value of the max gradient values expected:\n",
    "    dhmax = 1\n",
    "\n",
    "    # Display the two DEMs as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(dhdx, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "    ax1.set_title('dhdx') # surface slope x\n",
    "    fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "    im2 = ax2.imshow(dhdy, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "    ax2.set_title('dhdy') # surface slope y\n",
    "    fig.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the gradient maps to tif files\n",
    "    print(\"Save surface slope maps to\", dempath)\n",
    "    rio_write(dempath+dhdx_outfile, dhdx, refdem, CHIPSIZE_M) # dhdx\n",
    "    rio_write(dempath_dhdy_outfile, dhdy, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    print(dhdy_outfile, 'and', dhdx_outfile, 'already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a318347",
   "metadata": {},
   "source": [
    "### vx, vy, srx, sry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d359149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vx_200m.tif , vy_200m.tif , srx_200m.tif , sry_200m.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "############### SEARCH RANGE MULTIPLICATION FACTOR ###############\n",
    "# search range limits are calculated as a scalar multiplied by the\n",
    "# reference velocity distribution\n",
    "sr_scaling = 16\n",
    "##################################################################\n",
    "\n",
    "# generate outfile names\n",
    "vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(refvpath+vx_outfile) or not os.path.exists(refvpath+vy_outfile): # if either vx, vy missing\n",
    "    # open the files with rasterio\n",
    "    vx_reader = rio.open(refvpath+vx_fname); vx0 = vx_reader.read(1)\n",
    "    vy_reader = rio.open(refvpath+vy_fname); vy0 = vy_reader.read(1)\n",
    "    vx_x = np.linspace(vx_reader.bounds.left, vx_reader.bounds.right, num=np.shape(vx0)[1])\n",
    "    vx_y = np.linspace(vx_reader.bounds.top, vx_reader.bounds.bottom, num=np.shape(vx0)[0])\n",
    "    vy_x = np.linspace(vy_reader.bounds.left, vy_reader.bounds.right, num=np.shape(vy0)[1])\n",
    "    vy_y = np.linspace(vy_reader.bounds.top, vy_reader.bounds.bottom, num=np.shape(vy0)[0])\n",
    "\n",
    "    # Resample to the DEM grid\n",
    "    fx = interp2d(vx_x, vx_y, vx0)\n",
    "    fy = interp2d(vy_x, vy_y, vy0)\n",
    "    vx_resamp = np.flipud(fx(new_x,new_y)) \n",
    "    vy_resamp = np.flipud(fy(new_x,new_y)) # flip up down\n",
    "\n",
    "    # Display the two velocity files as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(vx_resamp, cmap='Greys_r'); ax1.set_title('vx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(vy_resamp, cmap='Greys_r'); ax2.set_title('vy'); fig.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "    # CALCULATE SEARCH RANGE LIMITS MULTIPLY VX AND VY BY SOME NUMBER\n",
    "    srx_resamp = vx_resamp*sr_scaling; sry_resamp = vy_resamp*sr_scaling\n",
    "\n",
    "    # Display the two search range files as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(srx_resamp, cmap='Greys_r'); ax1.set_title('srx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(sry_resamp, cmap='Greys_r'); ax2.set_title('sry'); fig.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "    \n",
    "    # save the reference velocity and search range maps\n",
    "    rio_write(refvpath+vx_outfile, vx_resamp, refdem, CHIPSIZE_M) # vx\n",
    "    rio_write(refvpath+vy_outfile, vy_resamp, refdem, CHIPSIZE_M) # vy\n",
    "    rio_write(refvpath+srx_outfile, srx_resamp, refdem, CHIPSIZE_M) # srx\n",
    "    rio_write(refvpath+sry_outfile, sry_resamp, refdem, CHIPSIZE_M) # sry\n",
    "else:\n",
    "    print(vx_outfile, ',', vy_outfile, ',', srx_outfile, ',', sry_outfile, 'already exist.')\n",
    "    \n",
    "#     # OPTIONAL REMOVE\n",
    "#     os.remove(refvpath+vx_outfile); os.remove(refvpath+vy_outfile); \n",
    "#     os.remove(refvpath+srx_outfile); os.remove(refvpath+sry_outfile);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb20e4a",
   "metadata": {},
   "source": [
    "### stable surface mask - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beb84ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssm_200m.tif and TG_mask_200m.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "tg_outfile = 'TG_mask_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(refvpath+ssm_outfile) or not os.path.exists(refvpath+tg_outfile):\n",
    "    # read it in, process (resample, mask, etc.) and resave\n",
    "    ssmreader = rio.open(refvpath+'ice_mask.tif')\n",
    "    ssm = ssmreader.read(1)\n",
    "    ssm[ssm > 0] = 1; #ssm[ssm < 0.0] = 0; # make binary\n",
    "    ssm = ssm < 0 # find all stable areas (where.tif = 0)\n",
    "    \n",
    "    # do the same for Turner Glacier mask\n",
    "    tgreader = rio.open(refvpath+'TG_mask.tif'); tg_mask = tgreader.read(1)\n",
    "    tg_mask[tg_mask > 0] = 1\n",
    "#     tg_mask = tg_mask < 0\n",
    "\n",
    "    # grab x and y-values\n",
    "    ssm_x = np.linspace(ssmreader.bounds.left, ssmreader.bounds.right, num=np.shape(ssm)[1])\n",
    "    ssm_y = np.linspace(ssmreader.bounds.top, ssmreader.bounds.bottom, num=np.shape(ssm)[0])\n",
    "    tg_x = np.linspace(tgreader.bounds.left, tgreader.bounds.right, num=np.shape(tg_mask)[1])\n",
    "    tg_y = np.linspace(tgreader.bounds.top, tgreader.bounds.bottom, num=np.shape(tg_mask)[0])\n",
    "\n",
    "    # Resample to the DEM grid\n",
    "    f_ssm = interp2d(ssm_x, ssm_y, ssm)\n",
    "    f_tg = interp2d(tg_x, tg_y, tg_mask)\n",
    "    ssm_resamp = np.flipud(f_ssm(new_x,new_y))\n",
    "    tg_resamp = np.flipud(f_tg(new_x, new_y))\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ssm_im = ax.imshow(tg_resamp,cmap='gray',vmin=0)\n",
    "    ax.set_title('Stable Surface Mask')\n",
    "    fig.colorbar(ssm_im, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "#     # export\n",
    "#     rio_write(refvpath+ssm_outfile, ssm_resamp, refdem, CHIPSIZE_M)\n",
    "    rio_write(refvpath+tg_outfile, tg_resamp, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    print(ssm_outfile,'and',tg_outfile,'already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a83fb5",
   "metadata": {},
   "source": [
    "### csminx, csminy, csmax, csmaxy - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e784f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csminx_200m.tif , csminy_200m.tif , csmaxx_200m.tif , csmaxy_200m.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "# generate the file names\n",
    "csminx_fname = 'csminx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csminy_fname = 'csminy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csmaxx_fname = 'csmaxx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csmaxy_fname = 'csmaxy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "\n",
    "if not os.path.exists(refvpath+csminx_fname) or not os.path.exists(refvpath+csminy_fname): # if either csminx or csminy missing:\n",
    "    # create the rasters\n",
    "    csminx = np.ones(dem_resamp.shape)*CHIPSIZE_M # minimum chip size shaped like the resampled DEM\n",
    "    csminy = csminx # make identical in both directions\n",
    "    csmaxx = csminx*4 # make maximum chip size 4*minimum chip size (400)\n",
    "    csmaxy = csmaxx\n",
    "    \n",
    "    # export\n",
    "    rio_write(refvpath+csminx_fname, csminx, refdem, CHIPSIZE_M) # csminx\n",
    "    rio_write(refvpath+csminy_fname, csminy, refdem, CHIPSIZE_M) # csminy\n",
    "    rio_write(refvpath+csmaxx_fname, csmaxx, refdem, CHIPSIZE_M) # csmaxx\n",
    "    rio_write(refvpath+csmaxy_fname, csmaxy, refdem, CHIPSIZE_M) # csmaxx\n",
    "else:\n",
    "    print(csminx_fname, ',',csminy_fname, ',', csmaxx_fname, ',', csmaxy_fname, 'already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a12a5",
   "metadata": {},
   "source": [
    "## 2) Run geogrid with resampled DEM and other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f45b948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topsApp.py available for SAR image coregistration.\n"
     ]
    }
   ],
   "source": [
    "###### SET PATH PROCESSING PATHS ##########################\n",
    "out_path = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/output_AutoRIFT/' # output file path\n",
    "\n",
    "# # LS test:\n",
    "# imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/LS8images/useable_images/'\n",
    "# im1_name = 'LC08_L1TP_062018_20200401_20200410_01_T1_B8_BufferTurner.TIF'\n",
    "# im2_name = 'LC08_L1TP_062018_20200417_20200423_01_T1_B8_BufferTurner.TIF'\n",
    "# img_type = 'OPT'\n",
    "\n",
    "# S2 test:\n",
    "imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel2/'\n",
    "im1_name = 'S2A_7VEG_20200427_B08_clipped.tif'\n",
    "im2_name = 'S2A_7VEG_20200510_B08_clipped.tif'\n",
    "img_type = 'OPT'\n",
    "\n",
    "# # PS test:\n",
    "# imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/Planet_test/'\n",
    "# im1_name = 'PS_20190304_clipped.tif'\n",
    "# im2_name = 'PS_20190307_clipped.tif'\n",
    "# img_type = 'OPT'\n",
    "\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b21def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                        dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                       temp_dir): # SAR needed only\n",
    "    \n",
    "    CHIPSIZE_M = MINCHIPSIZE # set minimum chip size equal\n",
    "    ############ Clear all old geogrid files ##########################\n",
    "    for file in os.listdir(out_path):\n",
    "        if file.startswith('window') and file.endswith('.tif'):\n",
    "            print('removed', file)\n",
    "            os.remove(out_path+file)\n",
    "    print('Old files cleared.'); print()\n",
    "\n",
    "    dem_info = gdal.Info(dem, format='json') # grab info from DEM\n",
    "    print('Obtained DEM info.'); print()\n",
    "\n",
    "    ############ Run geogrid optical or SAR ##########################\n",
    "    if img_type == 'OPT': # Optical images\n",
    "        print('Processing optical images with geogrid.'); print()\n",
    "        obj = GeogridOptical() # initialize geogrid object\n",
    "\n",
    "        ############ Coregister the optical data (from coregisterLoadMetadataOptical) #############\n",
    "        x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s,0)\n",
    "\n",
    "        # grab dates from file names\n",
    "        im1_name = indir_m.split('/')[-1]; im2_name = indir_s.split('/')[-1]\n",
    "        if 'LC' in im1_name and 'LC' in im2_name:\n",
    "            ds1 = im1_name.split('_')[3]\n",
    "            ds2 = im1_name.split('_')[4]\n",
    "        elif 'S2' in im1_name and 'S2' in im2_name:\n",
    "            ds1 = im1_name[9:17]\n",
    "            ds2 = im2_name[9:17]\n",
    "        elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "            ds1 = im1_name[3:11]\n",
    "            ds2 = im2_name[3:11]\n",
    "        else:\n",
    "            raise Exception('Optical data NOT supported yet!') \n",
    "        print('Optical images coregistered.'); print()\n",
    "\n",
    "        ########### Load geogrid inputs and run (from runGeogridOptical) ################\n",
    "\n",
    "        # grab info from above\n",
    "        obj.startingX = trans[0]; obj.startingY = trans[3]\n",
    "        obj.XSize = trans[1]; obj.YSize = trans[5]\n",
    "        d0 = datetime.date(int(ds1[0:4]),int(ds1[4:6]),int(ds1[6:8]))\n",
    "        d1 = datetime.date(int(ds2[0:4]),int(ds2[4:6]),int(ds2[6:8]))\n",
    "        date_dt_base = d1 - d0\n",
    "        obj.repeatTime = date_dt_base.total_seconds()\n",
    "        obj.numberOfLines = ysize1; obj.numberOfSamples = xsize1\n",
    "        obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "\n",
    "        # customize no data value and minimimum chip size\n",
    "        obj.nodata_out = NO_DATA_VAL\n",
    "        obj.chipSizeX0 = MINCHIPSIZE\n",
    "\n",
    "        # set raster paths and names\n",
    "        obj.dat1name = indir_m # first image\n",
    "        obj.demname = dem # DEM\n",
    "        obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "        obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "        obj.srxname = srx; obj.sryname = sry # search range limits\n",
    "        obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "        obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "        obj.ssmname = ssm # stable surface mask\n",
    "        obj.winlocname = \"window_location.tif\"\n",
    "        obj.winoffname = \"window_offset.tif\"\n",
    "        obj.winsrname = \"window_search_range.tif\"\n",
    "        obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "        obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "        obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "        obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "        obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "\n",
    "        obj.runGeogrid() # RUN GEOGRID\n",
    "        print('Optical geogrid finished.'); print()\n",
    "\n",
    "    elif img_type == 'SAR': # SAR images\n",
    "        print('Processing SAR images with geogrid.'); print();\n",
    "        ############ Load SAR metadata from coreg_files ##################################\n",
    "        # Store sensing start info for 2nd SAR image (in temp_dir+secondary/)\n",
    "        frames = []\n",
    "        for swath in range(1,4):\n",
    "            inxml = os.path.join(temp_dir+'secondary/', 'IW{0}.xml'.format(swath))\n",
    "            if os.path.exists(inxml):\n",
    "                pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file\n",
    "                frames.append(ifg)\n",
    "        info1_sensingStart = min([x.sensingStart for x in frames]) # store info1_sensingStart\n",
    "\n",
    "        # Load other info from 1st SAR image (in temp_dir+reference/)\n",
    "        del frames; frames = [] \n",
    "        for swath in range(1,4):\n",
    "            inxml = os.path.join(temp_dir+'reference/', 'IW{0}.xml'.format(swath))\n",
    "            if os.path.exists(inxml):\n",
    "                pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file        \n",
    "                frames.append(ifg)\n",
    "        print('SAR metadata loaded.'); print()\n",
    "\n",
    "        ############ Get merged orbit getMergedOrbit() ################################## \n",
    "        # Create merged orbit\n",
    "        orb = Orbit(); orb.configure()\n",
    "        burst = frames[0].bursts[0]\n",
    "        # Add first burst orbit to begin with\n",
    "        for sv in burst.orbit:\n",
    "            orb.addStateVector(sv)\n",
    "        for pp in frames:\n",
    "            # Add all state vectors\n",
    "            for bb in pp.bursts:\n",
    "                for sv in bb.orbit:\n",
    "                    if (sv.time< orb.minTime) or (sv.time > orb.maxTime):\n",
    "                        orb.addStateVector(sv)\n",
    "        print('Merged orbit created.'); print()\n",
    "\n",
    "        ############ Load geogrid inputs and run ###################################\n",
    "        obj = Geogrid()\n",
    "        obj.configure()\n",
    "\n",
    "        obj.orbit = orb # grab merged orbit\n",
    "        obj.startingRange = min([x.startingRange for x in frames])\n",
    "        obj.rangePixelSize = frames[0].bursts[0].rangePixelSize\n",
    "        obj.sensingStart = min([x.sensingStart for x in frames])\n",
    "        obj.prf = 1.0 / frames[0].bursts[0].azimuthTimeInterval\n",
    "        obj.lookSide = -1\n",
    "        obj.repeatTime = (info1_sensingStart - obj.sensingStart).total_seconds() # INFO1\n",
    "        obj.numberOfLines = int(np.round((max([x.sensingStop for x in frames])-obj.sensingStart).total_seconds()*obj.prf))+1\n",
    "        obj.numberOfSamples = int(np.round((max([x.farRange for x in frames])-obj.startingRange)/obj.rangePixelSize))+1\n",
    "        obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "\n",
    "        # custom no data value and chip size\n",
    "        obj.nodata_out = NO_DATA_VAL\n",
    "        obj.chipSizeX0 = CHIPSIZE_M\n",
    "\n",
    "        # set raster paths and names\n",
    "        obj.demname = dem # DEM\n",
    "        obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "        obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "        obj.srxname = srx; obj.sryname = sry # search range limmits\n",
    "        obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "        obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "        obj.ssmname = ssm # stable surface mask\n",
    "        obj.winlocname = \"window_location.tif\"\n",
    "        obj.winoffname = \"window_offset.tif\"\n",
    "        obj.winsrname = \"window_search_range.tif\"\n",
    "        obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "        obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "        obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "        obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "        obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "\n",
    "        obj.getIncidenceAngle() # SAR specific\n",
    "        obj.geogrid() # run geogrid\n",
    "        print('SAR geogrid finished.'); print();\n",
    "\n",
    "    else: # not OPT or SAR\n",
    "        print('Image type flag not recognized :', img_type)\n",
    "\n",
    "\n",
    "    ############ Move files produced to the out_path directory ##############\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if file.startswith('window') and file.endswith('.tif'):\n",
    "            shutil.move(os.getcwd()+'/'+file, out_path+file)\n",
    "    print('Geogrid output files moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b586844",
   "metadata": {},
   "source": [
    "### For SAR images only: Coregister SAR images with topsApp.py\n",
    "\n",
    "#### Put TG's custom topApp.xml on repo\n",
    "\n",
    "Requires orbit and auxiliary files for the S1 images as well as the creation of 3 xml files (topsApp.xml, reference.xml, secondary.xml).\n",
    "\n",
    "Prior to any processing, download S1 .SAFE files, auxiliary instrument files, and orbit files.\n",
    "\n",
    "- Download .SAFE files from Alaska Satellite Facility: https://search.asf.alaska.edu/\n",
    "- Download auxiliary instrument files from https://qc.sentinel1.copernicus.eu/aux_ins/\n",
    "- Download orbit files from Copernicus Sentinels POD Data Hub: https://scihub.copernicus.eu/gnss/#/home\n",
    "\n",
    "Although Sentinel-1 restituted orbits (RESORB) are of good quality, it is recommended to use the precise orbits (POEORB) when available. Typically, precise orbits are available with a 15 to 20-day lag from the day of the acquisition.\n",
    "\n",
    "To proceed with automatic downloads from ASF, **wget** must be downloaded. Create a wget configuration file in the image directory using terminal following the [ASF API instructions](https://docs.asf.alaska.edu/api/tools/):\n",
    "\n",
    "    echo 'http_user=CHANGE_ME' >> wget.conf\n",
    "    echo 'http_password=CHANGE_ME' >> wget.conf\n",
    "    chmod 600 wget.conf\n",
    "    export WGETRC=\"wget.conf\"\n",
    "    \n",
    "Also, create a netrc file containing the NASA Earthdata login info for topsApp.py.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5d74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coregister pre-process the SAR images, generating XM: files\n",
    "# if img_type == 'SAR':\n",
    "#     create_netrc('.netrc') # create netrc file with Earthdata credentials if necessary\n",
    "    \n",
    "#     # make a folder to hold intermediate files used for coregistration\n",
    "#     temp_dir = imgpath+'coreg_files/'\n",
    "#     if not os.path.exists(temp_dir): # if doesn't exist\n",
    "#         os.mkdir(temp_dir) # make it\n",
    "#         print('coreg_files folder created.'); print()\n",
    "#     else: # CLEAR ENTIRE FOLDER?\n",
    "#         for file in os.listdir(temp_dir): # if it already exists:\n",
    "#             if file.endswith('.xml'): # clear all the old XML files\n",
    "#                 os.remove(temp_dir+file)\n",
    "#         print('coreg_files folder already exists. Old files cleared.'); print()\n",
    "           \n",
    "#     # download the orbit files\n",
    "#     orbit_dir = imgpath+'orbits/'\n",
    "#     config_path = imgpath+'wget.conf' # path to the WGET config file\n",
    "#     if not os.path.isdir(orbit_dir): # if orbits folder doesn't exist\n",
    "#         os.mkdir(orbit_dir) # make it \n",
    "#     if os.path.isfile(imgpath+'wget.conf'):\n",
    "#         print('Downloading orbit files: ')\n",
    "#         download_orbits(imgpath+im1_name, config_path, orbit_dir) # orbit file for 1st image\n",
    "#         download_orbits(imgpath+im2_name, config_path, orbit_dir) # orbit file for 2nd image\n",
    "#     else:\n",
    "#         print('Error: wget.conf not in ',imgpath)\n",
    "#     print()\n",
    "    \n",
    "#     # move template XML files into the temporary folder\n",
    "#     for xmlname in ['topsApp.xml', 'reference.xml','secondary.xml']:\n",
    "#         if os.path.isfile(imgpath+xmlname):# if these xml template files exist\n",
    "#             shutil.copyfile(imgpath+xmlname, temp_dir+xmlname) # copy them into the temp folder\n",
    "#         else:\n",
    "#             print('Error: missing the template file '+xmlname); print()\n",
    "            \n",
    "#     # auto update reference.xml & secondary.xml using XML Element Tree (ET) package\n",
    "#     tree1 = ET.parse(temp_dir+'reference.xml'); root1 = tree1.getroot() # first image\n",
    "#     tree2 = ET.parse(temp_dir+'secondary.xml'); root2 = tree2.getroot() # second image\n",
    "#     for prop in root1.iter(): # REFERENCE\n",
    "#         if prop.get('name') == 'orbit directory':\n",
    "#             prop.text = orbit_dir # set orbit directory to orbit_dir\n",
    "#         if prop.get('name') == 'safe':\n",
    "#             prop.text = imgpath+im1_name # set SAFE.zip file path\n",
    "#     tree1.write(temp_dir+'reference.xml') # over-write\n",
    "#     for prop in root2.iter(): # SECONDARY\n",
    "#         if prop.get('name') == 'orbit directory':\n",
    "#             prop.text = orbit_dir # set orbit directory to orbit_dir\n",
    "#         if prop.get('name') == 'safe':\n",
    "#             prop.text = imgpath+im2_name # set SAFE.zip file path\n",
    "#     tree2.write(temp_dir+'secondary.xml') # over-write\n",
    "    \n",
    "#     # Run topsApp.py coregistration (will take some time to complete)\n",
    "#     runtopsapp = 'cd '+temp_dir+'; ' # change directory into temp folder\n",
    "#     runtopsapp += 'python3 '+imgpath+'topsApp.py --start=startup --end=rangecoreg' # tun topsapp.py\n",
    "#     subprocess.run(runtopsapp, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cb1eb",
   "metadata": {},
   "source": [
    "### Set custom Geogrid parameters (Optical and SAR)\n",
    "\n",
    "     input \"dhdxname\"/\"dhdyname\"                          -> output \"winro2vxname\"/\"winro2vyname\"\n",
    "     input \"dhdxname\"/\"dhdyname\" and \"vxname\"/\"vyname\"    -> output \"winro2vxname\"/\"winro2vyname\" and \"winoffname\" \n",
    "     input \"dhdxname\"/\"dhdyname\" and \"srxname\"/\"sryname\"  -> output \"winro2vxname\"/\"winro2vyname\" and \"winsrname\"\n",
    "     input \"csminxname\"/\"csminyname\"                      -> output \"wincsminname\"\n",
    "     input \"csmaxxname\"/\"csmaxyname\"                      -> output \"wincsmaxname\"\n",
    "     input \"ssmname\"                                      -> output \"winssmname\"\n",
    "\n",
    "#### Best  MINCHIPSIZE >= SCALAR*PIXRES where SCALAR = 16 or some other power of 2\n",
    "\n",
    "    LS = 200\n",
    "    S2 = 160\n",
    "    PS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3760e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dem_outfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###### SET CUSTOM PARAMETERS FOR GEOGRID ################\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dem \u001b[38;5;241m=\u001b[39m dempath\u001b[38;5;241m+\u001b[39m\u001b[43mdem_outfile\u001b[49m \u001b[38;5;66;03m# path to the resampled DEM produced in the previous step (outfile name)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m indir_m \u001b[38;5;241m=\u001b[39m imgpath\u001b[38;5;241m+\u001b[39mim1_name\n\u001b[1;32m      4\u001b[0m indir_s \u001b[38;5;241m=\u001b[39m imgpath\u001b[38;5;241m+\u001b[39mim2_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dem_outfile' is not defined"
     ]
    }
   ],
   "source": [
    "###### SET CUSTOM PARAMETERS FOR GEOGRID ################\n",
    "dem = dempath+dem_outfile # path to the resampled DEM produced in the previous step (outfile name)\n",
    "indir_m = imgpath+im1_name\n",
    "indir_s = imgpath+im2_name\n",
    "MINCHIPSIZE = 160 # smallest chip size allowed in image horizontal direction (in m)\n",
    "NO_DATA_VAL = -32767 # no data value in the output products\n",
    "temp_dir = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/coreg_files/'\n",
    "\n",
    "if MINCHIPSIZE > CHIPSIZE_M:\n",
    "    warning = 'Your minimum chip size for autoRIFT exceeds the output grid size (CHIPSIZE_M). '\n",
    "    warning += 'Please increase the resampled DEM gridsize (CHIPSIZE_M).'\n",
    "    print(warning)\n",
    "\n",
    "# optional inputs (set as '' to leave blank)\n",
    "dhdx = dempath+dhdx_outfile\n",
    "dhdy = dempath+dhdy_outfile\n",
    "vx = refvpath+vx_outfile\n",
    "vy = refvpath+vy_outfile\n",
    "# srx = refvpath+srx_outfile\n",
    "# sry = refvpath+sry_outfile\n",
    "csminx = refvpath+csminx_fname\n",
    "csminy = refvpath+csminy_fname\n",
    "csmaxx = refvpath+csmaxx_fname\n",
    "csmaxy = refvpath+csmaxy_fname\n",
    "ssm = refvpath+ssm_outfile # stable surface mask \n",
    "\n",
    "# dhdx = ''\n",
    "# dhdy = ''\n",
    "# vx = ''\n",
    "# vy = ''\n",
    "srx = ''\n",
    "sry = ''\n",
    "# csminx = ''\n",
    "# csminy = ''\n",
    "# csmaxx = ''\n",
    "# csmaxy = ''\n",
    "# ssm = ''\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "531c6457",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indir_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_geogrid_inhouse(out_path, img_type, \u001b[43mindir_m\u001b[49m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, \u001b[38;5;66;03m# required inputs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                     dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, \u001b[38;5;66;03m# optional inputs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                     temp_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indir_m' is not defined"
     ]
    }
   ],
   "source": [
    "run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                    dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                    temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ Clear all old geogrid files ##########################\n",
    "# for file in os.listdir(out_path):\n",
    "#     if file.startswith('window') and file.endswith('.tif'):\n",
    "#         print('removed', file)\n",
    "#         os.remove(out_path+file)\n",
    "# print('Old files cleared.'); print()\n",
    "\n",
    "# dem_info = gdal.Info(dem, format='json') # grab info from DEM\n",
    "# print('Obtained DEM info.'); print()\n",
    "\n",
    "# ############ Run geogrid optical or SAR ##########################\n",
    "# if img_type == 'OPT': # Optical images\n",
    "#     print('Processing optical images with geogrid.'); print()\n",
    "#     obj = GeogridOptical() # initialize geogrid object\n",
    "    \n",
    "#     ############ Coregister the optical data (from coregisterLoadMetadataOptical) #############\n",
    "#     x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s,0)\n",
    "    \n",
    "#     # grab dates from file names\n",
    "#     if 'LC' in im1_name and 'LC' in im2_name:\n",
    "#         ds1 = im1_name.split('_')[3]\n",
    "#         ds2 = im1_name.split('_')[4]\n",
    "#     elif 'S2' in im1_name and 'S2' in im2_name:\n",
    "#         ds1 = im1_name[9:17]\n",
    "#         ds2 = im2_name[9:17]\n",
    "#     elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "#         ds1 = im1_name[3:11]\n",
    "#         ds2 = im2_name[3:11]\n",
    "#     else:\n",
    "#         raise Exception('Optical data NOT supported yet!') \n",
    "#     print('Optical images coregistered.'); print()\n",
    "    \n",
    "#     ########### Load geogrid inputs and run (from runGeogridOptical) ################\n",
    "    \n",
    "#     # grab info from above\n",
    "#     obj.startingX = trans[0]; obj.startingY = trans[3]\n",
    "#     obj.XSize = trans[1]; obj.YSize = trans[5]\n",
    "#     d0 = date(np.int(ds1[0:4]),np.int(ds1[4:6]),np.int(ds1[6:8]))\n",
    "#     d1 = date(np.int(ds2[0:4]),np.int(ds2[4:6]),np.int(ds2[6:8]))\n",
    "#     date_dt_base = d1 - d0\n",
    "#     obj.repeatTime = date_dt_base.total_seconds()\n",
    "#     obj.numberOfLines = ysize1; obj.numberOfSamples = xsize1\n",
    "#     obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "    \n",
    "#     # customize no data value and minimimum chip size\n",
    "#     obj.nodata_out = NO_DATA_VAL\n",
    "#     obj.chipSizeX0 = CHIPSIZE_M\n",
    "    \n",
    "#     # set raster paths and names\n",
    "#     obj.dat1name = indir_m # first image\n",
    "#     obj.demname = dem # DEM\n",
    "#     obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "#     obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "#     obj.srxname = srx; obj.sryname = sry # search range limits\n",
    "#     obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "#     obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "#     obj.ssmname = ssm # stable surface mask\n",
    "#     obj.winlocname = \"window_location.tif\"\n",
    "#     obj.winoffname = \"window_offset.tif\"\n",
    "#     obj.winsrname = \"window_search_range.tif\"\n",
    "#     obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "#     obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "#     obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "#     obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "#     obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "    \n",
    "#     obj.runGeogrid() # RUN GEOGRID\n",
    "#     print('Optical geogrid finished.'); print()\n",
    "    \n",
    "# elif img_type == 'SAR': # SAR images\n",
    "#     print('Processing SAR images with geogrid.'); print();\n",
    "#     ############ Load SAR metadata from coreg_files ##################################\n",
    "#     # Store sensing start info for 2nd SAR image (in temp_dir+secondary/)\n",
    "#     frames = []\n",
    "#     for swath in range(1,4):\n",
    "#         inxml = os.path.join(temp_dir+'secondary/', 'IW{0}.xml'.format(swath))\n",
    "#         if os.path.exists(inxml):\n",
    "#             pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file\n",
    "#             frames.append(ifg)\n",
    "#     info1_sensingStart = min([x.sensingStart for x in frames]) # store info1_sensingStart\n",
    "    \n",
    "#     # Load other info from 1st SAR image (in temp_dir+reference/)\n",
    "#     del frames; frames = [] \n",
    "#     for swath in range(1,4):\n",
    "#         inxml = os.path.join(temp_dir+'reference/', 'IW{0}.xml'.format(swath))\n",
    "#         if os.path.exists(inxml):\n",
    "#             pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file        \n",
    "#             frames.append(ifg)\n",
    "#     print('SAR metadata loaded.'); print()\n",
    "    \n",
    "#     ############ Get merged orbit getMergedOrbit() ################################## \n",
    "#     # Create merged orbit\n",
    "#     orb = Orbit(); orb.configure()\n",
    "#     burst = frames[0].bursts[0]\n",
    "#     # Add first burst orbit to begin with\n",
    "#     for sv in burst.orbit:\n",
    "#         orb.addStateVector(sv)\n",
    "#     for pp in frames:\n",
    "#         # Add all state vectors\n",
    "#         for bb in pp.bursts:\n",
    "#             for sv in bb.orbit:\n",
    "#                 if (sv.time< orb.minTime) or (sv.time > orb.maxTime):\n",
    "#                     orb.addStateVector(sv)\n",
    "#     print('Merged orbit created.'); print()\n",
    "                    \n",
    "#     ############ Load geogrid inputs and run ###################################\n",
    "#     obj = Geogrid()\n",
    "#     obj.configure()\n",
    "    \n",
    "#     obj.orbit = orb # grab merged orbit\n",
    "#     obj.startingRange = min([x.startingRange for x in frames])\n",
    "#     obj.rangePixelSize = frames[0].bursts[0].rangePixelSize\n",
    "#     obj.sensingStart = min([x.sensingStart for x in frames])\n",
    "#     obj.prf = 1.0 / frames[0].bursts[0].azimuthTimeInterval\n",
    "#     obj.lookSide = -1\n",
    "#     obj.repeatTime = (info1_sensingStart - obj.sensingStart).total_seconds() # INFO1\n",
    "#     obj.numberOfLines = int(np.round((max([x.sensingStop for x in frames])-obj.sensingStart).total_seconds()*obj.prf))+1\n",
    "#     obj.numberOfSamples = int(np.round((max([x.farRange for x in frames])-obj.startingRange)/obj.rangePixelSize))+1\n",
    "#     obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "    \n",
    "#     # custom no data value and chip size\n",
    "#     obj.nodata_out = NO_DATA_VAL\n",
    "#     obj.chipSizeX0 = CHIPSIZE_M\n",
    "    \n",
    "#     # set raster paths and names\n",
    "#     obj.demname = dem # DEM\n",
    "#     obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "#     obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "#     obj.srxname = srx; obj.sryname = sry # search range limmits\n",
    "#     obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "#     obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "#     obj.ssmname = ssm # stable surface mask\n",
    "#     obj.winlocname = \"window_location.tif\"\n",
    "#     obj.winoffname = \"window_offset.tif\"\n",
    "#     obj.winsrname = \"window_search_range.tif\"\n",
    "#     obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "#     obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "#     obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "#     obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "#     obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "    \n",
    "#     obj.getIncidenceAngle() # SAR specific\n",
    "#     obj.geogrid() # run geogrid\n",
    "#     print('SAR geogrid finished.'); print();\n",
    "    \n",
    "# else: # not OPT or SAR\n",
    "#     print('Image type flag not recognized :', img_type)\n",
    "    \n",
    "\n",
    "# ############ Move files produced to the out_path directory ##############\n",
    "# for file in os.listdir(os.getcwd()):\n",
    "#     if file.startswith('window') and file.endswith('.tif'):\n",
    "#         shutil.move(os.getcwd()+'/'+file, out_path+file)\n",
    "# print('Geogrid output files moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22290d",
   "metadata": {},
   "source": [
    "## 3) Run autoRIFT with new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45646160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify files produced from geogrid\n",
    "gp = out_path\n",
    "\n",
    "# remove all empty grids\n",
    "for grid in os.listdir(gp): \n",
    "    if grid.startswith('window') and grid.endswith('.tif'):\n",
    "        reader = rio.open(gp+grid) # read dataset\n",
    "        data_found = False \n",
    "        for band in range(1,reader.count+1):\n",
    "            testband = reader.read(band) # read in the band\n",
    "            if np.count_nonzero(testband[testband != NO_DATA_VAL]) > 0:\n",
    "                data_found = True\n",
    "        if not data_found:\n",
    "            print(grid, 'has no data. Removed.')\n",
    "            os.remove(gp+grid)\n",
    "        else:\n",
    "            print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in AutoRIFT parameters using the files - make all of these function arguments\n",
    "mpflag = 0 # leave multiprocessing off\n",
    "\n",
    "# GRID LOCATION (required) from window_location.tif\n",
    "grid_location = rio.open(gp+'window_location.tif')\n",
    "xGrid = grid_location.read(1) # 1st band in window location\n",
    "yGrid = grid_location.read(2) # 2nd band in window location\n",
    "\n",
    "# optional parameters (default None or zero until filled)\n",
    "init_offset = None; search_range = None\n",
    "chip_size_min = None; chip_size_max = None\n",
    "offset2vx = None; offset2vy = None; stable_surface_mask = None\n",
    "Dx0 = None; Dy0 = None; CSMINx0 = None\n",
    "SRx0 = None; SRy0 = None;\n",
    "CSMAXx0 = None; CSMAXy0 = None; SSM = None\n",
    "noDataMask = np.zeros(xGrid.shape).astype(int)\n",
    "\n",
    "if os.path.exists(gp+'window_offset.tif'): # Dx0 and Dy0 from window_offset.tif\n",
    "    init_offset = rio.open(gp+'window_offset.tif')\n",
    "    Dx0 = init_offset.read(1); Dy0 = init_offset.read(2)\n",
    "if os.path.exists(gp+'window_search_range.tif'): # SRx0 and SRy0 from window_search_range.tif\n",
    "    search_range = rio.open(gp+'window_search_range.tif')\n",
    "    SRx0 = search_range.read(1); SRy0 = search_range.read(2)\n",
    "if os.path.exists(gp+'window_chip_size_min.tif'): # CSMINx0 and CSMINy0 from window_chip_size_min.tif\n",
    "    chip_size_min = rio.open(gp+'window_chip_size_min.tif')\n",
    "    CSMINx0 = chip_size_min.read(1); CSMINy0 = chip_size_min.read(2)\n",
    "if os.path.exists(gp+'window_chip_size_max.tif'): # CSMAXx0 and CSMAXy0 from window_chip_size_max.tif\n",
    "    chip_size_max = rio.open(gp+'window_chip_size_max.tif')\n",
    "    CSMAXx0 = chip_size_max.read(1); CSMAXy0 = chip_size_max.read(2)\n",
    "if os.path.exists(gp+'window_rdr_off2vel_x_vec.tif'): # offset2vx from window_rdr_off2vel_x_vec.tif\n",
    "    offset2vx = gp+'window_rdr_off2vel_x_vec.tif' # path to be read in with GDAL\n",
    "if os.path.exists(gp+'window_rdr_off2vel_y_vec.tif'): # offset2vy from window_rdr_off2vel_y_vec.tif\n",
    "    offset2vy = gp+'window_rdr_off2vel_y_vec.tif' \n",
    "if os.path.exists(gp+'window_stable_surface_mask.tif'): # noDataMask from window_stable_surface_mask.tif\n",
    "    stable_surface_mask = rio.open(gp+'window_stable_surface_mask.tif')\n",
    "    noDataMask = stable_surface_mask.read(1)\n",
    "    \n",
    "# other parameters\n",
    "nodata = NO_DATA_VAL # use same as in previous steps\n",
    "geogrid_run_info=None\n",
    "print('AutoRIFT parameters loaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### CHOOSE FILTERS & SAMPLING ########################\n",
    "# Filters:\n",
    "# options are HPS (high pass), WAL (wallis), SOB (sobel), DB (logarithmic operator)\n",
    "FILTER = 'WAL'\n",
    "WALLISFILTERWIDTH = 32 # only for wallis filter, must be a power of 2\n",
    "\n",
    "# Sampling:\n",
    "SPARSE_SEARCH_SAMPLE_RATE = 16 # how many samples to skip to speed up processing\n",
    "OVERSAMPLE_RATIO = 0 # enter in a constant scalar or 0 for default parameters\n",
    "############################################################################\n",
    "print('Filters and autoRIFT sample rates chosen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoRIFT  \n",
    "def run_autoRIFT_inhouse(out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                         FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATIO, OVERSAMPLE_RATIO, MINCHIPSIZE,\n",
    "                         Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                         noDataMask, nodataval, geogrid_run_info):\n",
    "    CHIPSIZE_M = MINCHIPSIZE # set minimum chip size equal\n",
    "    \n",
    "    # requires grid location from geogrid\n",
    "    origSize = xGrid.shape # grab original size from xGrid\n",
    "    \n",
    "    if img_type == 'OPT': ############# OPTICAL SETTINGS ############################# \n",
    "        print('Processing optical images with autoRIFT.'); print()\n",
    "        optflag = 1 # turn on optical flag\n",
    "        # Coregister and read in the two images (from loadProductOptical())\n",
    "        obj = GeogridOptical()\n",
    "        x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s,0)\n",
    "\n",
    "        # read dates from filenames\n",
    "        if 'LC' in indir_m and 'LC' in indir_s:\n",
    "            ds1 = indir_m.split('/')[-1].split('_')[3]; ds2 = indir_s.split('/')[-1].split('_')[3]\n",
    "            sat = 'LS'\n",
    "        elif 'S2' in indir_m and 'S2' in indir_s:\n",
    "            ds1 = indir_m.split('/')[-1][9:17]; ds2 = indir_s.split('/')[-1][9:17]\n",
    "            sat = 'S2'\n",
    "        elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "            ds1 = indir_m.split('/')[-1].split('_')[1]\n",
    "            ds2 = indir_s.split('/')[-1].split('_')[1]\n",
    "            sat = 'PS'\n",
    "        else:\n",
    "            raise Exception('Optical data NOT supported yet!')\n",
    "\n",
    "        # read in the images\n",
    "        DS1 = gdal.Open(indir_m); DS2 = gdal.Open(indir_s)\n",
    "        I1 = DS1.ReadAsArray(xoff=x1a, yoff=y1a, xsize=xsize1, ysize=ysize1)\n",
    "        I1 = I1.astype(np.float32)\n",
    "        I2 = DS2.ReadAsArray(xoff=x2a, yoff=y2a, xsize=xsize2, ysize=ysize2)\n",
    "        I2 = I2.astype(np.float32)\n",
    "        DS1=None; DS2=None # clear DS1 and DS2\n",
    "\n",
    "        # Initialize autoRIFT object (from runAutorift())\n",
    "        obj = autoRIFT_ISCE()\n",
    "        obj.configure()\n",
    "\n",
    "    elif img_type == 'SAR': ############# SAR SETTINGS #############################  \n",
    "        print('Processing SAR images with autoRIFT.'); print()\n",
    "        optflag = 0 # turn off opt flag\n",
    "        # Read in the two SAR images (from loadProduct())\n",
    "        img1 = IML.mmapFromISCE(filename1, logging); I1 = IMG.bands[0]\n",
    "        img2 = IML.mmapFromISCE(filename2, logging); I2 = IMG.bands[0]\n",
    "        I1 = np.abs(I1); I2 = np.abs(I2) # SAR amplitude only\n",
    "        \n",
    "    else:\n",
    "        print(\"Image type not recognized. Use either 'OPT' or 'SAR'.\")\n",
    "        \n",
    "    ############# Initialize autoRIFT object (from runAutorift()) ##################\n",
    "    obj = autoRIFT_ISCE()\n",
    "    obj.configure()\n",
    "    \n",
    "    obj.MultiThread = mpflag # multiprocessing\n",
    "    obj.I1 = I1; obj.I2 = I2 # assign the images\n",
    "    obj.xGrid = xGrid; obj.yGrid = yGrid # assign the grid \n",
    "\n",
    "    # GENERATE NO DATA MASK\n",
    "    # where offset searching will be skipped based on \n",
    "    # 1) imported nodata mask and/or 2) zero values in the image\n",
    "    for ii in range(obj.xGrid.shape[0]):\n",
    "        for jj in range(obj.xGrid.shape[1]):\n",
    "            if (obj.yGrid[ii,jj] != nodata)&(obj.xGrid[ii,jj] != nodata):\n",
    "                if (I1[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]==0)|(I2[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]==0):\n",
    "                    noDataMask[ii,jj] = True\n",
    "                    \n",
    "    # SEARCH RANGE\n",
    "    if SRx0 is None:\n",
    "        # default is a zero array\n",
    "#        ###########     uncomment to customize SearchLimit based on velocity distribution \n",
    "        if Dx0 is not None:\n",
    "            obj.SearchLimitX = np.int32(4+(25-4)/(np.max(np.abs(Dx0[np.logical_not(noDataMask)]))-np.min(np.abs(Dx0[np.logical_not(noDataMask)])))*(np.abs(Dx0)-np.min(np.abs(Dx0[np.logical_not(noDataMask)]))))\n",
    "        else:\n",
    "            obj.SearchLimitX = 15\n",
    "        obj.SearchLimitY = 15\n",
    "#        ###########\n",
    "        obj.SearchLimitX = obj.SearchLimitX * np.logical_not(noDataMask)\n",
    "        obj.SearchLimitY = obj.SearchLimitY * np.logical_not(noDataMask)\n",
    "    else:\n",
    "        obj.SearchLimitX = SRx0\n",
    "        obj.SearchLimitY = SRy0\n",
    "       ############ add buffer to search range\n",
    "        obj.SearchLimitX[obj.SearchLimitX!=0] = obj.SearchLimitX[obj.SearchLimitX!=0] + 2\n",
    "        obj.SearchLimitY[obj.SearchLimitY!=0] = obj.SearchLimitY[obj.SearchLimitY!=0] + 2\n",
    "    \n",
    "    # CHIP SIZE\n",
    "    if CSMINx0 is not None:\n",
    "        obj.ChipSizeMaxX = CSMAXx0\n",
    "        obj.ChipSizeMinX = CSMINx0\n",
    "        \n",
    "        gridspacingx = MINCHIPSIZE # use the grid spacing from above\n",
    "        chipsizex0 = MINCHIPSIZE\n",
    "        pixsizex = trans[1] # grab from coregister function\n",
    "    \n",
    "        obj.ChipSize0X = int(np.ceil(chipsizex0/pixsizex/4)*4)\n",
    "        obj.GridSpacingX = int(obj.ChipSize0X*gridspacingx/chipsizex0)\n",
    "\n",
    "        RATIO_Y2X = CSMINy0/CSMINx0\n",
    "        obj.ScaleChipSizeY = np.median(RATIO_Y2X[(CSMINx0!=nodata)&(CSMINy0!=nodata)])\n",
    "#         obj.ScaleChipSizeY = 1 # USE SCALE OF 1 for square pixels\n",
    "    else:\n",
    "        if ((optflag == 1)&(xGrid is not None)):\n",
    "            obj.ChipSizeMaxX = 32 # pixels\n",
    "            obj.ChipSizeMinX = 16 # pixels\n",
    "            obj.ChipSize0X = 16 # pixels\n",
    "    \n",
    "    # DOWNSTREAM SEARCH OFFSET\n",
    "    if Dx0 is not None:\n",
    "        obj.Dx0 = Dx0\n",
    "        obj.Dy0 = Dy0\n",
    "    else:\n",
    "        obj.Dx0 = obj.Dx0 * np.logical_not(noDataMask)\n",
    "        obj.Dy0 = obj.Dy0 * np.logical_not(noDataMask)\n",
    "\n",
    "    # REPLACE NO DATA VALUES WITH 0\n",
    "    obj.xGrid[noDataMask] = 0\n",
    "    obj.yGrid[noDataMask] = 0\n",
    "    obj.Dx0[noDataMask] = 0\n",
    "    obj.Dy0[noDataMask] = 0\n",
    "    if SRx0 is not None:\n",
    "        obj.SearchLimitX[noDataMask] = 0\n",
    "        obj.SearchLimitY[noDataMask] = 0\n",
    "    if CSMINx0 is not None:\n",
    "        obj.ChipSizeMaxX[noDataMask] = 0\n",
    "        obj.ChipSizeMinX[noDataMask] = 0\n",
    "    \n",
    "    # convert azimuth offset to vertical offset as used in autoRIFT convention for SAR images\n",
    "    if optflag == 0:\n",
    "        obj.Dy0 = -1 * obj.Dy0\n",
    "        \n",
    "    ############## AutoRIFT Pre-processing (from runAutorift()) ############################\n",
    "    t1 = time.time()\n",
    "    print(\"Pre-process Start!!!\")\n",
    "    \n",
    "    # FILTERING:\n",
    "    if FILTER == 'WAL': \n",
    "        obj.preprocess_filt_wal() # WALLIS FILTER\n",
    "#         obj.zeroMask = 1 # removes edges\n",
    "        obj.WallisFilterWidth = WALLISFILTERWIDTH # optional, default supposedly 21\n",
    "    elif FILTER == 'HPS':\n",
    "        obj.preprocess_filt_hps() # HIGH PASS FILTER\n",
    "    elif FILTER == 'SOB':\n",
    "        obj.preprocess_filt_sob() # SOBEL FILTER\n",
    "    elif FILTER == 'LAP':\n",
    "        obj.preprocess_filt_lap()\n",
    "    elif FILTER == 'DB':\n",
    "        obj.preprocess_db() # LOGARITHMIC OPERATOR (NO FILTER), FOR TOPOGRAPHY\n",
    "    else:\n",
    "        print(FILTER, 'not recognized. Using default high pass filter instead.')\n",
    "        obj.preprocess_filt_hps() # HIGH PASS FILTER\n",
    "        \n",
    "    print(\"Pre-process Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # CONVERT TO UNIFORM DATA TYPE\n",
    "    t1 = time.time()\n",
    "#    obj.DataType = 0\n",
    "    obj.uniform_data_type()\n",
    "    print(\"Uniform Data Type Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # OTHER :\n",
    "    obj.sparseSearchSampleRate = 1\n",
    "#    obj.colfiltChunkSize = 4\n",
    "\n",
    "    obj.OverSampleRatio = 64\n",
    "    if CSMINx0 is not None:\n",
    "        obj.OverSampleRatio = {obj.ChipSize0X:16,obj.ChipSize0X*2:32,obj.ChipSize0X*4:64,obj.ChipSize0X*8:64}\n",
    "    \n",
    "    # SEE ORIGINAL CODE TO EXPORT PREPROCESSED IMAGES\n",
    "    \n",
    "    ####################### Run AutoRIFT (from runAutorift())  ############################\n",
    "    t1 = time.time()\n",
    "    print(\"AutoRIFT Start!!!\")\n",
    "    obj.runAutorift()\n",
    "    print(\"AutoRIFT Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    noDataMask = cv2.dilate(noDataMask.astype(np.uint8),kernel,iterations = 1)\n",
    "    noDataMask = noDataMask.astype(np.bool)\n",
    "\n",
    "    # AT THIS POINT, THESE VARIABLES WILL BE CREATED:\n",
    "    # obj.Dx, obj.Dy, obj.InterpMask, obj.ChipSizeX, obj.GridSpacingX, \n",
    "    # obj.ScaleChipSizeY, obj.SearchLimitX, obj.SearchLimitY, obj.origSize, noDataMask\n",
    "    \n",
    "    # PLOT RESULTS\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15,5))\n",
    "    im1 = ax1.imshow(obj.Dx); ax1.set_title('Dx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(obj.Dy); ax2.set_title('Dy'); fig.colorbar(im2, ax=ax2)\n",
    "    im3 = ax3.imshow(np.sqrt((obj.Dx**2) + (obj.Dy**2))); ax3.set_title('D_total'); fig.colorbar(im3,ax=ax3)\n",
    "    plt.suptitle(ds1+' to '+ds2)\n",
    "    plt.show()\n",
    "\n",
    "    ####################### Write outputs (from runAutorift())  ############################\n",
    "    t1 = time.time()\n",
    "    print(\"Write Outputs Start!!!\")\n",
    "          \n",
    "    # Write text file with parameters\n",
    "    f =  open(out_path+'parameters_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.txt', 'w')\n",
    "    f.write('Geogrid/AutoRIFT parameters for offset_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.tif:')\n",
    "    f.write('NO_DATA_VAL: '+str(NO_DATA_VAL))\n",
    "    f.write('Min chip size: '+str(MINCHIPSIZE))\n",
    "    f.write('DEM: '+dem)\n",
    "    f.write('dhdx: '+dhdx); f.write('dhdy: '+dhdy)\n",
    "    f.write('vx: '+vx); f.write('vy: '+vy)\n",
    "    f.write('srx: '+srx); f.write('sry: '+sry)\n",
    "    f.write('csminx: '+csminx); f.write('csminy: '+csminy)\n",
    "    f.write('csmaxx: '+csmaxx); f.write('csmaxy: '+csmaxy)\n",
    "    f.write('stable surface mask: '+ssm)\n",
    "    f.write('FILTER: '+FILTER)\n",
    "    f.write('WALLISFILTERWIDTH: '+str(WALLISFILTERWIDTH))\n",
    "    f.write('Spare search sample rate: '+str(SPARSE_SEARCH_SAMPLE_RATE))\n",
    "    f.write('Oversample ratio: '+str(OVERSAMPLE_RATIO))\n",
    "    if offset2vx is not None and offset2vy is not None:\n",
    "        f.write('Velocity.TIF file created.')\n",
    "    else:\n",
    "        f.write('Velocity.TIF not created.')\n",
    "    f.close() # close the parameter text file\n",
    "          \n",
    "    # open the window_location.tif file to gdalinfo\n",
    "    ds = gdal.Open(gp+'window_location.tif')\n",
    "    tran = ds.GetGeoTransform()\n",
    "    proj = ds.GetProjection()\n",
    "    srs = ds.GetSpatialRef()\n",
    "    \n",
    "    # initialize arrays\n",
    "    DX = np.zeros(origSize,dtype=np.float32) * np.nan; DY = np.zeros(origSize,dtype=np.float32) * np.nan\n",
    "    INTERPMASK = np.zeros(origSize,dtype=np.float32); CHIPSIZEX = np.zeros(origSize,dtype=np.float32)\n",
    "    SEARCHLIMITX = np.zeros(origSize,dtype=np.float32); SEARCHLIMITY = np.zeros(origSize,dtype=np.float32)\n",
    "    \n",
    "    # fill in arays\n",
    "    Dx = obj.Dx; Dy = obj.Dy; InterpMask = obj.InterpMask; ChipSizeX = obj.ChipSizeX\n",
    "    SearchLimitX = obj.SearchLimitX; SearchLimitY = obj.SearchLimitY\n",
    "    DX[0:Dx.shape[0],0:Dx.shape[1]] = Dx;  DY[0:Dy.shape[0],0:Dy.shape[1]] = Dy\n",
    "    INTERPMASK[0:InterpMask.shape[0],0:InterpMask.shape[1]] = InterpMask\n",
    "    CHIPSIZEX[0:ChipSizeX.shape[0],0:ChipSizeX.shape[1]] = ChipSizeX\n",
    "    SEARCHLIMITX[0:SearchLimitX.shape[0],0:SearchLimitX.shape[1]] = SearchLimitX\n",
    "    SEARCHLIMITY[0:SearchLimitY.shape[0],0:SearchLimitY.shape[1]] = SearchLimitY\n",
    "    \n",
    "    # mask out no data\n",
    "    DX[noDataMask] = np.nan; DY[noDataMask] = np.nan\n",
    "    INTERPMASK[noDataMask] = 0; CHIPSIZEX[noDataMask] = 0\n",
    "    SEARCHLIMITX[noDataMask] = 0; SEARCHLIMITY[noDataMask] = 0\n",
    "    if SSM is not None:\n",
    "        SSM[noDataMask] = False\n",
    "    DX[SEARCHLIMITX == 0] = np.nan; DY[SEARCHLIMITX == 0] = np.nan\n",
    "    INTERPMASK[SEARCHLIMITX == 0] = 0; CHIPSIZEX[SEARCHLIMITX == 0] = 0\n",
    "    if SSM is not None:\n",
    "        SSM[SEARCHLIMITX == 0] = False\n",
    "\n",
    "    # SAVE TO OFFSET.MAT FILE\n",
    "    sio.savemat('offset_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.mat', # offset mat filename\n",
    "                {'Dx':DX,'Dy':DY,'InterpMask':INTERPMASK,'ChipSizeX':CHIPSIZEX})\n",
    "    print('Offset.mat written.')\n",
    "    \n",
    "    # CREATE THE GEOTIFFS\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # OFFSET.TIF\n",
    "    outRaster = driver.Create(\"offset_\"+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+\".tif\", # offset filename\n",
    "                              int(xGrid.shape[1]), int(xGrid.shape[0]), 5, gdal.GDT_Float32)\n",
    "    outRaster.SetGeoTransform(tran); outRaster.SetProjection(proj) # projections\n",
    "    outband = outRaster.GetRasterBand(1); outband.WriteArray(DX) # DX\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(2); outband.WriteArray(DY) # DY\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(3); outband.WriteArray(np.sqrt((DX**2) + (DY**2))) # DY\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(4); outband.WriteArray(INTERPMASK) # INTERPMASK\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(5); outband.WriteArray(CHIPSIZEX) # CHIPSIZE\n",
    "    outband.FlushCache()\n",
    "    del outRaster\n",
    "    print('Offset.tif written.')\n",
    "    \n",
    "    # VELOCITY.TIF\n",
    "    if offset2vx is not None and offset2vy is not None:\n",
    "        ds = gdal.Open(offset2vx) #### VX\n",
    "        band = ds.GetRasterBand(1); offset2vx_1 = band.ReadAsArray()\n",
    "        band = ds.GetRasterBand(2); offset2vx_2 = band.ReadAsArray()\n",
    "        if ds.RasterCount > 2:\n",
    "                band = ds.GetRasterBand(3)\n",
    "                offset2vr = band.ReadAsArray()\n",
    "        else:\n",
    "                offset2vr = None\n",
    "        band=None; ds=None\n",
    "        offset2vx_1[offset2vx_1 == nodata] = np.nan\n",
    "        offset2vx_2[offset2vx_2 == nodata] = np.nan\n",
    "\n",
    "        ds = gdal.Open(offset2vy) #### VY\n",
    "        band = ds.GetRasterBand(1); offset2vy_1 = band.ReadAsArray()\n",
    "        band = ds.GetRasterBand(2); offset2vy_2 = band.ReadAsArray()\n",
    "        if ds.RasterCount > 2:\n",
    "                band = ds.GetRasterBand(3)\n",
    "                offset2va = band.ReadAsArray()\n",
    "        else:\n",
    "                offset2va = None\n",
    "        band=None; ds=None\n",
    "        offset2vy_1[offset2vy_1 == nodata] = np.nan; offset2vy_2[offset2vy_2 == nodata] = np.nan\n",
    "        \n",
    "        if offset2va is not None:\n",
    "            offset2va[offset2va == nodata] = np.nan\n",
    "\n",
    "        VX = offset2vx_1 * DX + offset2vx_2 * DY\n",
    "        VY = offset2vy_1 * DX + offset2vy_2 * DY\n",
    "        VX = VX.astype(np.float32); VY = VY.astype(np.float32)\n",
    "\n",
    "        outRaster = driver.Create(\"velocity_\"+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+\".tif\", # velocity filename\n",
    "                                  int(xGrid.shape[1]), int(xGrid.shape[0]), 3, gdal.GDT_Float32)\n",
    "        outRaster.SetGeoTransform(tran); outRaster.SetProjection(proj)\n",
    "        outband = outRaster.GetRasterBand(1); outband.WriteArray(VX) # VX\n",
    "        outband.FlushCache()\n",
    "        outband = outRaster.GetRasterBand(2); outband.WriteArray(VY) # VY\n",
    "        outband.FlushCache()\n",
    "        outband = outRaster.GetRasterBand(3); outband.WriteArray(np.sqrt((VX**2) + (VY**2))) # V\n",
    "        outband.FlushCache()\n",
    "        del outRaster\n",
    "        print('Velocity.tif written.')\n",
    "    \n",
    "    print(\"Write Outputs Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # Move files produced to the out_path directory\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if 'offset' in file or ('velocity' in file and '.tif' in file):\n",
    "            shutil.move(os.getcwd()+'/'+file, out_path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82473ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run autoRIFT with function\n",
    "run_autoRIFT_inhouse(out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                         FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATIO, OVERSAMPLE_RATIO,\n",
    "                         Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                         noDataMask, nodataval, geogrid_run_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a12c25",
   "metadata": {},
   "source": [
    "# Run geogrid and autoRIFT on all images in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6106fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s2path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel2/' # input S2 images\n",
    "LS8path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/LS8images/useable_images/' # input LS8 images\n",
    "PSpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/Planet_test/Planet_test_all/'\n",
    "S1path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/'\n",
    "# PSpath = '/Volumes/SGlacier/surge_projects/Working_clipped_2019/'\n",
    "# boxpath = '/Users/surging/Documents/TG/BoxTurner/BoxTurner_UTM_07.shp' # the shapefile for Turner\n",
    "# autoriftpath = '/Users/surging/Documents/TG/autoRIFT/' # path to the autorift scripts\n",
    "# vmap_path = '/Users/surging/Documents/TG/vmap_test/' # output velocity map folder\n",
    "# basepath = '/Users/surging/Documents/TG/optical-offset-tracking/' # path where this script is located\n",
    "\n",
    "######### Set minimum and maximum time separation and the platform (L8, S2, PS, S1) ###############\n",
    "platform = 'L8'\n",
    "startdate = '20130101' # inclusive start date\n",
    "enddate = '20160101' # inclusive end date\n",
    "min_dt = 3 # minimum time separation between images\n",
    "max_dt = 30 # maximum time separation between images\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87550d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the folder path based on the platform\n",
    "if platform == 'S2': # sentinel-2\n",
    "    path = s2path\n",
    "    ext = '_clipped.tif' # image filename extension\n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 2 # split filename by underscore, index corresponds to image date\n",
    "elif platform == 'L8': # landsat 8\n",
    "    path = LS8path\n",
    "    ext = '.TIF'\n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 3\n",
    "elif platform == 'PS': # PlanetScope\n",
    "    path = PSpath\n",
    "    ext = '_clipped.tif' \n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 1\n",
    "elif platform == 'S1': # sentinel-1\n",
    "    path = S1path\n",
    "    ext = '.zip' \n",
    "    img_type = 'SAR'\n",
    "    date_split_idx = 5\n",
    "else:\n",
    "    print('Platform', platform, 'not recognized. Options are \"S2\", \"L8\", \"PS\", and \"S1\"')\n",
    "\n",
    "# record all possible images and their dates\n",
    "dates = []; files = []  \n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(ext):\n",
    "        date = file.split('_')[date_split_idx] # grab the date from the filename\n",
    "        if platform == 'S1': # S1 filenames need another split\n",
    "            date = date[:8]\n",
    "        dates.append(date); files.append(file) # store the date and filename\n",
    "files_df = pd.DataFrame(list(zip(files,dates)),columns=['filename','date'])\n",
    "files_df = files_df.sort_values(by='date',ignore_index=True) # sort the dataframe by ascending date\n",
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out those before and afte the input start and end date\n",
    "files_df = files_df[(files_df.date >= startdate) & (files_df.date <= enddate)]\n",
    "files_df = files_df.reset_index(drop=True) # reset index for searching\n",
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PARAMETERS FOR GEOGRID #################################################### \n",
    "out_path = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/output_AutoRIFT/' # output file path\n",
    "dem = dempath+dem_outfile # path to the resampled DEM produced in the previous step (outfile name)\n",
    "#             MINCHIPSIZE = CHIPSIZE_M # smallest chip size allowed in image horizontal direction (in m)\n",
    "#             print(MINCHIPSIZE)\n",
    "NO_DATA_VAL = -32767 # no data value in the output products\n",
    "temp_dir = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/coreg_files/'\n",
    "\n",
    "# PARAMETERS FOR AUTORIFT CHOSEN\n",
    "# options are HPS (high pass), WAL (wallis), SOB (sobel), DB (logarithmic operator)\n",
    "FILTER = 'HPS'\n",
    "WALLISFILTERWIDTH = 32 # only for wallis filter, must be a power of 2\n",
    "\n",
    "# Sampling:\n",
    "SPARSE_SEARCH_SAMPLE_RATE = 16 # how many samples to skip to speed up processing\n",
    "OVERSAMPLE_RATIO = 0 # enter in a constant scalar or 0 for default parameters\n",
    "print('Filters and autoRIFT sample rates chosen.')\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71208a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ENTER DEM INFO, AND REFERENCE VELOCITY INFO ########### \n",
    "# enter in the path to your best DEM over the region\n",
    "dempath = '/Users/jukesliu/Documents/TURNER/DATA/ICE_THICKNESS/surface/DEMs_previous/'\n",
    "demname = 'IfSAR_5m_DSM_clipped.tif'\n",
    "\n",
    "# path to the reference files for geogrid (vx, vy, ssm)\n",
    "refvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/forAutoRIFT/' \n",
    "vx_fname = 'vx_cropped.tif' # name of reference vx file\n",
    "vy_fname = 'vy_cropped.tif' # name of reference vy file\n",
    "\n",
    "sr_scaling = 16 # multiply by vx and vy to generate search range limits\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692b7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try multiple chip sizes and sats\n",
    "for platform in ['S2','L8']:\n",
    "    # assign the folder path based on the platform\n",
    "    if platform == 'S2': # sentinel-2\n",
    "        path = s2path\n",
    "        ext = '_clipped.tif' # image filename extension\n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 2 # split filename by underscore, index corresponds to image date\n",
    "    elif platform == 'L8': # landsat 8\n",
    "        path = LS8path\n",
    "        ext = '.TIF'\n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 3\n",
    "    elif platform == 'PS': # PlanetScope\n",
    "        path = PSpath\n",
    "        ext = '_clipped.tif' \n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 1\n",
    "    elif platform == 'S1': # sentinel-1\n",
    "        path = S1path\n",
    "        ext = '.zip' \n",
    "        img_type = 'SAR'\n",
    "        date_split_idx = 5\n",
    "    else:\n",
    "        print('Platform', platform, 'not recognized. Options are \"S2\", \"L8\", \"PS\", and \"S1\"')\n",
    "\n",
    "    # record all possible images and their dates\n",
    "    dates = []; files = []  \n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(ext):\n",
    "            date = file.split('_')[date_split_idx] # grab the date from the filename\n",
    "            if platform == 'S1': # S1 filenames need another split\n",
    "                date = date[:8]\n",
    "            dates.append(date); files.append(file) # store the date and filename\n",
    "    files_df = pd.DataFrame(list(zip(files,dates)),columns=['filename','date'])\n",
    "    files_df = files_df.sort_values(by='date',ignore_index=True) # sort the dataframe by ascending date\n",
    "    files_df = files_df[(files_df.date >= startdate) & (files_df.date <= enddate)]\n",
    "    files_df = files_df.reset_index(drop=True) # reset index for searching\n",
    "    \n",
    "    for min_chipsize in [100,160,200,300]: # MULTIPLE CHIP SIZES\n",
    "        MINCHIPSIZE = min_chipsize\n",
    "        # generate geogrid inputs and grab outfilenames\n",
    "        [dem_outfile, dhdx_outfile, dhdy_outfile, \n",
    "         vx_outfile, vy_outfile, srx_outfile, \n",
    "         sry_outfile, ssm_outfile, tg_outfile] = generate_geogrid_inputs(min_chipsize, dempath, demname, \n",
    "                                                                         refvpath, vx_fname, vy_fname, \n",
    "                                                                         sr_scaling)\n",
    "        #######################################################################\n",
    "        # # CHOOSE OPTIONAL INPUTS: (set as '' to leave blank)\n",
    "        # # 1) surface slope:\n",
    "        #   dhdx = ''; dhdy = ''  \n",
    "        dhdx = dempath+dhdx_outfile; dhdy = dempath+dhdy_outfile\n",
    "        \n",
    "        # # 2) reference velocity:\n",
    "        vx = ''; vy = ''    \n",
    "#         vx = refvpath+vx_outfile; vy = refvpath+vy_outfile\n",
    "        \n",
    "        # # 3) chip sizes:\n",
    "        csminx = ''; csminy = ''\n",
    "        csmaxx = ''; csmaxy = ''  \n",
    "        #             csminx = refvpath+csminx_fname; csminy = refvpath+csminy_fname\n",
    "        #             csmaxx = refvpath+csmaxx_fname; csmaxy = refvpath+csmaxy_fname\n",
    "        \n",
    "        # # 4) stable surface mask:\n",
    "        ssm = ''\n",
    "        #             ssm = refvpath+ssm_outfile # stable surface mask \n",
    "        \n",
    "        # # 5) search range limit:\n",
    "        srx = ''; sry = '' # for best results, never input search range limit\n",
    "        #######################################################################\n",
    "        \n",
    "        for rownum in range(0,len(files_df)-1):\n",
    "            if rownum == 0: # for the first row, idx1 = 0 and idx2 = 1\n",
    "                idx1 = rownum\n",
    "            idx2 = idx1+1 # reset index 2 as the next item \n",
    "\n",
    "            if idx1 < len(files_df) and idx2 < len(files_df): # don't surpass the end of the data\n",
    "                # identify the successive image pairs:\n",
    "                m = files_df.loc[idx1,'filename']; s = files_df.loc[idx2, 'filename']\n",
    "\n",
    "                # grab the two dates and convert to datetime objects\n",
    "                d1s = m.split('_')[date_split_idx]; d2s = s.split('_')[date_split_idx]\n",
    "                if platform == 'S1': # S1 filenames need another split\n",
    "                    d1s = d1s[:8]; d2s = d2s[:8]\n",
    "                \n",
    "                # Skip if output already exists for this combination of parameters\n",
    "#                 if os.path.exists(out_path+'offset_'+d1s+'_'+d2s+'_'+str(min_chipsize)+'m_'+platform+'.tif'):\n",
    "#                     print('offset_'+d1s+'_'+d2s+'_'+str(min_chipsize)+'m_'+platform+'.tif already exists.')\n",
    "#                 else:\n",
    "                if True:\n",
    "                    # calculate time separation\n",
    "                    d1 = datetime.datetime.strptime(d1s, '%Y%m%d'); d2 = datetime.datetime.strptime(d2s, '%Y%m%d')\n",
    "                    dt = d2-d1; dt = int(dt.days)\n",
    "\n",
    "                    # evaluate if dates are within the appropriate time separation\n",
    "                    while (dt < min_dt or dt > max_dt): # dt is outside of appropriate time separation\n",
    "                        idx2 += 1 # increment index 2\n",
    "\n",
    "                        if idx2 > len(files_df)-1:\n",
    "                            print('No image pair with correct time separation found. Skip to next.')\n",
    "                            idx2 = idx1+1 # reset index 2 as the next item \n",
    "                            m = None; s = None\n",
    "                            break\n",
    "\n",
    "                        # Keep looking for the correct time separation:\n",
    "                        m = files_df.loc[idx1,'filename']; s = files_df.loc[idx2, 'filename']\n",
    "\n",
    "                        # grab the two dates and convert to datetime objects\n",
    "                        d1s = m.split('_')[date_split_idx]; d2s = s.split('_')[date_split_idx]\n",
    "                        if platform == 'S1': # S1 filenames need another split\n",
    "                            d1s = d1s[:8]; d2s = d2s[:8]\n",
    "                        d1 = datetime.datetime.strptime(d1s, '%Y%m%d'); d2 = datetime.datetime.strptime(d2s, '%Y%m%d')\n",
    "                        dt = d2-d1; dt = int(dt.days)\n",
    "\n",
    "                    # run geogrid and autoRIFT\n",
    "                    if m is not None and s is not None:\n",
    "                        print(m,s)\n",
    "                        indir_m = path+m; indir_s = path+s # path to the two images\n",
    "                        dem = dempath+'IfSAR_'+str(MINCHIPSIZE)+'m_DSM_clipped.tif'\n",
    "                        dhdx = dempath+'IfSAR_'+str(MINCHIPSIZE)+'m_DSM_clipped_dhdx.tif'\n",
    "                        dhdy = dempath+'IfSAR_'+str(MINCHIPSIZE)+'m_DSM_clipped_dhdy.tif'\n",
    "                        ###################### RUN GEOGRID ################################\n",
    "                        run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                                            dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                                            temp_dir)\n",
    "\n",
    "                        ##################### PREP AUTORIFT ##############################\n",
    "                        gp = out_path # identify files produced from geogrid\n",
    "                        # remove all empty grids\n",
    "                        for grid in os.listdir(gp): \n",
    "                            if grid.startswith('window') and grid.endswith('.tif'):\n",
    "                                reader = rio.open(gp+grid) # read dataset\n",
    "                                data_found = False \n",
    "                                for band in range(1,reader.count+1):\n",
    "                                    testband = reader.read(band) # read in the band\n",
    "                                    if np.count_nonzero(testband[testband != NO_DATA_VAL]) > 0:\n",
    "                                        data_found = True\n",
    "                                if not data_found:\n",
    "                                    print(grid, 'has no data. Removed.')\n",
    "                                    os.remove(gp+grid)\n",
    "\n",
    "                        # fill in AutoRIFT parameters using the files\n",
    "                        mpflag = 0 # leave multiprocessing off\n",
    "\n",
    "                        # GRID LOCATION (required) from window_location.tif\n",
    "                        grid_location = rio.open(gp+'window_location.tif')\n",
    "                        xGrid = grid_location.read(1) # 1st band in window location\n",
    "                        yGrid = grid_location.read(2) # 2nd band in window location\n",
    "\n",
    "                        # optional parameters (default None or zero until filled)\n",
    "                        init_offset = None; search_range = None\n",
    "                        chip_size_min = None; chip_size_max = None\n",
    "                        offset2vx = None; offset2vy = None; stable_surface_mask = None\n",
    "                        Dx0 = None; Dy0 = None; CSMINx0 = None\n",
    "                        SRx0 = None; SRy0 = None;\n",
    "                        CSMAXx0 = None; CSMAXy0 = None; SSM = None\n",
    "                        noDataMask = np.zeros(xGrid.shape).astype(int)\n",
    "\n",
    "                        if os.path.exists(gp+'window_offset.tif'): # Dx0 and Dy0 from window_offset.tif\n",
    "                            init_offset = rio.open(gp+'window_offset.tif')\n",
    "                            Dx0 = init_offset.read(1); Dy0 = init_offset.read(2)\n",
    "                        if os.path.exists(gp+'window_search_range.tif'): # SRx0 and SRy0 from window_search_range.tif\n",
    "                            search_range = rio.open(gp+'window_search_range.tif')\n",
    "                            SRx0 = search_range.read(1); SRy0 = search_range.read(2)\n",
    "                        if os.path.exists(gp+'window_chip_size_min.tif'): # CSMINx0 and CSMINy0 from window_chip_size_min.tif\n",
    "                            chip_size_min = rio.open(gp+'window_chip_size_min.tif')\n",
    "                            CSMINx0 = chip_size_min.read(1); CSMINy0 = chip_size_min.read(2)\n",
    "                        if os.path.exists(gp+'window_chip_size_max.tif'): # CSMAXx0 and CSMAXy0 from window_chip_size_max.tif\n",
    "                            chip_size_max = rio.open(gp+'window_chip_size_max.tif')\n",
    "                            CSMAXx0 = chip_size_max.read(1); CSMAXy0 = chip_size_max.read(2)\n",
    "                        if os.path.exists(gp+'window_rdr_off2vel_x_vec.tif'): # offset2vx from window_rdr_off2vel_x_vec.tif\n",
    "                            offset2vx = gp+'window_rdr_off2vel_x_vec.tif' # path to be read in with GDAL\n",
    "                        if os.path.exists(gp+'window_rdr_off2vel_y_vec.tif'): # offset2vy from window_rdr_off2vel_y_vec.tif\n",
    "                            offset2vy = gp+'window_rdr_off2vel_y_vec.tif' \n",
    "                        if os.path.exists(gp+'window_stable_surface_mask.tif'): # noDataMask from window_stable_surface_mask.tif\n",
    "                            stable_surface_mask = rio.open(gp+'window_stable_surface_mask.tif')\n",
    "                            noDataMask = stable_surface_mask.read(1)\n",
    "\n",
    "                        # other parameters\n",
    "                        nodata = NO_DATA_VAL # use same as in previous steps\n",
    "                        geogrid_run_info=None\n",
    "                        print('AutoRIFT parameters loaded.')\n",
    "\n",
    "                        ##################### RUN AUTORIFT ##############################\n",
    "                        # run autoRIFT with function\n",
    "                        run_autoRIFT_inhouse(out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                                                 FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATE, \n",
    "                                                 OVERSAMPLE_RATIO, MINCHIPSIZE,\n",
    "                                                 Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                                                 noDataMask, nodata, geogrid_run_info)\n",
    "\n",
    "                idx1 = idx2 # set the second image index as the new first image index\n",
    "                if idx2 >= len(files_df)-1:\n",
    "                    print('Finished searching.')\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f72562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40803436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newautoriftenv",
   "language": "python",
   "name": "newautoriftenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
