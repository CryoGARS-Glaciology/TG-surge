{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to map velocities using autoRIFT with adjustable parameters\n",
    "\n",
    "_Last modified on 2022-05-17 by jukesliu@u.boisestate.edu._\n",
    "\n",
    "Majority of the code is borrowed from testGeogrid_ISCE.py and testautoRIFT_ISCE.py from the autoRIFT GitHub repository (https://github.com/nasa-jpl/autoRIFT/). Requires ISCE and autoRIFT to be installed.\n",
    "\n",
    "Change the kernel to __newautoriftenv__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Open Source version of ISCE.\n",
      "Some of the workflows depend on a separate licensed package.\n",
      "To obtain the licensed package, please make a request for ISCE\n",
      "through the website: https://download.jpl.nasa.gov/ops/request/index.cfm.\n",
      "Alternatively, if you are a member, or can become a member of WinSAR\n",
      "you may be able to obtain access to a version of the licensed sofware at\n",
      "https://winsar.unavco.org/software/isce\n"
     ]
    }
   ],
   "source": [
    "import rasterio as rio\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "import isce\n",
    "from contrib.geo_autoRIFT.geogrid import Geogrid, GeogridOptical\n",
    "from iscesys.Component.ProductManager import ProductManager as PM\n",
    "from isceobj.Orbit.Orbit import Orbit\n",
    "from osgeo import gdal, osr\n",
    "import struct\n",
    "import re\n",
    "from datetime import date\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request\n",
    "from components.contrib.geo_autoRIFT.autoRIFT import autoRIFT_ISCE\n",
    "import isceobj\n",
    "import time\n",
    "import subprocess\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "\n",
    "gdal.AllRegister() # register all GDAL drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Resample DEM and other autoRIFT/geogrid input rasters to the desired chip size\n",
    "\n",
    "Recommended chip size is >= 16*pixel_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizable parameters for geogrid:\n",
    "\n",
    "    dhdx, dhdy:              x/y local surface slope maps (unitless)\n",
    "    vx,vy:                   x/y reference velocity maps (in units of m/yr)\n",
    "    srx, sry:                x/y velocity search range limit maps (in units of m/yr)\n",
    "    csminx, csminy:          x/y chip size minimum maps (in units of m; constant ratio between x and y)\n",
    "    csmaxx, csmaxy:          x/y chip size maximum maps (in units of m; constant ratio between x and y)\n",
    "    ssm:                     stable surface mask (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rio_write(out_path, nparray, ref_raster, grid_spacing):\n",
    "    # Function to write a numpy array to a Geotiff using rasterio\n",
    "    # The geotiff will have the same bounds and crs as the reference raster\n",
    "    # An evenly-spaced grid will be created with the grid spacing entered\n",
    "    #\n",
    "    # INPUTS:\n",
    "    #   out_path: the path with name of the output gtiff file\n",
    "    #   nparray: the nparray to write to gtiff\n",
    "    #   ref_raster: the reference raster (we borrow its crs and bounding coordinates)\n",
    "    #   grid_spacing: the spatial resolution of the output raster\n",
    "    \n",
    "    import rasterio as rio\n",
    "    import numpy as np\n",
    "    nparray = np.array(nparray) # make sure it's an np array\n",
    "    \n",
    "    with rio.open(out_path,'w',\n",
    "                      driver='GTiff',\n",
    "                      height=nparray.shape[0], # new shape\n",
    "                      width=nparray.shape[1], # new shape\n",
    "                      dtype=nparray.dtype, # data type\n",
    "                      count=1,\n",
    "                      crs=ref_raster.crs, # the EPSG from the original DEM\n",
    "                      transform=rio.Affine(grid_spacing, 0.0, ref_raster.bounds.left, # modified transform\n",
    "                                           0.0, -grid_spacing, ref_raster.bounds.top)) as dst:\n",
    "            dst.write(nparray, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ENTER CHIP SIZE, DEM INFO, AND REFERENCE VELOCITY INFO ########### \n",
    "CHIPSIZE_M = 100 # enter in desired grid size in meters (default is 32 pixels)\n",
    "\n",
    "# enter in the path to your best DEM over the region\n",
    "dempath = '/Users/jukesliu/Documents/TURNER/DATA/ICE_THICKNESS/surface/DEMs_previous/'\n",
    "demname = 'IfSAR_5m_DSM_clipped.tif'\n",
    "\n",
    "# path to the reference files for geogrid (vx, vy, ssm)\n",
    "refvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/forAutoRIFT/' \n",
    "vx_fname = 'vx_cropped.tif' # name of reference vx file\n",
    "vy_fname = 'vy_cropped.tif' # name of reference vy file\n",
    "\n",
    "sr_scaling = 16 # multiply by vx and vy to generate search range limits\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_geogrid_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ONE CHIPSIZE:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_geogrid_inputs\u001b[49m(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_geogrid_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# ONE CHIPSIZE:\n",
    "generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling)\n",
    "\n",
    "# # MANY CHIPSIZES:\n",
    "# for CHIPSIZE_M in [100,150,160,200,300, 350, 400]:\n",
    "#     generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling)\n",
    "#     print(); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling):\n",
    "    \n",
    "    # GRAB DEM INFO\n",
    "    refdem = rio.open(dempath+demname) # open DEM using rasterio\n",
    "    elev = refdem.read(1) # read in the first and only band (elevations)\n",
    "\n",
    "    # grab the x and y grid values from the DEM:\n",
    "    dem_x = np.linspace(refdem.bounds.left, refdem.bounds.right, num=np.shape(elev)[1])\n",
    "    dem_y = np.linspace(refdem.bounds.top, refdem.bounds.bottom, num=np.shape(elev)[0])\n",
    "\n",
    "    # grab the resampled x and y grid values from the DEM\n",
    "    new_x = np.arange(refdem.bounds.left, refdem.bounds.right, CHIPSIZE_M)\n",
    "    new_y = np.arange(refdem.bounds.top, refdem.bounds.bottom, -CHIPSIZE_M)\n",
    "    \n",
    "    # RESAMPLE THE DEM\n",
    "    dem_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped.tif' # generate new filename\n",
    "    if not os.path.exists(dempath+dem_outfile): # if the resampled DEM does not exist already\n",
    "        # Create thew new x and y grid values using DEM bounds and the chipsize\n",
    "        dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "        print(dem_resamp.shape)\n",
    "\n",
    "        # Resample to your new DEM bounds\n",
    "        f = interp2d(dem_x, dem_y, elev) # create DEM interpolation object\n",
    "        dem_resamp = f(new_x,new_y) # resample the NIR data to the DSM coordinates\n",
    "        dem_resamp = np.flipud(dem_resamp) # flip up down\n",
    "        print(\"Resampled to new dimensions:\",dem_resamp.shape)\n",
    "\n",
    "        # Display the two DEMs as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(elev, cmap='Greys_r', vmin=0)\n",
    "        ax1.set_title('Original DEM: '+str(refdem.transform[0])+' m') # original spatial resolution\n",
    "        fig.colorbar(im1, ax=ax1,label='Elevation [m]')\n",
    "\n",
    "        im2 = ax2.imshow(dem_resamp, cmap='Greys_r', vmin=0)\n",
    "        ax2.set_title('Resampled DEM: '+str(CHIPSIZE_M)+' m') # new spatial resolution\n",
    "        fig.colorbar(im2, ax=ax2,label='Elevation [m]')\n",
    "        plt.show()\n",
    "\n",
    "        # Save the resampled DEM to georeferenced tif file\n",
    "        print(\"Save resampled DEM to\", dempath+dem_outfile)\n",
    "        rio_write(dempath+dem_outfile, dem_resamp, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        # load the existing resampled DEM\n",
    "        dem_r = rio.open(dempath+dem_outfile) # open DEM using rasterio\n",
    "        dem_resamp = dem_r.read(1) # read in the first and only band (elevations)\n",
    "        print(dem_outfile, ' already exists.')\n",
    "    \n",
    "    # CREATE DHDX, DHDY\n",
    "    dhdx_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdx.tif' # generate new filename\n",
    "    dhdy_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdy.tif' # generate new filename\n",
    "    if not os.path.exists(dempath+dhdx_outfile) or not os.path.exists(dempath+dhdy_outfile): # if either is missing\n",
    "        # Produce dhdx and dhdy maps from resampled DEM\n",
    "        dhdx = np.gradient(dem_resamp, axis=1)/CHIPSIZE_M\n",
    "        dhdy = np.gradient(dem_resamp, axis=0)/CHIPSIZE_M\n",
    "\n",
    "        # Filter out borders with high gradient values\n",
    "        grad_thresh = 5\n",
    "        dhdx[abs(dhdx) > grad_thresh] = 0; dhdy[abs(dhdy) > grad_thresh] = 0\n",
    "\n",
    "        # absolute value of the max gradient values expected:\n",
    "        dhmax = 1\n",
    "\n",
    "        # Display the two DEMs as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(dhdx, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "        ax1.set_title('dhdx') # surface slope x\n",
    "        fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "        im2 = ax2.imshow(dhdy, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "        ax2.set_title('dhdy') # surface slope y\n",
    "        fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # Save the gradient maps to tif files\n",
    "        print(\"Save surface slope maps to\", dempath)\n",
    "        rio_write(dempath+dhdx_outfile, dhdx, refdem, CHIPSIZE_M) # dhdx\n",
    "        rio_write(dempath+dhdy_outfile, dhdy, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        print(dhdy_outfile, 'and', dhdx_outfile, 'already exist.')\n",
    "\n",
    "    # VX, VY, SRX, SRY\n",
    "    vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    if not os.path.exists(refvpath+vx_outfile) or not os.path.exists(refvpath+vy_outfile): # if either vx, vy missing\n",
    "        # open the files with rasterio\n",
    "        vx_reader = rio.open(refvpath+vx_fname); vx0 = vx_reader.read(1)\n",
    "        vy_reader = rio.open(refvpath+vy_fname); vy0 = vy_reader.read(1)\n",
    "        vx_x = np.linspace(vx_reader.bounds.left, vx_reader.bounds.right, num=np.shape(vx0)[1])\n",
    "        vx_y = np.linspace(vx_reader.bounds.top, vx_reader.bounds.bottom, num=np.shape(vx0)[0])\n",
    "        vy_x = np.linspace(vy_reader.bounds.left, vy_reader.bounds.right, num=np.shape(vy0)[1])\n",
    "        vy_y = np.linspace(vy_reader.bounds.top, vy_reader.bounds.bottom, num=np.shape(vy0)[0])\n",
    "\n",
    "        # Resample to the DEM grid\n",
    "        fx = interp2d(vx_x, vx_y, vx0)\n",
    "        fy = interp2d(vy_x, vy_y, vy0)\n",
    "        vx_resamp = np.flipud(fx(new_x,new_y)) \n",
    "        vy_resamp = np.flipud(fy(new_x,new_y)) # flip up down\n",
    "\n",
    "        # Display the two velocity files as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(vx_resamp, cmap='Greys_r'); ax1.set_title('vx'); fig.colorbar(im1, ax=ax1)\n",
    "        im2 = ax2.imshow(vy_resamp, cmap='Greys_r'); ax2.set_title('vy'); fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # CALCULATE SEARCH RANGE LIMITS MULTIPLY VX AND VY BY SOME NUMBER\n",
    "        srx_resamp = vx_resamp*sr_scaling; sry_resamp = vy_resamp*sr_scaling\n",
    "\n",
    "        # Display the two search range files as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(srx_resamp, cmap='Greys_r'); ax1.set_title('srx'); fig.colorbar(im1, ax=ax1)\n",
    "        im2 = ax2.imshow(sry_resamp, cmap='Greys_r'); ax2.set_title('sry'); fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # save the reference velocity and search range maps\n",
    "        rio_write(refvpath+vx_outfile, vx_resamp, refdem, CHIPSIZE_M) # vx\n",
    "        rio_write(refvpath+vy_outfile, vy_resamp, refdem, CHIPSIZE_M) # vy\n",
    "        rio_write(refvpath+srx_outfile, srx_resamp, refdem, CHIPSIZE_M) # srx\n",
    "        rio_write(refvpath+sry_outfile, sry_resamp, refdem, CHIPSIZE_M) # sry\n",
    "    else:\n",
    "        print(vx_outfile, ',', vy_outfile, ',', srx_outfile, ',', sry_outfile, 'already exist.')  \n",
    "    \n",
    "    # MASKS\n",
    "    ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    tg_outfile = 'TG_mask_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    tg_mt_outfile = 'TG_mask_MT_'+str(CHIPSIZE_M)+'m.tif'\n",
    "    tg_nt_outfile = 'TG_mask_NT_'+str(CHIPSIZE_M)+'m.tif'\n",
    "    tg_st_outfile = 'TG_mask_ST_'+str(CHIPSIZE_M)+'m.tif'\n",
    "\n",
    "    if True: # overwrite all\n",
    "        # read it in, process (resample, mask, etc.) and resave\n",
    "        ssmreader = rio.open(refvpath+'ice_mask_200mbuffer.tif')\n",
    "        ssm = ssmreader.read(1)\n",
    "        ssm[ssm > 0] = 1; #ssm[ssm < 0.0] = 0; # make binary\n",
    "        ssm = ssm < 1 # find all stable areas (where.tif = 0)\n",
    "\n",
    "        # do the same for Turner Glacier mask\n",
    "        tgreader = rio.open(refvpath+'TG_mask.tif'); tg_mask = tgreader.read(1)\n",
    "        tg_mask[tg_mask > 0] = 1; tg_mask = tg_mask > 0\n",
    "        \n",
    "        # and all the regional masks\n",
    "        tg_mt_reader = rio.open(refvpath+'TG_mask_MT.tif'); tg_mt = tg_mt_reader.read(1)\n",
    "        tg_mt[tg_mt > 0] = 1; tg_mt = tg_mt > 0\n",
    "        tg_nt_reader = rio.open(refvpath+'TG_mask_NT.tif'); tg_nt = tg_nt_reader.read(1)\n",
    "        tg_nt[tg_nt > 0] = 1; tg_nt = tg_nt > 0\n",
    "        tg_st_reader = rio.open(refvpath+'TG_mask_ST.tif'); tg_st = tg_st_reader.read(1)\n",
    "        tg_st[tg_st > 0] = 1; tg_st = tg_st > 0\n",
    "        \n",
    "#         fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(10,5)); ax1.imshow(tg_mt); ax1.set_title('main truk')\n",
    "#         ax2.imshow(tg_nt); ax2.set_title('N tributary'); ax3.imshow(tg_st); ax3.set_title('S tributary')\n",
    "#         plt.show()\n",
    "\n",
    "        # grab x and y-values\n",
    "        ssm_x = np.linspace(ssmreader.bounds.left, ssmreader.bounds.right, num=np.shape(ssm)[1])\n",
    "        ssm_y = np.linspace(ssmreader.bounds.top, ssmreader.bounds.bottom, num=np.shape(ssm)[0])\n",
    "        tg_x = np.linspace(tgreader.bounds.left, tgreader.bounds.right, num=np.shape(tg_mask)[1])\n",
    "        tg_y = np.linspace(tgreader.bounds.top, tgreader.bounds.bottom, num=np.shape(tg_mask)[0])\n",
    "        tg_mtx = np.linspace(tg_mt_reader.bounds.left, tg_mt_reader.bounds.right, num=np.shape(tg_mt)[1])\n",
    "        tg_mty = np.linspace(tg_mt_reader.bounds.top, tg_mt_reader.bounds.bottom, num=np.shape(tg_mt)[0])\n",
    "\n",
    "        # Resample to the DEM grid\n",
    "        f_ssm = interp2d(ssm_x, ssm_y, ssm)\n",
    "        f_tg = interp2d(tg_x, tg_y, tg_mask)\n",
    "        f_tg_mt = interp2d(tg_mtx, tg_mty, tg_mt)\n",
    "        f_tg_nt = interp2d(tg_mtx, tg_mty, tg_nt)\n",
    "        f_tg_st = interp2d(tg_mtx, tg_mty, tg_st)\n",
    "        \n",
    "        ssm_resamp = np.flipud(f_ssm(new_x,new_y))\n",
    "        tg_resamp = np.flipud(f_tg(new_x, new_y))\n",
    "        tg_mt_resamp = np.flipud(f_tg_mt(new_x, new_y))\n",
    "        tg_nt_resamp = np.flipud(f_tg_nt(new_x, new_y))\n",
    "        tg_st_resamp = np.flipud(f_tg_st(new_x, new_y))\n",
    "        \n",
    "        # plot\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ssm_im = ax.imshow(ssm_resamp,cmap='gray',vmin=0)\n",
    "        ax.set_title('Stable Surface Mask')\n",
    "        fig.colorbar(ssm_im, ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "        # export\n",
    "        rio_write(refvpath+ssm_outfile, ssm_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_outfile, tg_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_mt_outfile, tg_mt_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_nt_outfile, tg_nt_resamp, refdem, CHIPSIZE_M)\n",
    "        rio_write(refvpath+tg_st_outfile, tg_st_resamp, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        print(ssm_outfile,'and',tg_outfile,'already exist.')\n",
    "    \n",
    "    return dem_outfile, dhdx_outfile, dhdy_outfile, vx_outfile, vy_outfile, srx_outfile, sry_outfile, ssm_outfile, tg_outfile\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and grab info from your DEM\n",
    "refdem = rio.open(dempath+demname) # open DEM using rasterio\n",
    "elev = refdem.read(1) # read in the first and only band (elevations)\n",
    "\n",
    "# grab the x and y grid values from the DEM:\n",
    "dem_x = np.linspace(refdem.bounds.left, refdem.bounds.right, num=np.shape(elev)[1])\n",
    "dem_y = np.linspace(refdem.bounds.top, refdem.bounds.bottom, num=np.shape(elev)[0])\n",
    "\n",
    "# grab the resampled x and y grid values from the DEM\n",
    "new_x = np.arange(refdem.bounds.left, refdem.bounds.right, CHIPSIZE_M)\n",
    "new_y = np.arange(refdem.bounds.top, refdem.bounds.bottom, -CHIPSIZE_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IfSAR_100m_DSM_clipped.tif  already exists.\n"
     ]
    }
   ],
   "source": [
    "dem_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(dempath+dem_outfile): # if the resampled DEM does not exist already\n",
    "    # Create thew new x and y grid values using DEM bounds and the chipsize\n",
    "    dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "    print(dem_resamp.shape)\n",
    "    \n",
    "    # Resample to your new DEM bounds\n",
    "    f = interp2d(dem_x, dem_y, elev) # create DEM interpolation object\n",
    "    dem_resamp = f(new_x,new_y) # resample the NIR data to the DSM coordinates\n",
    "    dem_resamp = np.flipud(dem_resamp) # flip up down\n",
    "    print(\"Resampled to new dimensions:\",dem_resamp.shape)\n",
    "    \n",
    "    # Display the two DEMs as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(elev, cmap='Greys_r', vmin=0)\n",
    "    ax1.set_title('Original DEM: '+str(refdem.transform[0])+' m') # original spatial resolution\n",
    "    fig.colorbar(im1, ax=ax1,label='Elevation [m]')\n",
    "\n",
    "    im2 = ax2.imshow(dem_resamp, cmap='Greys_r', vmin=0)\n",
    "    ax2.set_title('Resampled DEM: '+str(CHIPSIZE_M)+' m') # new spatial resolution\n",
    "    fig.colorbar(im2, ax=ax2,label='Elevation [m]')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the resampled DEM to georeferenced tif file\n",
    "    print(\"Save resampled DEM to\", dempath+dem_outfile)\n",
    "    rio_write(dempath+dem_outfile, dem_resamp, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    # load the empty grid\n",
    "    dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "    print(dem_outfile, ' already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate geogrid outputs one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhdx, dhdy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IfSAR_200m_DSM_clipped_dhdy.tif and IfSAR_200m_DSM_clipped_dhdx.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "dhdx_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdx.tif' # generate new filename\n",
    "dhdy_outfile = 'IfSAR_'+str(CHIPSIZE_M)+'m_DSM_clipped_dhdy.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(dempath+dhdx_outfile) or not os.path.exists(dempath+dhdy_outfile): # if either is missing\n",
    "    # Produce dhdx and dhdy maps from resampled DEM\n",
    "    dhdx = np.gradient(dem_resamp, axis=1)/CHIPSIZE_M\n",
    "    dhdy = np.gradient(dem_resamp, axis=0)/CHIPSIZE_M\n",
    "\n",
    "    # Filter out borders with high gradient values\n",
    "    grad_thresh = 5\n",
    "    dhdx[abs(dhdx) > grad_thresh] = 0; dhdy[abs(dhdy) > grad_thresh] = 0\n",
    "\n",
    "    # absolute value of the max gradient values expected:\n",
    "    dhmax = 1\n",
    "\n",
    "    # Display the two DEMs as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(dhdx, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "    ax1.set_title('dhdx') # surface slope x\n",
    "    fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "    im2 = ax2.imshow(dhdy, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "    ax2.set_title('dhdy') # surface slope y\n",
    "    fig.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the gradient maps to tif files\n",
    "    print(\"Save surface slope maps to\", dempath)\n",
    "    rio_write(dempath+dhdx_outfile, dhdx, refdem, CHIPSIZE_M) # dhdx\n",
    "    rio_write(dempath_dhdy_outfile, dhdy, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    print(dhdy_outfile, 'and', dhdx_outfile, 'already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vx, vy, srx, sry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vx_200m.tif , vy_200m.tif , srx_200m.tif , sry_200m.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "############### SEARCH RANGE MULTIPLICATION FACTOR ###############\n",
    "# search range limits are calculated as a scalar multiplied by the\n",
    "# reference velocity distribution\n",
    "sr_scaling = 16\n",
    "##################################################################\n",
    "\n",
    "# generate outfile names\n",
    "vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(refvpath+vx_outfile) or not os.path.exists(refvpath+vy_outfile): # if either vx, vy missing\n",
    "    # open the files with rasterio\n",
    "    vx_reader = rio.open(refvpath+vx_fname); vx0 = vx_reader.read(1)\n",
    "    vy_reader = rio.open(refvpath+vy_fname); vy0 = vy_reader.read(1)\n",
    "    vx_x = np.linspace(vx_reader.bounds.left, vx_reader.bounds.right, num=np.shape(vx0)[1])\n",
    "    vx_y = np.linspace(vx_reader.bounds.top, vx_reader.bounds.bottom, num=np.shape(vx0)[0])\n",
    "    vy_x = np.linspace(vy_reader.bounds.left, vy_reader.bounds.right, num=np.shape(vy0)[1])\n",
    "    vy_y = np.linspace(vy_reader.bounds.top, vy_reader.bounds.bottom, num=np.shape(vy0)[0])\n",
    "\n",
    "    # Resample to the DEM grid\n",
    "    fx = interp2d(vx_x, vx_y, vx0)\n",
    "    fy = interp2d(vy_x, vy_y, vy0)\n",
    "    vx_resamp = np.flipud(fx(new_x,new_y)) \n",
    "    vy_resamp = np.flipud(fy(new_x,new_y)) # flip up down\n",
    "\n",
    "    # Display the two velocity files as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(vx_resamp, cmap='Greys_r'); ax1.set_title('vx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(vy_resamp, cmap='Greys_r'); ax2.set_title('vy'); fig.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "\n",
    "    # CALCULATE SEARCH RANGE LIMITS MULTIPLY VX AND VY BY SOME NUMBER\n",
    "    srx_resamp = vx_resamp*sr_scaling; sry_resamp = vy_resamp*sr_scaling\n",
    "\n",
    "    # Display the two search range files as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(srx_resamp, cmap='Greys_r'); ax1.set_title('srx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(sry_resamp, cmap='Greys_r'); ax2.set_title('sry'); fig.colorbar(im2, ax=ax2)\n",
    "    plt.show()\n",
    "    \n",
    "    # save the reference velocity and search range maps\n",
    "    rio_write(refvpath+vx_outfile, vx_resamp, refdem, CHIPSIZE_M) # vx\n",
    "    rio_write(refvpath+vy_outfile, vy_resamp, refdem, CHIPSIZE_M) # vy\n",
    "    rio_write(refvpath+srx_outfile, srx_resamp, refdem, CHIPSIZE_M) # srx\n",
    "    rio_write(refvpath+sry_outfile, sry_resamp, refdem, CHIPSIZE_M) # sry\n",
    "else:\n",
    "    print(vx_outfile, ',', vy_outfile, ',', srx_outfile, ',', sry_outfile, 'already exist.')\n",
    "    \n",
    "#     # OPTIONAL REMOVE\n",
    "#     os.remove(refvpath+vx_outfile); os.remove(refvpath+vy_outfile); \n",
    "#     os.remove(refvpath+srx_outfile); os.remove(refvpath+sry_outfile);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stable surface mask - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHIPSIZE_M = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssm_200m.tif and TG_mask_200m.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "tg_outfile = 'TG_mask_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(refvpath+ssm_outfile) or not os.path.exists(refvpath+tg_outfile):\n",
    "    # read it in, process (resample, mask, etc.) and resave\n",
    "    ssmreader = rio.open(refvpath+'ice_mask.tif')\n",
    "    ssm = ssmreader.read(1)\n",
    "    ssm[ssm > 0] = 1; #ssm[ssm < 0.0] = 0; # make binary\n",
    "    ssm = ssm < 0 # find all stable areas (where.tif = 0)\n",
    "    \n",
    "    # do the same for Turner Glacier mask\n",
    "    tgreader = rio.open(refvpath+'TG_mask.tif'); tg_mask = tgreader.read(1)\n",
    "    tg_mask[tg_mask > 0] = 1\n",
    "#     tg_mask = tg_mask < 0\n",
    "\n",
    "    # grab x and y-values\n",
    "    ssm_x = np.linspace(ssmreader.bounds.left, ssmreader.bounds.right, num=np.shape(ssm)[1])\n",
    "    ssm_y = np.linspace(ssmreader.bounds.top, ssmreader.bounds.bottom, num=np.shape(ssm)[0])\n",
    "    tg_x = np.linspace(tgreader.bounds.left, tgreader.bounds.right, num=np.shape(tg_mask)[1])\n",
    "    tg_y = np.linspace(tgreader.bounds.top, tgreader.bounds.bottom, num=np.shape(tg_mask)[0])\n",
    "\n",
    "    # Resample to the DEM grid\n",
    "    f_ssm = interp2d(ssm_x, ssm_y, ssm)\n",
    "    f_tg = interp2d(tg_x, tg_y, tg_mask)\n",
    "    ssm_resamp = np.flipud(f_ssm(new_x,new_y))\n",
    "    tg_resamp = np.flipud(f_tg(new_x, new_y))\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ssm_im = ax.imshow(tg_resamp,cmap='gray',vmin=0)\n",
    "    ax.set_title('Stable Surface Mask')\n",
    "    fig.colorbar(ssm_im, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "#     # export\n",
    "#     rio_write(refvpath+ssm_outfile, ssm_resamp, refdem, CHIPSIZE_M)\n",
    "    rio_write(refvpath+tg_outfile, tg_resamp, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    print(ssm_outfile,'and',tg_outfile,'already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csminx, csminy, csmax, csmaxy - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csminx_200m.tif , csminy_200m.tif , csmaxx_200m.tif , csmaxy_200m.tif already exist.\n"
     ]
    }
   ],
   "source": [
    "# generate the file names\n",
    "csminx_fname = 'csminx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csminy_fname = 'csminy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csmaxx_fname = 'csmaxx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csmaxy_fname = 'csmaxy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "\n",
    "if not os.path.exists(refvpath+csminx_fname) or not os.path.exists(refvpath+csminy_fname): # if either csminx or csminy missing:\n",
    "    # create the rasters\n",
    "    csminx = np.ones(dem_resamp.shape)*CHIPSIZE_M # minimum chip size shaped like the resampled DEM\n",
    "    csminy = csminx # make identical in both directions\n",
    "    csmaxx = csminx*4 # make maximum chip size 4*minimum chip size (400)\n",
    "    csmaxy = csmaxx\n",
    "    \n",
    "    # export\n",
    "    rio_write(refvpath+csminx_fname, csminx, refdem, CHIPSIZE_M) # csminx\n",
    "    rio_write(refvpath+csminy_fname, csminy, refdem, CHIPSIZE_M) # csminy\n",
    "    rio_write(refvpath+csmaxx_fname, csmaxx, refdem, CHIPSIZE_M) # csmaxx\n",
    "    rio_write(refvpath+csmaxy_fname, csmaxy, refdem, CHIPSIZE_M) # csmaxx\n",
    "else:\n",
    "    print(csminx_fname, ',',csminy_fname, ',', csmaxx_fname, ',', csmaxy_fname, 'already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run geogrid with resampled DEM and other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topsApp.py available for SAR image coregistration.\n"
     ]
    }
   ],
   "source": [
    "###### SET PATH PROCESSING PATHS ##########################\n",
    "out_path = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/output_AutoRIFT/' # output file path\n",
    "# # LS test:\n",
    "# imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/LS8images/useable_images/'\n",
    "# im1_name = 'LC08_L1TP_062018_20200401_20200410_01_T1_B8_BufferTurner.TIF'\n",
    "# im2_name = 'LC08_L1TP_062018_20200417_20200423_01_T1_B8_BufferTurner.TIF'\n",
    "# img_type = 'OPT'\n",
    "\n",
    "# # S2 test:\n",
    "# imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel2/'\n",
    "# im1_name = 'S2A_7VEG_20200427_B08_clipped.tif'\n",
    "# im2_name = 'S2A_7VEG_20200510_B08_clipped.tif'\n",
    "# img_type = 'OPT'\n",
    "\n",
    "# # PS test:\n",
    "# imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/Planet_test/'\n",
    "# im1_name = 'PS_20190304_clipped.tif'\n",
    "# im2_name = 'PS_20190307_clipped.tif'\n",
    "# img_type = 'OPT'\n",
    "\n",
    "# S1 test\n",
    "imgpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/'\n",
    "im1_name = 'S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip'\n",
    "im2_name = 'S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip'\n",
    "img_type = 'SAR'\n",
    "if not os.path.exists(imgpath+'topsApp.py'):\n",
    "    print('topsApp.py not in folder. Place topsApp.py in', imgpath)\n",
    "else:\n",
    "    print('topsApp.py available for SAR image coregistration.')\n",
    "\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "#                         dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "#                        temp_dir): # SAR needed only\n",
    "    \n",
    "#     CHIPSIZE_M = MINCHIPSIZE # set minimum chip size equal\n",
    "#     ############ Clear all old geogrid files ##########################\n",
    "#     for file in os.listdir(out_path):\n",
    "#         if file.startswith('window') and file.endswith('.tif'):\n",
    "#             print('removed', file)\n",
    "#             os.remove(out_path+file)\n",
    "#     print('Old files cleared.'); print()\n",
    "\n",
    "#     dem_info = gdal.Info(dem, format='json') # grab info from DEM\n",
    "#     print('Obtained DEM info.'); print()\n",
    "\n",
    "#     ############ Run geogrid optical or SAR ##########################\n",
    "#     if img_type == 'OPT': # Optical images\n",
    "#         print('Processing optical images with geogrid.'); print()\n",
    "#         obj = GeogridOptical() # initialize geogrid object\n",
    "\n",
    "#         ############ Coregister the optical data (from coregisterLoadMetadataOptical) #############\n",
    "#         x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s,0)\n",
    "\n",
    "#         # grab dates from file names\n",
    "#         im1_name = indir_m.split('/')[-1]; im2_name = indir_s.split('/')[-1]\n",
    "#         if 'LC' in im1_name and 'LC' in im2_name:\n",
    "#             ds1 = im1_name.split('_')[3]\n",
    "#             ds2 = im1_name.split('_')[4]\n",
    "#         elif 'S2' in im1_name and 'S2' in im2_name:\n",
    "#             ds1 = im1_name[9:17]\n",
    "#             ds2 = im2_name[9:17]\n",
    "#         elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "#             ds1 = im1_name[3:11]\n",
    "#             ds2 = im2_name[3:11]\n",
    "#         else:\n",
    "#             raise Exception('Optical data NOT supported yet!') \n",
    "#         print('Optical images coregistered.'); print()\n",
    "\n",
    "#         ########### Load geogrid inputs and run (from runGeogridOptical) ################\n",
    "\n",
    "#         # grab info from above\n",
    "#         obj.startingX = trans[0]; obj.startingY = trans[3]\n",
    "#         obj.XSize = trans[1]; obj.YSize = trans[5]\n",
    "#         d0 = datetime.date(int(ds1[0:4]),int(ds1[4:6]),int(ds1[6:8]))\n",
    "#         d1 = datetime.date(int(ds2[0:4]),int(ds2[4:6]),int(ds2[6:8]))\n",
    "#         date_dt_base = d1 - d0\n",
    "#         obj.repeatTime = date_dt_base.total_seconds()\n",
    "#         obj.numberOfLines = ysize1; obj.numberOfSamples = xsize1\n",
    "#         obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "\n",
    "#         # customize no data value and minimimum chip size\n",
    "#         obj.nodata_out = NO_DATA_VAL\n",
    "#         obj.chipSizeX0 = MINCHIPSIZE\n",
    "\n",
    "#         # set raster paths and names\n",
    "#         obj.dat1name = indir_m # first image\n",
    "#         obj.demname = dem # DEM\n",
    "#         obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "#         obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "#         obj.srxname = srx; obj.sryname = sry # search range limits\n",
    "#         obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "#         obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "#         obj.ssmname = ssm # stable surface mask\n",
    "#         obj.winlocname = \"window_location.tif\"\n",
    "#         obj.winoffname = \"window_offset.tif\"\n",
    "#         obj.winsrname = \"window_search_range.tif\"\n",
    "#         obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "#         obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "#         obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "#         obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "#         obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "\n",
    "#         obj.runGeogrid() # RUN GEOGRID\n",
    "#         print('Optical geogrid finished.'); print()\n",
    "\n",
    "#     elif img_type == 'SAR': # SAR images\n",
    "#         print('Processing SAR images with geogrid.'); print();\n",
    "#         ############ Load SAR metadata from coreg_files ##################################\n",
    "#         # Store sensing start info for 2nd SAR image (in temp_dir+secondary/)\n",
    "#         frames = []\n",
    "#         for swath in range(1,4):\n",
    "#             inxml = os.path.join(temp_dir+'secondary/', 'IW{0}.xml'.format(swath))\n",
    "#             if os.path.exists(inxml):\n",
    "#                 pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file\n",
    "#                 frames.append(ifg)\n",
    "#         info1_sensingStart = min([x.sensingStart for x in frames]) # store info1_sensingStart\n",
    "\n",
    "#         # Load other info from 1st SAR image (in temp_dir+reference/)\n",
    "#         del frames; frames = [] \n",
    "#         for swath in range(1,4):\n",
    "#             inxml = os.path.join(temp_dir+'reference/', 'IW{0}.xml'.format(swath))\n",
    "#             if os.path.exists(inxml):\n",
    "#                 pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file        \n",
    "#                 frames.append(ifg)\n",
    "#         print('SAR metadata loaded.'); print()\n",
    "\n",
    "#         ############ Get merged orbit getMergedOrbit() ################################## \n",
    "#         # Create merged orbit\n",
    "#         orb = Orbit(); orb.configure()\n",
    "#         burst = frames[0].bursts[0]\n",
    "#         # Add first burst orbit to begin with\n",
    "#         for sv in burst.orbit:\n",
    "#             orb.addStateVector(sv)\n",
    "#         for pp in frames:\n",
    "#             # Add all state vectors\n",
    "#             for bb in pp.bursts:\n",
    "#                 for sv in bb.orbit:\n",
    "#                     if (sv.time< orb.minTime) or (sv.time > orb.maxTime):\n",
    "#                         orb.addStateVector(sv)\n",
    "#         print('Merged orbit created.'); print()\n",
    "\n",
    "#         ############ Load geogrid inputs and run ###################################\n",
    "#         obj = Geogrid()\n",
    "#         obj.configure()\n",
    "\n",
    "#         obj.orbit = orb # grab merged orbit\n",
    "#         obj.startingRange = min([x.startingRange for x in frames])\n",
    "#         obj.rangePixelSize = frames[0].bursts[0].rangePixelSize\n",
    "#         obj.sensingStart = min([x.sensingStart for x in frames])\n",
    "#         obj.prf = 1.0 / frames[0].bursts[0].azimuthTimeInterval\n",
    "#         obj.lookSide = -1\n",
    "#         obj.repeatTime = (info1_sensingStart - obj.sensingStart).total_seconds() # INFO1\n",
    "#         obj.numberOfLines = int(np.round((max([x.sensingStop for x in frames])-obj.sensingStart).total_seconds()*obj.prf))+1\n",
    "#         obj.numberOfSamples = int(np.round((max([x.farRange for x in frames])-obj.startingRange)/obj.rangePixelSize))+1\n",
    "#         obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "\n",
    "#         # custom no data value and chip size\n",
    "#         obj.nodata_out = NO_DATA_VAL\n",
    "#         obj.chipSizeX0 = CHIPSIZE_M\n",
    "\n",
    "#         # set raster paths and names\n",
    "#         obj.demname = dem # DEM\n",
    "#         obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "#         obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "#         obj.srxname = srx; obj.sryname = sry # search range limmits\n",
    "#         obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "#         obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "#         obj.ssmname = ssm # stable surface mask\n",
    "#         obj.winlocname = \"window_location.tif\"\n",
    "#         obj.winoffname = \"window_offset.tif\"\n",
    "#         obj.winsrname = \"window_search_range.tif\"\n",
    "#         obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "#         obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "#         obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "#         obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "#         obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "\n",
    "#         obj.getIncidenceAngle() # SAR specific\n",
    "#         obj.geogrid() # run geogrid\n",
    "#         print('SAR geogrid finished.'); print();\n",
    "\n",
    "#     else: # not OPT or SAR\n",
    "#         print('Image type flag not recognized :', img_type)\n",
    "\n",
    "\n",
    "#     ############ Move files produced to the out_path directory ##############\n",
    "#     for file in os.listdir(os.getcwd()):\n",
    "#         if file.startswith('window') and file.endswith('.tif'):\n",
    "#             shutil.move(os.getcwd()+'/'+file, out_path+file)\n",
    "#     print('Geogrid output files moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For SAR images only: Coregister SAR images with topsApp.py\n",
    "\n",
    "#### Put TG's custom topApp.xml on repo\n",
    "\n",
    "Requires orbit and auxiliary files for the S1 images as well as the creation of 3 xml files (topsApp.xml, reference.xml, secondary.xml).\n",
    "\n",
    "Prior to any processing, download S1 .SAFE files, auxiliary instrument files, and orbit files.\n",
    "\n",
    "- Download .SAFE files from Alaska Satellite Facility: https://search.asf.alaska.edu/\n",
    "- Download auxiliary instrument files from https://qc.sentinel1.copernicus.eu/aux_ins/\n",
    "- Download orbit files from Copernicus Sentinels POD Data Hub: https://scihub.copernicus.eu/gnss/#/home\n",
    "\n",
    "Although Sentinel-1 restituted orbits (RESORB) are of good quality, it is recommended to use the precise orbits (POEORB) when available. Typically, precise orbits are available with a 15 to 20-day lag from the day of the acquisition.\n",
    "\n",
    "To proceed with automatic downloads from ASF, **wget** must be downloaded. Create a wget configuration file in the image directory using terminal following the [ASF API instructions](https://docs.asf.alaska.edu/api/tools/):\n",
    "\n",
    "    echo 'http_user=CHANGE_ME' >> wget.conf\n",
    "    echo 'http_password=CHANGE_ME' >> wget.conf\n",
    "    chmod 600 wget.conf\n",
    "    export WGETRC=\"wget.conf\"\n",
    "    \n",
    "Also, create a netrc file containing the NASA Earthdata login info for topsApp.py.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create .netrc file with Earthdata credentials\n",
    "def create_netrc(netrc_name):\n",
    "    from netrc import netrc\n",
    "    from subprocess import Popen\n",
    "    from getpass import getpass\n",
    "    \n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    if os.path.exists(homeDir+'/'+netrc_name):\n",
    "        print(netrc_name, ' with Earthdata credentials already exists.')\n",
    "\n",
    "    urs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\n",
    "    prompts = ['Enter NASA Earthdata Login Username: ',\n",
    "               'Enter NASA Earthdata Login Password: ']\n",
    "    # Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "    try:\n",
    "        netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n",
    "        netrc(netrcDir).authenticators(urs)[0]\n",
    "    # Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "    except FileNotFoundError:\n",
    "        Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "        Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "        Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n",
    "        # Set restrictive permissions\n",
    "        Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_orbits(SAFEzipfilepath, config_path, out_dir):\n",
    "    # DOWNLOADS PRECISE ORBIT FILES FROM ASF\n",
    "    # Requires installation of wget\n",
    "    # INPUTS:\n",
    "    #  SAFEzipfilepath: path to the SAFE file \n",
    "    #  configpath: \n",
    "    #  our_dir: path to the orbit directory where orbit files will be saved\n",
    "    \n",
    "    orb_type = 'aux_poeorb'\n",
    "    zipname = SAFEzipfilepath.split('/')[-1] \n",
    "    time = zipname.split('_')[5]\n",
    "    S1 = zipname.split('_')[0][-3:]\n",
    "    scene_center_time = datetime.datetime.strptime(time,\"%Y%m%dT%H%M%S\")\n",
    "    validity_start_time = scene_center_time-datetime.timedelta(days=1)\n",
    "    validity_end_time =  scene_center_time+datetime.timedelta(days=1)\n",
    "    \n",
    "    # ASF URL\n",
    "    url = \"https://s1qc.asf.alaska.edu/%s/?validity_start=%s&validity_start=%s&validity_start=%s&sentinel1__mission=%s\" % (orb_type, validity_start_time.strftime(\"%Y\"),validity_start_time.strftime(\"%Y-%m\"), validity_start_time.strftime(\"%Y-%m-%d\"), S1)   \n",
    "    content = ( urllib.request.urlopen(url).read()) # read results\n",
    "    ii = re.findall('''href=[\"'](.[^\"']+)[\"']''', content.decode('utf-8'))\n",
    "    \n",
    "    for i in ii :\n",
    "        if '.EOF' in i:\n",
    "            if (validity_start_time.strftime(\"%Y%m%d\") in i) and (validity_end_time.strftime(\"%Y%m%d\") in i) and (S1 in i):\n",
    "                \n",
    "                # if it doesn't already exist\n",
    "                if not os.path.isfile(out_dir+i):\n",
    "                    wget_cmd = 'export WGETRC=\"'+config_path+'\"; '\n",
    "                    wget_cmd += 'wget -c -P '+out_dir+' '\n",
    "                    wget_cmd += \"https://s1qc.asf.alaska.edu/aux_poeorb/\"+i\n",
    "    #                 print(wget_cmd)\n",
    "                    subprocess.run(wget_cmd, shell=True,check=True)  \n",
    "                    print(i+' downloaded.')\n",
    "                else:\n",
    "                    print(i+' already exists in orbit folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".netrc  with Earthdata credentials already exists.\n",
      "coreg_files folder already exists. Old files cleared.\n",
      "\n",
      "Downloading orbit files: \n",
      "S1A_OPER_AUX_POEORB_OPOD_20210411T121741_V20210321T225942_20210323T005942.EOF already exists in orbit folder.\n",
      "S1A_OPER_AUX_POEORB_OPOD_20210423T121738_V20210402T225942_20210404T005942.EOF already exists in orbit folder.\n",
      "\n",
      "This is the Open Source version of ISCE.\n",
      "Some of the workflows depend on a separate licensed package.\n",
      "To obtain the licensed package, please make a request for ISCE\n",
      "through the website: https://download.jpl.nasa.gov/ops/request/index.cfm.\n",
      "Alternatively, if you are a member, or can become a member of WinSAR\n",
      "you may be able to obtain access to a version of the licensed sofware at\n",
      "https://winsar.unavco.org/software/isce\n",
      "2022-08-26 14:01:37,385 - isce.insar - INFO - ISCE VERSION = 2.5.3, RELEASE_SVN_REVISION = ,RELEASE_DATE = 20210823, CURRENT_SVN_REVISION = \n",
      "ISCE VERSION = 2.5.3, RELEASE_SVN_REVISION = ,RELEASE_DATE = 20210823, CURRENT_SVN_REVISION = \n",
      "Step processing\n",
      "Running step startup\n",
      "None\n",
      "The currently supported sensors are:  ['SENTINEL1']\n",
      "Dumping the application's pickle object _insar to file  PICKLE/startup\n",
      "The remaining steps are (in order):  ['preprocess', 'computeBaselines', 'verifyDEM', 'topo', 'subsetoverlaps', 'coarseoffsets', 'coarseresamp', 'overlapifg', 'prepesd', 'esd', 'rangecoreg', 'fineoffsets', 'fineresamp', 'ion', 'burstifg', 'mergebursts', 'filter', 'unwrap', 'unwrap2stage', 'geocode', 'denseoffsets', 'filteroffsets', 'geocodeoffsets']\n",
      "Running step preprocess\n",
      "Could not extract swath 1 from ['/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip']\n",
      "Generated error:  [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip'\n",
      "Could not extract swath 1 from ['/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip']\n",
      "Generated error:  [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip'\n",
      "Could not extract swath 2 from ['/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip']\n",
      "Generated error:  [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip'\n",
      "Could not extract swath 2 from ['/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip']\n",
      "Generated error:  [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip'\n",
      "Could not extract swath 3 from ['/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip']\n",
      "Generated error:  [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210322T154835_20210322T154901_037113_045E94_6840.zip'\n",
      "Could not extract swath 3 from ['/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip']\n",
      "Generated error:  [Errno 2] No such file or directory: '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/S1A_IW_SLC__1SDV_20210403T154835_20210403T154902_037288_04649A_5AF4.zip'\n",
      "2022-08-26 14:01:37,446 - isce.topsinsar.runPreprocessor - INFO - \n",
      "####################################################################################################\n",
      "    runPreprocessor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "common.Input list of swaths to process:  = [1, 2, 3]\n",
      "####################################################################################################\n",
      "Dumping the application's pickle object _insar to file  PICKLE/preprocess\n",
      "The remaining steps are (in order):  ['computeBaselines', 'verifyDEM', 'topo', 'subsetoverlaps', 'coarseoffsets', 'coarseresamp', 'overlapifg', 'prepesd', 'esd', 'rangecoreg', 'fineoffsets', 'fineresamp', 'ion', 'burstifg', 'mergebursts', 'filter', 'unwrap', 'unwrap2stage', 'geocode', 'denseoffsets', 'filteroffsets', 'geocodeoffsets']\n",
      "Running step computeBaselines\n",
      "Estimated burst offset:  0\n",
      "Skipping processing for swath number IW-2\n",
      "Skipping processing for swath number IW-3\n",
      "2022-08-26 14:01:37,739 - isce.topsinsar.runPreprocessor - INFO - \n",
      "####################################################################################################\n",
      "    runComputeBaseline\n",
      "----------------------------------------------------------------------------------------------------\n",
      "baseline.IW-1 Bpar at midrange for first common burst = 14.296145807369697\n",
      "baseline.IW-1 Bpar at midrange for last common burst = 14.181775997271247\n",
      "baseline.IW-1 Bperp at midrange for first common burst = 10.146425695936662\n",
      "baseline.IW-1 Bperp at midrange for last common burst = 10.246998912289678\n",
      "baseline.IW-1 First common burst in reference = 0\n",
      "baseline.IW-1 First common burst in secondary = 0\n",
      "baseline.IW-1 Last common burst in reference = 3\n",
      "baseline.IW-1 Last common burst in secondary = 3\n",
      "baseline.IW-1 Number of bursts in reference = 3\n",
      "baseline.IW-1 Number of bursts in secondary = 3\n",
      "baseline.IW-1 Number of common bursts = 3\n",
      "####################################################################################################\n",
      "Dumping the application's pickle object _insar to file  PICKLE/computeBaselines\n",
      "The remaining steps are (in order):  ['verifyDEM', 'topo', 'subsetoverlaps', 'coarseoffsets', 'coarseresamp', 'overlapifg', 'prepesd', 'esd', 'rangecoreg', 'fineoffsets', 'fineresamp', 'ion', 'burstifg', 'mergebursts', 'filter', 'unwrap', 'unwrap2stage', 'geocode', 'denseoffsets', 'filteroffsets', 'geocodeoffsets']\n",
      "Running step verifyDEM\n",
      "2022-08-26 14:01:48,361 - isce.iscesys.DataRetriever - ERROR - There was a problem in retrieving the file  http://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N60W142.SRTMGL1.hgt.zip. Check the name of the server or try again later in case the server is momentarily down.\n",
      "Polynomial Order: 0 - by - 0 \n",
      "0\t\n",
      "Polynomial Order: 0 - by - 0 \n",
      "0\t\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'cd /Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/coreg_files/; python3 /Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/topsApp.py --start=startup --end=rangecoreg' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m runtopsapp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtemp_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# change directory into temp folder\u001b[39;00m\n\u001b[1;32m     54\u001b[0m runtopsapp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython3 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mimgpath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopsApp.py --start=startup --end=rangecoreg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# tun topsapp.py\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntopsapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newautoriftenv/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cd /Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/coreg_files/; python3 /Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/topsApp.py --start=startup --end=rangecoreg' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# coregister pre-process the SAR images, generating XM: files\n",
    "if img_type == 'SAR':\n",
    "    create_netrc('.netrc') # create netrc file with Earthdata credentials if necessary\n",
    "    \n",
    "    # make a folder to hold intermediate files used for coregistration\n",
    "    temp_dir = imgpath+'coreg_files/'\n",
    "    if not os.path.exists(temp_dir): # if doesn't exist\n",
    "        os.mkdir(temp_dir) # make it\n",
    "        print('coreg_files folder created.'); print()\n",
    "    else: # CLEAR ENTIRE FOLDER?\n",
    "        for file in os.listdir(temp_dir): # if it already exists:\n",
    "            if file.endswith('.xml'): # clear all the old XML files\n",
    "                os.remove(temp_dir+file)\n",
    "        print('coreg_files folder already exists. Old files cleared.'); print()\n",
    "           \n",
    "    # download the orbit files\n",
    "    orbit_dir = imgpath+'orbits/'\n",
    "    config_path = imgpath+'wget.conf' # path to the WGET config file\n",
    "    if not os.path.isdir(orbit_dir): # if orbits folder doesn't exist\n",
    "        os.mkdir(orbit_dir) # make it \n",
    "    if os.path.isfile(imgpath+'wget.conf'):\n",
    "        print('Downloading orbit files: ')\n",
    "        download_orbits(imgpath+im1_name, config_path, orbit_dir) # orbit file for 1st image\n",
    "        download_orbits(imgpath+im2_name, config_path, orbit_dir) # orbit file for 2nd image\n",
    "    else:\n",
    "        print('Error: wget.conf not in ',imgpath)\n",
    "    print()\n",
    "    \n",
    "    # move template XML files into the temporary folder\n",
    "    for xmlname in ['topsApp.xml', 'reference.xml','secondary.xml']:\n",
    "        if os.path.isfile(imgpath+xmlname):# if these xml template files exist\n",
    "            shutil.copyfile(imgpath+xmlname, temp_dir+xmlname) # copy them into the temp folder\n",
    "        else:\n",
    "            print('Error: missing the template file '+xmlname); print()\n",
    "            \n",
    "    # auto update reference.xml & secondary.xml using XML Element Tree (ET) package\n",
    "    tree1 = ET.parse(temp_dir+'reference.xml'); root1 = tree1.getroot() # first image\n",
    "    tree2 = ET.parse(temp_dir+'secondary.xml'); root2 = tree2.getroot() # second image\n",
    "    for prop in root1.iter(): # REFERENCE\n",
    "        if prop.get('name') == 'orbit directory':\n",
    "            prop.text = orbit_dir # set orbit directory to orbit_dir\n",
    "        if prop.get('name') == 'safe':\n",
    "            prop.text = imgpath+im1_name # set SAFE.zip file path\n",
    "    tree1.write(temp_dir+'reference.xml') # over-write\n",
    "    for prop in root2.iter(): # SECONDARY\n",
    "        if prop.get('name') == 'orbit directory':\n",
    "            prop.text = orbit_dir # set orbit directory to orbit_dir\n",
    "        if prop.get('name') == 'safe':\n",
    "            prop.text = imgpath+im2_name # set SAFE.zip file path\n",
    "    tree2.write(temp_dir+'secondary.xml') # over-write\n",
    "    \n",
    "    # Run topsApp.py coregistration (will take some time to complete)\n",
    "    runtopsapp = 'cd '+temp_dir+'; ' # change directory into temp folder\n",
    "    runtopsapp += 'python3 '+imgpath+'topsApp.py --start=startup --end=rangecoreg' # tun topsapp.py\n",
    "    subprocess.run(runtopsapp, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set custom Geogrid parameters (Optical and SAR)\n",
    "\n",
    "     input \"dhdxname\"/\"dhdyname\"                          -> output \"winro2vxname\"/\"winro2vyname\"\n",
    "     input \"dhdxname\"/\"dhdyname\" and \"vxname\"/\"vyname\"    -> output \"winro2vxname\"/\"winro2vyname\" and \"winoffname\" \n",
    "     input \"dhdxname\"/\"dhdyname\" and \"srxname\"/\"sryname\"  -> output \"winro2vxname\"/\"winro2vyname\" and \"winsrname\"\n",
    "     input \"csminxname\"/\"csminyname\"                      -> output \"wincsminname\"\n",
    "     input \"csmaxxname\"/\"csmaxyname\"                      -> output \"wincsmaxname\"\n",
    "     input \"ssmname\"                                      -> output \"winssmname\"\n",
    "\n",
    "#### Best  MINCHIPSIZE >= SCALAR*PIXRES where SCALAR = 16 or some other power of 2\n",
    "\n",
    "    LS = 200\n",
    "    S2 = 160\n",
    "    PS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dem_outfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###### SET CUSTOM PARAMETERS FOR GEOGRID ################\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dem \u001b[38;5;241m=\u001b[39m dempath\u001b[38;5;241m+\u001b[39m\u001b[43mdem_outfile\u001b[49m \u001b[38;5;66;03m# path to the resampled DEM produced in the previous step (outfile name)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m indir_m \u001b[38;5;241m=\u001b[39m imgpath\u001b[38;5;241m+\u001b[39mim1_name\n\u001b[1;32m      4\u001b[0m indir_s \u001b[38;5;241m=\u001b[39m imgpath\u001b[38;5;241m+\u001b[39mim2_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dem_outfile' is not defined"
     ]
    }
   ],
   "source": [
    "###### SET CUSTOM PARAMETERS FOR GEOGRID ################\n",
    "dem = dempath+dem_outfile # path to the resampled DEM produced in the previous step (outfile name)\n",
    "indir_m = imgpath+im1_name\n",
    "indir_s = imgpath+im2_name\n",
    "MINCHIPSIZE = 160 # smallest chip size allowed in image horizontal direction (in m)\n",
    "NO_DATA_VAL = -32767 # no data value in the output products\n",
    "temp_dir = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/coreg_files/'\n",
    "\n",
    "if MINCHIPSIZE > CHIPSIZE_M:\n",
    "    warning = 'Your minimum chip size for autoRIFT exceeds the output grid size (CHIPSIZE_M). '\n",
    "    warning += 'Please increase the resampled DEM gridsize (CHIPSIZE_M).'\n",
    "    print(warning)\n",
    "\n",
    "# optional inputs (set as '' to leave blank)\n",
    "dhdx = dempath+dhdx_outfile\n",
    "dhdy = dempath+dhdy_outfile\n",
    "vx = refvpath+vx_outfile\n",
    "vy = refvpath+vy_outfile\n",
    "# srx = refvpath+srx_outfile\n",
    "# sry = refvpath+sry_outfile\n",
    "csminx = refvpath+csminx_fname\n",
    "csminy = refvpath+csminy_fname\n",
    "csmaxx = refvpath+csmaxx_fname\n",
    "csmaxy = refvpath+csmaxy_fname\n",
    "ssm = refvpath+ssm_outfile # stable surface mask \n",
    "\n",
    "# dhdx = ''\n",
    "# dhdy = ''\n",
    "# vx = ''\n",
    "# vy = ''\n",
    "srx = ''\n",
    "sry = ''\n",
    "# csminx = ''\n",
    "# csminy = ''\n",
    "# csmaxx = ''\n",
    "# csmaxy = ''\n",
    "# ssm = ''\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indir_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_geogrid_inhouse(out_path, img_type, \u001b[43mindir_m\u001b[49m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, \u001b[38;5;66;03m# required inputs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                     dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, \u001b[38;5;66;03m# optional inputs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                     temp_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indir_m' is not defined"
     ]
    }
   ],
   "source": [
    "run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                    dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                    temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ Clear all old geogrid files ##########################\n",
    "# for file in os.listdir(out_path):\n",
    "#     if file.startswith('window') and file.endswith('.tif'):\n",
    "#         print('removed', file)\n",
    "#         os.remove(out_path+file)\n",
    "# print('Old files cleared.'); print()\n",
    "\n",
    "# dem_info = gdal.Info(dem, format='json') # grab info from DEM\n",
    "# print('Obtained DEM info.'); print()\n",
    "\n",
    "# ############ Run geogrid optical or SAR ##########################\n",
    "# if img_type == 'OPT': # Optical images\n",
    "#     print('Processing optical images with geogrid.'); print()\n",
    "#     obj = GeogridOptical() # initialize geogrid object\n",
    "    \n",
    "#     ############ Coregister the optical data (from coregisterLoadMetadataOptical) #############\n",
    "#     x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s,0)\n",
    "    \n",
    "#     # grab dates from file names\n",
    "#     if 'LC' in im1_name and 'LC' in im2_name:\n",
    "#         ds1 = im1_name.split('_')[3]\n",
    "#         ds2 = im1_name.split('_')[4]\n",
    "#     elif 'S2' in im1_name and 'S2' in im2_name:\n",
    "#         ds1 = im1_name[9:17]\n",
    "#         ds2 = im2_name[9:17]\n",
    "#     elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "#         ds1 = im1_name[3:11]\n",
    "#         ds2 = im2_name[3:11]\n",
    "#     else:\n",
    "#         raise Exception('Optical data NOT supported yet!') \n",
    "#     print('Optical images coregistered.'); print()\n",
    "    \n",
    "#     ########### Load geogrid inputs and run (from runGeogridOptical) ################\n",
    "    \n",
    "#     # grab info from above\n",
    "#     obj.startingX = trans[0]; obj.startingY = trans[3]\n",
    "#     obj.XSize = trans[1]; obj.YSize = trans[5]\n",
    "#     d0 = date(np.int(ds1[0:4]),np.int(ds1[4:6]),np.int(ds1[6:8]))\n",
    "#     d1 = date(np.int(ds2[0:4]),np.int(ds2[4:6]),np.int(ds2[6:8]))\n",
    "#     date_dt_base = d1 - d0\n",
    "#     obj.repeatTime = date_dt_base.total_seconds()\n",
    "#     obj.numberOfLines = ysize1; obj.numberOfSamples = xsize1\n",
    "#     obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "    \n",
    "#     # customize no data value and minimimum chip size\n",
    "#     obj.nodata_out = NO_DATA_VAL\n",
    "#     obj.chipSizeX0 = CHIPSIZE_M\n",
    "    \n",
    "#     # set raster paths and names\n",
    "#     obj.dat1name = indir_m # first image\n",
    "#     obj.demname = dem # DEM\n",
    "#     obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "#     obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "#     obj.srxname = srx; obj.sryname = sry # search range limits\n",
    "#     obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "#     obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "#     obj.ssmname = ssm # stable surface mask\n",
    "#     obj.winlocname = \"window_location.tif\"\n",
    "#     obj.winoffname = \"window_offset.tif\"\n",
    "#     obj.winsrname = \"window_search_range.tif\"\n",
    "#     obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "#     obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "#     obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "#     obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "#     obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "    \n",
    "#     obj.runGeogrid() # RUN GEOGRID\n",
    "#     print('Optical geogrid finished.'); print()\n",
    "    \n",
    "# elif img_type == 'SAR': # SAR images\n",
    "#     print('Processing SAR images with geogrid.'); print();\n",
    "#     ############ Load SAR metadata from coreg_files ##################################\n",
    "#     # Store sensing start info for 2nd SAR image (in temp_dir+secondary/)\n",
    "#     frames = []\n",
    "#     for swath in range(1,4):\n",
    "#         inxml = os.path.join(temp_dir+'secondary/', 'IW{0}.xml'.format(swath))\n",
    "#         if os.path.exists(inxml):\n",
    "#             pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file\n",
    "#             frames.append(ifg)\n",
    "#     info1_sensingStart = min([x.sensingStart for x in frames]) # store info1_sensingStart\n",
    "    \n",
    "#     # Load other info from 1st SAR image (in temp_dir+reference/)\n",
    "#     del frames; frames = [] \n",
    "#     for swath in range(1,4):\n",
    "#         inxml = os.path.join(temp_dir+'reference/', 'IW{0}.xml'.format(swath))\n",
    "#         if os.path.exists(inxml):\n",
    "#             pm = PM(); pm.configure(); ifg = pm.loadProduct(inxml) # load XML file        \n",
    "#             frames.append(ifg)\n",
    "#     print('SAR metadata loaded.'); print()\n",
    "    \n",
    "#     ############ Get merged orbit getMergedOrbit() ################################## \n",
    "#     # Create merged orbit\n",
    "#     orb = Orbit(); orb.configure()\n",
    "#     burst = frames[0].bursts[0]\n",
    "#     # Add first burst orbit to begin with\n",
    "#     for sv in burst.orbit:\n",
    "#         orb.addStateVector(sv)\n",
    "#     for pp in frames:\n",
    "#         # Add all state vectors\n",
    "#         for bb in pp.bursts:\n",
    "#             for sv in bb.orbit:\n",
    "#                 if (sv.time< orb.minTime) or (sv.time > orb.maxTime):\n",
    "#                     orb.addStateVector(sv)\n",
    "#     print('Merged orbit created.'); print()\n",
    "                    \n",
    "#     ############ Load geogrid inputs and run ###################################\n",
    "#     obj = Geogrid()\n",
    "#     obj.configure()\n",
    "    \n",
    "#     obj.orbit = orb # grab merged orbit\n",
    "#     obj.startingRange = min([x.startingRange for x in frames])\n",
    "#     obj.rangePixelSize = frames[0].bursts[0].rangePixelSize\n",
    "#     obj.sensingStart = min([x.sensingStart for x in frames])\n",
    "#     obj.prf = 1.0 / frames[0].bursts[0].azimuthTimeInterval\n",
    "#     obj.lookSide = -1\n",
    "#     obj.repeatTime = (info1_sensingStart - obj.sensingStart).total_seconds() # INFO1\n",
    "#     obj.numberOfLines = int(np.round((max([x.sensingStop for x in frames])-obj.sensingStart).total_seconds()*obj.prf))+1\n",
    "#     obj.numberOfSamples = int(np.round((max([x.farRange for x in frames])-obj.startingRange)/obj.rangePixelSize))+1\n",
    "#     obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "    \n",
    "#     # custom no data value and chip size\n",
    "#     obj.nodata_out = NO_DATA_VAL\n",
    "#     obj.chipSizeX0 = CHIPSIZE_M\n",
    "    \n",
    "#     # set raster paths and names\n",
    "#     obj.demname = dem # DEM\n",
    "#     obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "#     obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "#     obj.srxname = srx; obj.sryname = sry # search range limmits\n",
    "#     obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "#     obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "#     obj.ssmname = ssm # stable surface mask\n",
    "#     obj.winlocname = \"window_location.tif\"\n",
    "#     obj.winoffname = \"window_offset.tif\"\n",
    "#     obj.winsrname = \"window_search_range.tif\"\n",
    "#     obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "#     obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "#     obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "#     obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "#     obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "    \n",
    "#     obj.getIncidenceAngle() # SAR specific\n",
    "#     obj.geogrid() # run geogrid\n",
    "#     print('SAR geogrid finished.'); print();\n",
    "    \n",
    "# else: # not OPT or SAR\n",
    "#     print('Image type flag not recognized :', img_type)\n",
    "    \n",
    "\n",
    "# ############ Move files produced to the out_path directory ##############\n",
    "# for file in os.listdir(os.getcwd()):\n",
    "#     if file.startswith('window') and file.endswith('.tif'):\n",
    "#         shutil.move(os.getcwd()+'/'+file, out_path+file)\n",
    "# print('Geogrid output files moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run autoRIFT with new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify files produced from geogrid\n",
    "gp = out_path\n",
    "\n",
    "# remove all empty grids\n",
    "for grid in os.listdir(gp): \n",
    "    if grid.startswith('window') and grid.endswith('.tif'):\n",
    "        reader = rio.open(gp+grid) # read dataset\n",
    "        data_found = False \n",
    "        for band in range(1,reader.count+1):\n",
    "            testband = reader.read(band) # read in the band\n",
    "            if np.count_nonzero(testband[testband != NO_DATA_VAL]) > 0:\n",
    "                data_found = True\n",
    "        if not data_found:\n",
    "            print(grid, 'has no data. Removed.')\n",
    "            os.remove(gp+grid)\n",
    "        else:\n",
    "            print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in AutoRIFT parameters using the files - make all of these function arguments\n",
    "mpflag = 0 # leave multiprocessing off\n",
    "\n",
    "# GRID LOCATION (required) from window_location.tif\n",
    "grid_location = rio.open(gp+'window_location.tif')\n",
    "xGrid = grid_location.read(1) # 1st band in window location\n",
    "yGrid = grid_location.read(2) # 2nd band in window location\n",
    "\n",
    "# optional parameters (default None or zero until filled)\n",
    "init_offset = None; search_range = None\n",
    "chip_size_min = None; chip_size_max = None\n",
    "offset2vx = None; offset2vy = None; stable_surface_mask = None\n",
    "Dx0 = None; Dy0 = None; CSMINx0 = None\n",
    "SRx0 = None; SRy0 = None;\n",
    "CSMAXx0 = None; CSMAXy0 = None; SSM = None\n",
    "noDataMask = np.zeros(xGrid.shape).astype(int)\n",
    "\n",
    "if os.path.exists(gp+'window_offset.tif'): # Dx0 and Dy0 from window_offset.tif\n",
    "    init_offset = rio.open(gp+'window_offset.tif')\n",
    "    Dx0 = init_offset.read(1); Dy0 = init_offset.read(2)\n",
    "if os.path.exists(gp+'window_search_range.tif'): # SRx0 and SRy0 from window_search_range.tif\n",
    "    search_range = rio.open(gp+'window_search_range.tif')\n",
    "    SRx0 = search_range.read(1); SRy0 = search_range.read(2)\n",
    "if os.path.exists(gp+'window_chip_size_min.tif'): # CSMINx0 and CSMINy0 from window_chip_size_min.tif\n",
    "    chip_size_min = rio.open(gp+'window_chip_size_min.tif')\n",
    "    CSMINx0 = chip_size_min.read(1); CSMINy0 = chip_size_min.read(2)\n",
    "if os.path.exists(gp+'window_chip_size_max.tif'): # CSMAXx0 and CSMAXy0 from window_chip_size_max.tif\n",
    "    chip_size_max = rio.open(gp+'window_chip_size_max.tif')\n",
    "    CSMAXx0 = chip_size_max.read(1); CSMAXy0 = chip_size_max.read(2)\n",
    "if os.path.exists(gp+'window_rdr_off2vel_x_vec.tif'): # offset2vx from window_rdr_off2vel_x_vec.tif\n",
    "    offset2vx = gp+'window_rdr_off2vel_x_vec.tif' # path to be read in with GDAL\n",
    "if os.path.exists(gp+'window_rdr_off2vel_y_vec.tif'): # offset2vy from window_rdr_off2vel_y_vec.tif\n",
    "    offset2vy = gp+'window_rdr_off2vel_y_vec.tif' \n",
    "if os.path.exists(gp+'window_stable_surface_mask.tif'): # noDataMask from window_stable_surface_mask.tif\n",
    "    stable_surface_mask = rio.open(gp+'window_stable_surface_mask.tif')\n",
    "    noDataMask = stable_surface_mask.read(1)\n",
    "    \n",
    "# other parameters\n",
    "nodata = NO_DATA_VAL # use same as in previous steps\n",
    "geogrid_run_info=None\n",
    "print('AutoRIFT parameters loaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### CHOOSE FILTERS & SAMPLING ########################\n",
    "# Filters:\n",
    "# options are HPS (high pass), WAL (wallis), SOB (sobel), DB (logarithmic operator)\n",
    "FILTER = 'WAL'\n",
    "WALLISFILTERWIDTH = 32 # only for wallis filter, must be a power of 2\n",
    "\n",
    "# Sampling:\n",
    "SPARSE_SEARCH_SAMPLE_RATE = 16 # how many samples to skip to speed up processing\n",
    "OVERSAMPLE_RATIO = 0 # enter in a constant scalar or 0 for default parameters\n",
    "############################################################################\n",
    "print('Filters and autoRIFT sample rates chosen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run autoRIFT with function\n",
    "run_autoRIFT_inhouse(out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                         FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATIO, OVERSAMPLE_RATIO,\n",
    "                         Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                         noDataMask, nodataval, geogrid_run_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoRIFT  \n",
    "def run_autoRIFT_inhouse(out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                         FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATIO, OVERSAMPLE_RATIO, MINCHIPSIZE,\n",
    "                         Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                         noDataMask, nodataval, geogrid_run_info):\n",
    "    CHIPSIZE_M = MINCHIPSIZE # set minimum chip size equal\n",
    "    \n",
    "    # requires grid location from geogrid\n",
    "    origSize = xGrid.shape # grab original size from xGrid\n",
    "    \n",
    "    if img_type == 'OPT': ############# OPTICAL SETTINGS ############################# \n",
    "        print('Processing optical images with autoRIFT.'); print()\n",
    "        optflag = 1 # turn on optical flag\n",
    "        # Coregister and read in the two images (from loadProductOptical())\n",
    "        obj = GeogridOptical()\n",
    "        x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s,0)\n",
    "\n",
    "        # read dates from filenames\n",
    "        if 'LC' in indir_m and 'LC' in indir_s:\n",
    "            ds1 = indir_m.split('/')[-1].split('_')[3]; ds2 = indir_s.split('/')[-1].split('_')[3]\n",
    "            sat = 'LS'\n",
    "        elif 'S2' in indir_m and 'S2' in indir_s:\n",
    "            ds1 = indir_m.split('/')[-1][9:17]; ds2 = indir_s.split('/')[-1][9:17]\n",
    "            sat = 'S2'\n",
    "        elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "            ds1 = indir_m.split('/')[-1].split('_')[1]\n",
    "            ds2 = indir_s.split('/')[-1].split('_')[1]\n",
    "            sat = 'PS'\n",
    "        else:\n",
    "            raise Exception('Optical data NOT supported yet!')\n",
    "\n",
    "        # read in the images\n",
    "        DS1 = gdal.Open(indir_m); DS2 = gdal.Open(indir_s)\n",
    "        I1 = DS1.ReadAsArray(xoff=x1a, yoff=y1a, xsize=xsize1, ysize=ysize1)\n",
    "        I1 = I1.astype(np.float32)\n",
    "        I2 = DS2.ReadAsArray(xoff=x2a, yoff=y2a, xsize=xsize2, ysize=ysize2)\n",
    "        I2 = I2.astype(np.float32)\n",
    "        DS1=None; DS2=None # clear DS1 and DS2\n",
    "\n",
    "        # Initialize autoRIFT object (from runAutorift())\n",
    "        obj = autoRIFT_ISCE()\n",
    "        obj.configure()\n",
    "\n",
    "    elif img_type == 'SAR': ############# SAR SETTINGS #############################  \n",
    "        print('Processing SAR images with autoRIFT.'); print()\n",
    "        optflag = 0 # turn off opt flag\n",
    "        # Read in the two SAR images (from loadProduct())\n",
    "        img1 = IML.mmapFromISCE(filename1, logging); I1 = IMG.bands[0]\n",
    "        img2 = IML.mmapFromISCE(filename2, logging); I2 = IMG.bands[0]\n",
    "        I1 = np.abs(I1); I2 = np.abs(I2) # SAR amplitude only\n",
    "        \n",
    "    else:\n",
    "        print(\"Image type not recognized. Use either 'OPT' or 'SAR'.\")\n",
    "        \n",
    "    ############# Initialize autoRIFT object (from runAutorift()) ##################\n",
    "    obj = autoRIFT_ISCE()\n",
    "    obj.configure()\n",
    "    \n",
    "    obj.MultiThread = mpflag # multiprocessing\n",
    "    obj.I1 = I1; obj.I2 = I2 # assign the images\n",
    "    obj.xGrid = xGrid; obj.yGrid = yGrid # assign the grid \n",
    "\n",
    "    # GENERATE NO DATA MASK\n",
    "    # where offset searching will be skipped based on \n",
    "    # 1) imported nodata mask and/or 2) zero values in the image\n",
    "    for ii in range(obj.xGrid.shape[0]):\n",
    "        for jj in range(obj.xGrid.shape[1]):\n",
    "            if (obj.yGrid[ii,jj] != nodata)&(obj.xGrid[ii,jj] != nodata):\n",
    "                if (I1[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]==0)|(I2[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]==0):\n",
    "                    noDataMask[ii,jj] = True\n",
    "                    \n",
    "    # SEARCH RANGE\n",
    "    if SRx0 is None:\n",
    "        # default is a zero array\n",
    "#        ###########     uncomment to customize SearchLimit based on velocity distribution \n",
    "        if Dx0 is not None:\n",
    "            obj.SearchLimitX = np.int32(4+(25-4)/(np.max(np.abs(Dx0[np.logical_not(noDataMask)]))-np.min(np.abs(Dx0[np.logical_not(noDataMask)])))*(np.abs(Dx0)-np.min(np.abs(Dx0[np.logical_not(noDataMask)]))))\n",
    "        else:\n",
    "            obj.SearchLimitX = 15\n",
    "        obj.SearchLimitY = 15\n",
    "#        ###########\n",
    "        obj.SearchLimitX = obj.SearchLimitX * np.logical_not(noDataMask)\n",
    "        obj.SearchLimitY = obj.SearchLimitY * np.logical_not(noDataMask)\n",
    "    else:\n",
    "        obj.SearchLimitX = SRx0\n",
    "        obj.SearchLimitY = SRy0\n",
    "       ############ add buffer to search range\n",
    "        obj.SearchLimitX[obj.SearchLimitX!=0] = obj.SearchLimitX[obj.SearchLimitX!=0] + 2\n",
    "        obj.SearchLimitY[obj.SearchLimitY!=0] = obj.SearchLimitY[obj.SearchLimitY!=0] + 2\n",
    "    \n",
    "    # CHIP SIZE\n",
    "    if CSMINx0 is not None:\n",
    "        obj.ChipSizeMaxX = CSMAXx0\n",
    "        obj.ChipSizeMinX = CSMINx0\n",
    "        \n",
    "        gridspacingx = MINCHIPSIZE # use the grid spacing from above\n",
    "        chipsizex0 = MINCHIPSIZE\n",
    "        pixsizex = trans[1] # grab from coregister function\n",
    "    \n",
    "        obj.ChipSize0X = int(np.ceil(chipsizex0/pixsizex/4)*4)\n",
    "        obj.GridSpacingX = int(obj.ChipSize0X*gridspacingx/chipsizex0)\n",
    "\n",
    "        RATIO_Y2X = CSMINy0/CSMINx0\n",
    "        obj.ScaleChipSizeY = np.median(RATIO_Y2X[(CSMINx0!=nodata)&(CSMINy0!=nodata)])\n",
    "#         obj.ScaleChipSizeY = 1 # USE SCALE OF 1 for square pixels\n",
    "    else:\n",
    "        if ((optflag == 1)&(xGrid is not None)):\n",
    "            obj.ChipSizeMaxX = 32 # pixels\n",
    "            obj.ChipSizeMinX = 16 # pixels\n",
    "            obj.ChipSize0X = 16 # pixels\n",
    "    \n",
    "    # DOWNSTREAM SEARCH OFFSET\n",
    "    if Dx0 is not None:\n",
    "        obj.Dx0 = Dx0\n",
    "        obj.Dy0 = Dy0\n",
    "    else:\n",
    "        obj.Dx0 = obj.Dx0 * np.logical_not(noDataMask)\n",
    "        obj.Dy0 = obj.Dy0 * np.logical_not(noDataMask)\n",
    "\n",
    "    # REPLACE NO DATA VALUES WITH 0\n",
    "    obj.xGrid[noDataMask] = 0\n",
    "    obj.yGrid[noDataMask] = 0\n",
    "    obj.Dx0[noDataMask] = 0\n",
    "    obj.Dy0[noDataMask] = 0\n",
    "    if SRx0 is not None:\n",
    "        obj.SearchLimitX[noDataMask] = 0\n",
    "        obj.SearchLimitY[noDataMask] = 0\n",
    "    if CSMINx0 is not None:\n",
    "        obj.ChipSizeMaxX[noDataMask] = 0\n",
    "        obj.ChipSizeMinX[noDataMask] = 0\n",
    "    \n",
    "    # convert azimuth offset to vertical offset as used in autoRIFT convention for SAR images\n",
    "    if optflag == 0:\n",
    "        obj.Dy0 = -1 * obj.Dy0\n",
    "        \n",
    "    ############## AutoRIFT Pre-processing (from runAutorift()) ############################\n",
    "    t1 = time.time()\n",
    "    print(\"Pre-process Start!!!\")\n",
    "    \n",
    "    # FILTERING:\n",
    "    if FILTER == 'WAL': \n",
    "        obj.preprocess_filt_wal() # WALLIS FILTER\n",
    "#         obj.zeroMask = 1 # removes edges\n",
    "        obj.WallisFilterWidth = WALLISFILTERWIDTH # optional, default supposedly 21\n",
    "    elif FILTER == 'HPS':\n",
    "        obj.preprocess_filt_hps() # HIGH PASS FILTER\n",
    "    elif FILTER == 'SOB':\n",
    "        obj.preprocess_filt_sob() # SOBEL FILTER\n",
    "    elif FILTER == 'LAP':\n",
    "        obj.preprocess_filt_lap()\n",
    "    elif FILTER == 'DB':\n",
    "        obj.preprocess_db() # LOGARITHMIC OPERATOR (NO FILTER), FOR TOPOGRAPHY\n",
    "    else:\n",
    "        print(FILTER, 'not recognized. Using default high pass filter instead.')\n",
    "        obj.preprocess_filt_hps() # HIGH PASS FILTER\n",
    "        \n",
    "    print(\"Pre-process Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # CONVERT TO UNIFORM DATA TYPE\n",
    "    t1 = time.time()\n",
    "#    obj.DataType = 0\n",
    "    obj.uniform_data_type()\n",
    "    print(\"Uniform Data Type Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # OTHER :\n",
    "    obj.sparseSearchSampleRate = 1\n",
    "#    obj.colfiltChunkSize = 4\n",
    "\n",
    "    obj.OverSampleRatio = 64\n",
    "    if CSMINx0 is not None:\n",
    "        obj.OverSampleRatio = {obj.ChipSize0X:16,obj.ChipSize0X*2:32,obj.ChipSize0X*4:64,obj.ChipSize0X*8:64}\n",
    "    \n",
    "    # SEE ORIGINAL CODE TO EXPORT PREPROCESSED IMAGES\n",
    "    \n",
    "    ####################### Run AutoRIFT (from runAutorift())  ############################\n",
    "    t1 = time.time()\n",
    "    print(\"AutoRIFT Start!!!\")\n",
    "    obj.runAutorift()\n",
    "    print(\"AutoRIFT Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    noDataMask = cv2.dilate(noDataMask.astype(np.uint8),kernel,iterations = 1)\n",
    "    noDataMask = noDataMask.astype(np.bool)\n",
    "\n",
    "    # AT THIS POINT, THESE VARIABLES WILL BE CREATED:\n",
    "    # obj.Dx, obj.Dy, obj.InterpMask, obj.ChipSizeX, obj.GridSpacingX, \n",
    "    # obj.ScaleChipSizeY, obj.SearchLimitX, obj.SearchLimitY, obj.origSize, noDataMask\n",
    "    \n",
    "    # PLOT RESULTS\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15,5))\n",
    "    im1 = ax1.imshow(obj.Dx); ax1.set_title('Dx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(obj.Dy); ax2.set_title('Dy'); fig.colorbar(im2, ax=ax2)\n",
    "    im3 = ax3.imshow(np.sqrt((obj.Dx**2) + (obj.Dy**2))); ax3.set_title('D_total'); fig.colorbar(im3,ax=ax3)\n",
    "    plt.suptitle(ds1+' to '+ds2)\n",
    "    plt.show()\n",
    "\n",
    "    ####################### Write outputs (from runAutorift())  ############################\n",
    "    t1 = time.time()\n",
    "    print(\"Write Outputs Start!!!\")\n",
    "          \n",
    "    # Write text file with parameters\n",
    "    f =  open(out_path+'parameters_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.txt', 'w')\n",
    "    f.write('Geogrid/AutoRIFT parameters for offset_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.tif:')\n",
    "    f.write('NO_DATA_VAL: '+str(NO_DATA_VAL))\n",
    "    f.write('Min chip size: '+str(MINCHIPSIZE))\n",
    "    f.write('DEM: '+dem)\n",
    "    f.write('dhdx: '+dhdx); f.write('dhdy: '+dhdy)\n",
    "    f.write('vx: '+vx); f.write('vy: '+vy)\n",
    "    f.write('srx: '+srx); f.write('sry: '+sry)\n",
    "    f.write('csminx: '+csminx); f.write('csminy: '+csminy)\n",
    "    f.write('csmaxx: '+csmaxx); f.write('csmaxy: '+csmaxy)\n",
    "    f.write('stable surface mask: '+ssm)\n",
    "    f.write('FILTER: '+FILTER)\n",
    "    f.write('WALLISFILTERWIDTH: '+str(WALLISFILTERWIDTH))\n",
    "    f.write('Spare search sample rate: '+str(SPARSE_SEARCH_SAMPLE_RATE))\n",
    "    f.write('Oversample ratio: '+str(OVERSAMPLE_RATIO))\n",
    "    if offset2vx is not None and offset2vy is not None:\n",
    "        f.write('Velocity.TIF file created.')\n",
    "    else:\n",
    "        f.write('Velocity.TIF not created.')\n",
    "    f.close() # close the parameter text file\n",
    "          \n",
    "    # open the window_location.tif file to gdalinfo\n",
    "    ds = gdal.Open(gp+'window_location.tif')\n",
    "    tran = ds.GetGeoTransform()\n",
    "    proj = ds.GetProjection()\n",
    "    srs = ds.GetSpatialRef()\n",
    "    \n",
    "    # initialize arrays\n",
    "    DX = np.zeros(origSize,dtype=np.float32) * np.nan; DY = np.zeros(origSize,dtype=np.float32) * np.nan\n",
    "    INTERPMASK = np.zeros(origSize,dtype=np.float32); CHIPSIZEX = np.zeros(origSize,dtype=np.float32)\n",
    "    SEARCHLIMITX = np.zeros(origSize,dtype=np.float32); SEARCHLIMITY = np.zeros(origSize,dtype=np.float32)\n",
    "    \n",
    "    # fill in arays\n",
    "    Dx = obj.Dx; Dy = obj.Dy; InterpMask = obj.InterpMask; ChipSizeX = obj.ChipSizeX\n",
    "    SearchLimitX = obj.SearchLimitX; SearchLimitY = obj.SearchLimitY\n",
    "    DX[0:Dx.shape[0],0:Dx.shape[1]] = Dx;  DY[0:Dy.shape[0],0:Dy.shape[1]] = Dy\n",
    "    INTERPMASK[0:InterpMask.shape[0],0:InterpMask.shape[1]] = InterpMask\n",
    "    CHIPSIZEX[0:ChipSizeX.shape[0],0:ChipSizeX.shape[1]] = ChipSizeX\n",
    "    SEARCHLIMITX[0:SearchLimitX.shape[0],0:SearchLimitX.shape[1]] = SearchLimitX\n",
    "    SEARCHLIMITY[0:SearchLimitY.shape[0],0:SearchLimitY.shape[1]] = SearchLimitY\n",
    "    \n",
    "    # mask out no data\n",
    "    DX[noDataMask] = np.nan; DY[noDataMask] = np.nan\n",
    "    INTERPMASK[noDataMask] = 0; CHIPSIZEX[noDataMask] = 0\n",
    "    SEARCHLIMITX[noDataMask] = 0; SEARCHLIMITY[noDataMask] = 0\n",
    "    if SSM is not None:\n",
    "        SSM[noDataMask] = False\n",
    "    DX[SEARCHLIMITX == 0] = np.nan; DY[SEARCHLIMITX == 0] = np.nan\n",
    "    INTERPMASK[SEARCHLIMITX == 0] = 0; CHIPSIZEX[SEARCHLIMITX == 0] = 0\n",
    "    if SSM is not None:\n",
    "        SSM[SEARCHLIMITX == 0] = False\n",
    "\n",
    "    # SAVE TO OFFSET.MAT FILE\n",
    "    sio.savemat('offset_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.mat', # offset mat filename\n",
    "                {'Dx':DX,'Dy':DY,'InterpMask':INTERPMASK,'ChipSizeX':CHIPSIZEX})\n",
    "    print('Offset.mat written.')\n",
    "    \n",
    "    # CREATE THE GEOTIFFS\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # OFFSET.TIF\n",
    "    outRaster = driver.Create(\"offset_\"+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+\".tif\", # offset filename\n",
    "                              int(xGrid.shape[1]), int(xGrid.shape[0]), 5, gdal.GDT_Float32)\n",
    "    outRaster.SetGeoTransform(tran); outRaster.SetProjection(proj) # projections\n",
    "    outband = outRaster.GetRasterBand(1); outband.WriteArray(DX) # DX\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(2); outband.WriteArray(DY) # DY\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(3); outband.WriteArray(np.sqrt((DX**2) + (DY**2))) # DY\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(4); outband.WriteArray(INTERPMASK) # INTERPMASK\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(5); outband.WriteArray(CHIPSIZEX) # CHIPSIZE\n",
    "    outband.FlushCache()\n",
    "    del outRaster\n",
    "    print('Offset.tif written.')\n",
    "    \n",
    "    # VELOCITY.TIF\n",
    "    if offset2vx is not None and offset2vy is not None:\n",
    "        ds = gdal.Open(offset2vx) #### VX\n",
    "        band = ds.GetRasterBand(1); offset2vx_1 = band.ReadAsArray()\n",
    "        band = ds.GetRasterBand(2); offset2vx_2 = band.ReadAsArray()\n",
    "        if ds.RasterCount > 2:\n",
    "                band = ds.GetRasterBand(3)\n",
    "                offset2vr = band.ReadAsArray()\n",
    "        else:\n",
    "                offset2vr = None\n",
    "        band=None; ds=None\n",
    "        offset2vx_1[offset2vx_1 == nodata] = np.nan\n",
    "        offset2vx_2[offset2vx_2 == nodata] = np.nan\n",
    "\n",
    "        ds = gdal.Open(offset2vy) #### VY\n",
    "        band = ds.GetRasterBand(1); offset2vy_1 = band.ReadAsArray()\n",
    "        band = ds.GetRasterBand(2); offset2vy_2 = band.ReadAsArray()\n",
    "        if ds.RasterCount > 2:\n",
    "                band = ds.GetRasterBand(3)\n",
    "                offset2va = band.ReadAsArray()\n",
    "        else:\n",
    "                offset2va = None\n",
    "        band=None; ds=None\n",
    "        offset2vy_1[offset2vy_1 == nodata] = np.nan; offset2vy_2[offset2vy_2 == nodata] = np.nan\n",
    "        \n",
    "        if offset2va is not None:\n",
    "            offset2va[offset2va == nodata] = np.nan\n",
    "\n",
    "        VX = offset2vx_1 * DX + offset2vx_2 * DY\n",
    "        VY = offset2vy_1 * DX + offset2vy_2 * DY\n",
    "        VX = VX.astype(np.float32); VY = VY.astype(np.float32)\n",
    "\n",
    "        outRaster = driver.Create(\"velocity_\"+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+\".tif\", # velocity filename\n",
    "                                  int(xGrid.shape[1]), int(xGrid.shape[0]), 3, gdal.GDT_Float32)\n",
    "        outRaster.SetGeoTransform(tran); outRaster.SetProjection(proj)\n",
    "        outband = outRaster.GetRasterBand(1); outband.WriteArray(VX) # VX\n",
    "        outband.FlushCache()\n",
    "        outband = outRaster.GetRasterBand(2); outband.WriteArray(VY) # VY\n",
    "        outband.FlushCache()\n",
    "        outband = outRaster.GetRasterBand(3); outband.WriteArray(np.sqrt((VX**2) + (VY**2))) # V\n",
    "        outband.FlushCache()\n",
    "        del outRaster\n",
    "        print('Velocity.tif written.')\n",
    "    \n",
    "    print(\"Write Outputs Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # Move files produced to the out_path directory\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if 'offset' in file or ('velocity' in file and '.tif' in file):\n",
    "            shutil.move(os.getcwd()+'/'+file, out_path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, dataName, metaName = loadImage(temp_dir+'reference/IW1.xml')\n",
    "# dataName = dataName+'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IML.memmap(dataName, nchannels=img.bands,\n",
    "#             nxx=img.width, nyy=img.length, scheme=img.scheme,\n",
    "#             dataType=IML.NUMPY_type(img.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loadProduct(temp_dir+'coarse_coreg/overlaps/IW1/burst_bot_02_03.slc.vrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memmap(dataName+'.vrt', nchannels=img.bands, nxx=img.width, nyy=img.length, scheme=img.scheme,\n",
    "#             dataType=IML.NUMPY_type(img.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = temp_dir+'coarse_coreg/overlaps/IW1/burst_bot_02_03.slc.vrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(temp_dir+'reference/IW1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmapFromISCE(temp_dir+'reference/IW1/burst_01.slc.vrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE REFERENCE AND SECONDARY IMAGES (ISCE OR GDAL), REQUIRES .XML AND .VRT (only burst xmls?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jukesliu/Documents/PhD/ISCE_Aug2022/insar/secondary/IW1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, dataName, metaName = loadImage('/Users/jukesliu/Documents/PhD/ISCE_Aug2022/dense_offsets_ampcor/SK_test/fine_interferogram/IW2.xml')\n",
    "img.width\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from imageMath import IML\n",
    "    \n",
    "def loadProduct(filename):\n",
    "    '''\n",
    "    Load the product using Product Manager.\n",
    "    '''\n",
    "    import isce\n",
    "    import logging\n",
    "    from imageMath import IML\n",
    "\n",
    "    IMG = IML.mmapFromISCE(filename, logging)\n",
    "    img = IMG.bands[0]\n",
    "#    pdb.set_trace()\n",
    "    return img\n",
    "\n",
    "def mmapFromISCE(fname, logger=None):\n",
    "    '''\n",
    "    Create a file mmap object using information in an ISCE XML.\n",
    "    '''\n",
    "    try:\n",
    "        img, dataName, metaName = loadImage(fname)\n",
    "        isceFile = True\n",
    "        dataName = dataName+'.vrt'\n",
    "    except:\n",
    "        try:\n",
    "            img = loadGDALImage(fname)\n",
    "            isceFile=False\n",
    "            dataName = fname\n",
    "        except:\n",
    "            raise Exception('Input file: {0} should either be an ISCE image / GDAL image. Appears to be neither'.format(fname))\n",
    "\n",
    "#     if logger is not None:\n",
    "#         logger.debug('Creating readonly ISCE mmap with \\n' +\n",
    "#             'file = %s \\n'%(dataName) + \n",
    "#             'bands = %d \\n'%(img.bands) + \n",
    "#             'width = %d \\n'%(img.width) + \n",
    "#             'length = %d \\n'%(img.length)+\n",
    "#             'scheme = %s \\n'%(img.scheme) +\n",
    "#             'dtype = %s \\n'%(img.dataType))\n",
    "    \n",
    "    if isceFile:\n",
    "        mObj = IML.memmap(dataName, nchannels=img.bands,\n",
    "            nxx=img.width, nyy=img.length, scheme=img.scheme,\n",
    "            dataType=IML.NUMPY_type(img.dataType))\n",
    "    else:\n",
    "        mObj = memmapGDAL(dataName)\n",
    "\n",
    "    return mObj\n",
    "\n",
    "def loadImage(fname):\n",
    "    '''\n",
    "    Load into appropriate image object.\n",
    "    '''\n",
    "    try:\n",
    "        import iscesys\n",
    "        import isceobj\n",
    "        from iscesys.Parsers.FileParserFactory import createFileParser\n",
    "    except:\n",
    "        raise ImportError('ISCE has not been installed or is not importable')\n",
    "\n",
    "    if not fname.endswith('.xml'):\n",
    "        dataName = fname\n",
    "        metaName = fname[:-4] + '.xml'\n",
    "    else:\n",
    "        metaName = fname\n",
    "        dataName = os.path.splitext(fname)[0]\n",
    "\n",
    "    parser = createFileParser('xml')\n",
    "    prop,fac,misc = parser.parse(metaName)\n",
    "\n",
    "    if 'reference' in prop:\n",
    "        img=isceobj.createDemImage()\n",
    "        img.init(prop,fac,misc)\n",
    "    elif 'number_good_bytes' in prop:\n",
    "        img = isceobj.createRawImage()\n",
    "        img.init(prop,fac,misc)\n",
    "    else:\n",
    "        img = isceobj.createImage()\n",
    "        img.init(prop,fac,misc)\n",
    "\n",
    "    img.setAccessMode('READ')\n",
    "    return img, dataName, metaName\n",
    "\n",
    "def loadGDALImage(fname):\n",
    "    '''\n",
    "    Similar to loadImage but only returns metadata.\n",
    "    '''\n",
    "\n",
    "    from osgeo import gdal\n",
    "\n",
    "    class Dummy(object):\n",
    "        pass\n",
    "\n",
    "\n",
    "    ds = gdal.Open(fname, gdal.GA_ReadOnly)\n",
    "    drv = ds.GetDriver()\n",
    "    bnd = ds.GetRasterBand(1)\n",
    "\n",
    "    img = Dummy()\n",
    "    img.bands = ds.RasterCount \n",
    "    img.width = ds.RasterXSize\n",
    "    img.length = ds.RasterYSize\n",
    "    img.scheme = drv.GetDescription()\n",
    "    if bnd.DataType == 10:\n",
    "        img.dataType = 'CFLOAT'\n",
    "    else:\n",
    "        img.dataType = gdal.GetDataTypeByName(bnd.DataType)\n",
    "\n",
    "    bnd = None\n",
    "    drv = None\n",
    "    ds = None\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class memmapGDAL(object):\n",
    "#     '''\n",
    "#     Create a memmap like object from GDAL.\n",
    "#     '''\n",
    "\n",
    "#     from osgeo import gdal\n",
    "\n",
    "#     class BandWrapper:\n",
    "#         '''\n",
    "#         Wrap a GDAL band in a numpy like slicable object.\n",
    "#         '''\n",
    "\n",
    "#         def __init__(self, dataset, band):\n",
    "#             '''\n",
    "#             Init from a GDAL raster band.\n",
    "#             '''\n",
    "\n",
    "#             self.data = dataset.GetRasterBand(band)\n",
    "#             self.width = dataset.RasterXSize\n",
    "#             self.length = data.RasterYSize\n",
    "\n",
    "#         def __getitem__(self, *args):\n",
    "            \n",
    "#             xmin = max(int(args[0][1].start),0)\n",
    "#             xmax = min(int(args[0][1].stop)+xmin, self.width) - xmin\n",
    "#             ymin = max(int(args[0][0].start),0)\n",
    "#             ymax = min(int(args[0][1].stop)+ymin, self.length) - ymin\n",
    "\n",
    "#             res = self.data.ReadAsArray(xmin, ymin, xmax,ymax)\n",
    "#             return res\n",
    "\n",
    "#         def __del__(self):\n",
    "#             self.data = None\n",
    "\n",
    "\n",
    "#     def __init__(self, fname):\n",
    "#         '''\n",
    "#         Constructor.\n",
    "#         '''\n",
    "        \n",
    "#         self.name = fname\n",
    "#         self.data = gdal.Open(self.name, gdal.GA_ReadOnly)\n",
    "#         self.width = self.data.RasterXSize\n",
    "#         self.length = self.data.RasterYSize\n",
    "\n",
    "#         self.bands = []\n",
    "#         for ii in range(self.data.RasterCount):\n",
    "#             self.bands.append( BandWrapper(self.data, ii+1))\n",
    "\n",
    "#     def __del__(self):\n",
    "#         self.data = None\n",
    "\n",
    "# class memmap(object):\n",
    "#     '''Create the memap object.'''\n",
    "#     def __init__(self,fname, mode='readonly', nchannels=1, nxx=None, nyy=None, scheme='BSQ', dataType='f'):\n",
    "#         '''Init function.'''\n",
    "\n",
    "#         fsize = np.zeros(1, dtype=dataType).itemsize\n",
    "\n",
    "#         if nxx is None:\n",
    "#             raise ValueError('Undefined file width for : %s'%(fname))\n",
    "\n",
    "#         if mode=='write':\n",
    "#             if nyy is None:\n",
    "#                 raise ValueError('Undefined file length for opening file: %s in write mode.'%(fname))\n",
    "#         else:\n",
    "#             try:\n",
    "#                 nbytes = os.path.getsize(fname)\n",
    "#             except:\n",
    "#                 raise ValueError('Non-existent file : %s'%(fname))\n",
    "\n",
    "# #            if nyy is None:\n",
    "# #                nyy = nbytes//(fsize*nchannels*nxx)\n",
    "# #\n",
    "# #                if (nxx*nyy*fsize*nchannels) != nbytes:\n",
    "# #                    raise ValueError('File size mismatch for %s. Fractional number of lines'%(fname))\n",
    "# #            elif (nxx*nyy*fsize*nchannels) > nbytes:\n",
    "# #                    raise ValueError('File size mismatch for %s. Number of bytes expected: %d'%(nbytes))\n",
    "             \n",
    "\n",
    "#         self.name = fname\n",
    "#         self.width = nxx\n",
    "#         self.length = nyy\n",
    "\n",
    "#         ####List of memmap objects\n",
    "#         acc = []\n",
    "\n",
    "#         ####Create the memmap for the full file\n",
    "#         nshape = nchannels*nyy*nxx\n",
    "#         omap = np.memmap(fname, dtype=dataType, mode=mode, \n",
    "#                 shape = (nshape,))\n",
    "\n",
    "#         if scheme.upper() == 'BIL':\n",
    "#             nstrides = (nchannels*nxx*fsize, fsize)\n",
    "\n",
    "#             for band in range(nchannels):\n",
    "#                 ###Starting offset\n",
    "#                 noffset = band*nxx\n",
    "\n",
    "#                 ###Temporary view\n",
    "#                 tmap = omap[noffset:]\n",
    "\n",
    "#                 ####Trick it into creating a 2D array\n",
    "#                 fmap = as_strided(tmap, shape=(nyy,nxx), strides=nstrides)\n",
    "\n",
    "#                 ###Add to list of objects\n",
    "#                 acc.append(fmap)\n",
    "\n",
    "#         elif scheme.upper() == 'BSQ':\n",
    "#             nstrides = (fsize, fsize)\n",
    "\n",
    "#             for band in range(nchannels):\n",
    "#                 ###Starting offset\n",
    "#                 noffset = band*nxx*nyy\n",
    "\n",
    "#                 ###Temporary view\n",
    "#                 tmap = omap[noffset:noffset+nxx*nyy]\n",
    "\n",
    "#                 ####Reshape into 2D array\n",
    "#                 fmap = as_strided(tmap, shape=(nyy,nxx))\n",
    "\n",
    "#                 ###Add to lits of objects\n",
    "#                 acc.append(fmap)\n",
    "\n",
    "#         elif scheme.upper() == 'BIP':\n",
    "#             nstrides = (nchannels*nxx*fsize,nchannels*fsize)\n",
    "\n",
    "#             for band in range(nchannels):\n",
    "#                 ####Starting offset\n",
    "#                 noffset = band\n",
    "\n",
    "#                 ####Temporary view\n",
    "#                 tmap = omap[noffset:]\n",
    "\n",
    "#                 ####Trick it into interpreting ot as a 2D array\n",
    "#                 fmap = as_strided(tmap, shape=(nyy,nxx), strides=nstrides)\n",
    "\n",
    "#                 ####Add to the list of objects\n",
    "#                 acc.append(fmap)\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError('Unknown file scheme: %s for file %s'%(scheme,fname))\n",
    "\n",
    "#         ######Assigning list of objects to self.bands\n",
    "#         self.bands = acc\n",
    "\n",
    "#     def flush(self):\n",
    "#         '''\n",
    "#         If mmap opened in write mode, would be useful to have flush functionality on old systems.\n",
    "#         '''\n",
    "\n",
    "#         self.bands[0].base.base.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gdal2isce_xml(fname):\n",
    "#     \"\"\"\n",
    "#     Generate ISCE xml file from gdal supported file\n",
    "\n",
    "#     Example: import isce\n",
    "#              from applications.gdal2isce_xml import gdal2isce_xml\n",
    "#              xml_file = gdal2isce_xml(fname+'.vrt')\n",
    "#     \"\"\"\n",
    "\n",
    "#     # open the GDAL file and get typical data informationi\n",
    "#     GDAL2ISCE_DATATYPE = {\n",
    "#        1 : 'BYTE',\n",
    "#        2 : 'uint16',\n",
    "#        3 : 'SHORT',\n",
    "#        4 : 'uint32',\n",
    "#        5 : 'INT',\n",
    "#        6 : 'FLOAT',\n",
    "#        7 : 'DOUBLE',\n",
    "#        10: 'CFLOAT',\n",
    "#        11: 'complex128',\n",
    "#     }\n",
    "# #    GDAL2NUMPY_DATATYPE = {\n",
    "# #       1 : np.uint8,\n",
    "# #       2 : np.uint16,\n",
    "# #       3 : np.int16,\n",
    "# #       4 : np.uint32,\n",
    "# #       5 : np.int32,\n",
    "# #       6 : np.float32,\n",
    "# #       7 : np.float64,\n",
    "# #       10: np.complex64,\n",
    "# #       11: np.complex128,\n",
    "# #     }\n",
    "\n",
    "#     # check if the input file is a vrt\n",
    "#     fbase, fext = os.path.splitext(fname)\n",
    "#     print(fext)\n",
    "#     if fext == \".vrt\":\n",
    "#         outname = fbase\n",
    "#     else:\n",
    "#         outname = fname\n",
    "#     print(outname)\n",
    "\n",
    "#     # open the GDAL file and get typical ds information\n",
    "#     ds =  gdal.Open(fname, gdal.GA_ReadOnly)\n",
    "#     width = ds.RasterXSize\n",
    "#     length = ds.RasterYSize\n",
    "#     bands = ds.RasterCount\n",
    "#     print(\"width:       \" + \"\\t\" + str(width))\n",
    "#     print(\"length:      \" + \"\\t\" + str(length))\n",
    "#     print(\"num of bands:\" + \"\\t\" + str(bands))\n",
    "\n",
    "#     # getting the datatype information\n",
    "#     raster = ds.GetRasterBand(1)\n",
    "#     dataTypeGdal = raster.DataType\n",
    "\n",
    "#     # user look-up dictionary from gdal to isce format\n",
    "#     dataType= GDAL2ISCE_DATATYPE[dataTypeGdal]\n",
    "#     print(\"dataType: \" + \"\\t\" + str(dataType))\n",
    "\n",
    "#     # transformation contains gridcorners (lines/pixels or lonlat and the spacing 1/-1 or deltalon/deltalat)\n",
    "#     transform = ds.GetGeoTransform()\n",
    "#     # if a complex data type, then create complex image\n",
    "#     # if a real data type, then create a regular image\n",
    "\n",
    "#     img = isceobj.createImage()\n",
    "#     img.setFilename(os.path.abspath(outname))\n",
    "#     img.setWidth(width)\n",
    "#     img.setLength(length)\n",
    "#     img.setAccessMode('READ')\n",
    "#     img.bands = bands\n",
    "#     img.dataType = dataType\n",
    "\n",
    "#     # interleave\n",
    "#     md = ds.GetMetadata('IMAGE_STRUCTURE')\n",
    "#     sch = md.get('INTERLEAVE', None)\n",
    "#     if sch == 'LINE':\n",
    "#         img.scheme = 'BIL'\n",
    "#     elif sch == 'PIXEL':\n",
    "#         img.scheme = 'BIP'\n",
    "#     elif sch == 'BAND':\n",
    "#         img.scheme = 'BSQ'\n",
    "#     else:\n",
    "#         print('Unrecognized interleaving scheme, {}'.format(sch))\n",
    "#         print('Assuming default, BIP')\n",
    "#         img.scheme = 'BIP'\n",
    "\n",
    "#     img.firstLongitude = transform[0]\n",
    "#     img.firstLatitude = transform[3] \n",
    "#     img.deltaLatitude = transform[5] \n",
    "#     img.deltaLongitude = transform[1]\n",
    "\n",
    "#     xml_file = outname + \".xml\"\n",
    "#     img.dump(xml_file)\n",
    "\n",
    "#     return xml_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run geogrid and autoRIFT on all images in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s2path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel2/' # input S2 images\n",
    "LS8path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/LS8images/useable_images/' # input LS8 images\n",
    "PSpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/Planet_test/Planet_test_all/'\n",
    "S1path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/'\n",
    "# PSpath = '/Volumes/SGlacier/surge_projects/Working_clipped_2019/'\n",
    "# boxpath = '/Users/surging/Documents/TG/BoxTurner/BoxTurner_UTM_07.shp' # the shapefile for Turner\n",
    "# autoriftpath = '/Users/surging/Documents/TG/autoRIFT/' # path to the autorift scripts\n",
    "# vmap_path = '/Users/surging/Documents/TG/vmap_test/' # output velocity map folder\n",
    "# basepath = '/Users/surging/Documents/TG/optical-offset-tracking/' # path where this script is located\n",
    "\n",
    "######### Set minimum and maximum time separation and the platform (L8, S2, PS, S1) ###############\n",
    "platform = 'L8'\n",
    "startdate = '20130101' # inclusive start date\n",
    "enddate = '20160101' # inclusive end date\n",
    "min_dt = 3 # minimum time separation between images\n",
    "max_dt = 30 # maximum time separation between images\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the folder path based on the platform\n",
    "if platform == 'S2': # sentinel-2\n",
    "    path = s2path\n",
    "    ext = '_clipped.tif' # image filename extension\n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 2 # split filename by underscore, index corresponds to image date\n",
    "elif platform == 'L8': # landsat 8\n",
    "    path = LS8path\n",
    "    ext = '.TIF'\n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 3\n",
    "elif platform == 'PS': # PlanetScope\n",
    "    path = PSpath\n",
    "    ext = '_clipped.tif' \n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 1\n",
    "elif platform == 'S1': # sentinel-1\n",
    "    path = S1path\n",
    "    ext = '.zip' \n",
    "    img_type = 'SAR'\n",
    "    date_split_idx = 5\n",
    "else:\n",
    "    print('Platform', platform, 'not recognized. Options are \"S2\", \"L8\", \"PS\", and \"S1\"')\n",
    "\n",
    "# record all possible images and their dates\n",
    "dates = []; files = []  \n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(ext):\n",
    "        date = file.split('_')[date_split_idx] # grab the date from the filename\n",
    "        if platform == 'S1': # S1 filenames need another split\n",
    "            date = date[:8]\n",
    "        dates.append(date); files.append(file) # store the date and filename\n",
    "files_df = pd.DataFrame(list(zip(files,dates)),columns=['filename','date'])\n",
    "files_df = files_df.sort_values(by='date',ignore_index=True) # sort the dataframe by ascending date\n",
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out those before and afte the input start and end date\n",
    "files_df = files_df[(files_df.date >= startdate) & (files_df.date <= enddate)]\n",
    "files_df = files_df.reset_index(drop=True) # reset index for searching\n",
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PARAMETERS FOR GEOGRID #################################################### \n",
    "out_path = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/output_AutoRIFT/' # output file path\n",
    "dem = dempath+dem_outfile # path to the resampled DEM produced in the previous step (outfile name)\n",
    "#             MINCHIPSIZE = CHIPSIZE_M # smallest chip size allowed in image horizontal direction (in m)\n",
    "#             print(MINCHIPSIZE)\n",
    "NO_DATA_VAL = -32767 # no data value in the output products\n",
    "temp_dir = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/coreg_files/'\n",
    "\n",
    "# PARAMETERS FOR AUTORIFT CHOSEN\n",
    "# options are HPS (high pass), WAL (wallis), SOB (sobel), DB (logarithmic operator)\n",
    "FILTER = 'HPS'\n",
    "WALLISFILTERWIDTH = 32 # only for wallis filter, must be a power of 2\n",
    "\n",
    "# Sampling:\n",
    "SPARSE_SEARCH_SAMPLE_RATE = 16 # how many samples to skip to speed up processing\n",
    "OVERSAMPLE_RATIO = 0 # enter in a constant scalar or 0 for default parameters\n",
    "print('Filters and autoRIFT sample rates chosen.')\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ENTER DEM INFO, AND REFERENCE VELOCITY INFO ########### \n",
    "# enter in the path to your best DEM over the region\n",
    "dempath = '/Users/jukesliu/Documents/TURNER/DATA/ICE_THICKNESS/surface/DEMs_previous/'\n",
    "demname = 'IfSAR_5m_DSM_clipped.tif'\n",
    "\n",
    "# path to the reference files for geogrid (vx, vy, ssm)\n",
    "refvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/forAutoRIFT/' \n",
    "vx_fname = 'vx_cropped.tif' # name of reference vx file\n",
    "vy_fname = 'vy_cropped.tif' # name of reference vy file\n",
    "\n",
    "sr_scaling = 16 # multiply by vx and vy to generate search range limits\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try multiple chip sizes and sats\n",
    "for platform in ['S2','L8']:\n",
    "    # assign the folder path based on the platform\n",
    "    if platform == 'S2': # sentinel-2\n",
    "        path = s2path\n",
    "        ext = '_clipped.tif' # image filename extension\n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 2 # split filename by underscore, index corresponds to image date\n",
    "    elif platform == 'L8': # landsat 8\n",
    "        path = LS8path\n",
    "        ext = '.TIF'\n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 3\n",
    "    elif platform == 'PS': # PlanetScope\n",
    "        path = PSpath\n",
    "        ext = '_clipped.tif' \n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 1\n",
    "    elif platform == 'S1': # sentinel-1\n",
    "        path = S1path\n",
    "        ext = '.zip' \n",
    "        img_type = 'SAR'\n",
    "        date_split_idx = 5\n",
    "    else:\n",
    "        print('Platform', platform, 'not recognized. Options are \"S2\", \"L8\", \"PS\", and \"S1\"')\n",
    "\n",
    "    # record all possible images and their dates\n",
    "    dates = []; files = []  \n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(ext):\n",
    "            date = file.split('_')[date_split_idx] # grab the date from the filename\n",
    "            if platform == 'S1': # S1 filenames need another split\n",
    "                date = date[:8]\n",
    "            dates.append(date); files.append(file) # store the date and filename\n",
    "    files_df = pd.DataFrame(list(zip(files,dates)),columns=['filename','date'])\n",
    "    files_df = files_df.sort_values(by='date',ignore_index=True) # sort the dataframe by ascending date\n",
    "    files_df = files_df[(files_df.date >= startdate) & (files_df.date <= enddate)]\n",
    "    files_df = files_df.reset_index(drop=True) # reset index for searching\n",
    "    \n",
    "    for min_chipsize in [100,160,200,300]: # MULTIPLE CHIP SIZES\n",
    "        MINCHIPSIZE = min_chipsize\n",
    "        # generate geogrid inputs and grab outfilenames\n",
    "        [dem_outfile, dhdx_outfile, dhdy_outfile, \n",
    "         vx_outfile, vy_outfile, srx_outfile, \n",
    "         sry_outfile, ssm_outfile, tg_outfile] = generate_geogrid_inputs(min_chipsize, dempath, demname, \n",
    "                                                                         refvpath, vx_fname, vy_fname, \n",
    "                                                                         sr_scaling)\n",
    "        #######################################################################\n",
    "        # # CHOOSE OPTIONAL INPUTS: (set as '' to leave blank)\n",
    "        # # 1) surface slope:\n",
    "        #   dhdx = ''; dhdy = ''  \n",
    "        dhdx = dempath+dhdx_outfile; dhdy = dempath+dhdy_outfile\n",
    "        \n",
    "        # # 2) reference velocity:\n",
    "        vx = ''; vy = ''    \n",
    "#         vx = refvpath+vx_outfile; vy = refvpath+vy_outfile\n",
    "        \n",
    "        # # 3) chip sizes:\n",
    "        csminx = ''; csminy = ''\n",
    "        csmaxx = ''; csmaxy = ''  \n",
    "        #             csminx = refvpath+csminx_fname; csminy = refvpath+csminy_fname\n",
    "        #             csmaxx = refvpath+csmaxx_fname; csmaxy = refvpath+csmaxy_fname\n",
    "        \n",
    "        # # 4) stable surface mask:\n",
    "        ssm = ''\n",
    "        #             ssm = refvpath+ssm_outfile # stable surface mask \n",
    "        \n",
    "        # # 5) search range limit:\n",
    "        srx = ''; sry = '' # for best results, never input search range limit\n",
    "        #######################################################################\n",
    "        \n",
    "        for rownum in range(0,len(files_df)-1):\n",
    "            if rownum == 0: # for the first row, idx1 = 0 and idx2 = 1\n",
    "                idx1 = rownum\n",
    "            idx2 = idx1+1 # reset index 2 as the next item \n",
    "\n",
    "            if idx1 < len(files_df) and idx2 < len(files_df): # don't surpass the end of the data\n",
    "                # identify the successive image pairs:\n",
    "                m = files_df.loc[idx1,'filename']; s = files_df.loc[idx2, 'filename']\n",
    "\n",
    "                # grab the two dates and convert to datetime objects\n",
    "                d1s = m.split('_')[date_split_idx]; d2s = s.split('_')[date_split_idx]\n",
    "                if platform == 'S1': # S1 filenames need another split\n",
    "                    d1s = d1s[:8]; d2s = d2s[:8]\n",
    "                \n",
    "                # Skip if output already exists for this combination of parameters\n",
    "#                 if os.path.exists(out_path+'offset_'+d1s+'_'+d2s+'_'+str(min_chipsize)+'m_'+platform+'.tif'):\n",
    "#                     print('offset_'+d1s+'_'+d2s+'_'+str(min_chipsize)+'m_'+platform+'.tif already exists.')\n",
    "#                 else:\n",
    "                if True:\n",
    "                    # calculate time separation\n",
    "                    d1 = datetime.datetime.strptime(d1s, '%Y%m%d'); d2 = datetime.datetime.strptime(d2s, '%Y%m%d')\n",
    "                    dt = d2-d1; dt = int(dt.days)\n",
    "\n",
    "                    # evaluate if dates are within the appropriate time separation\n",
    "                    while (dt < min_dt or dt > max_dt): # dt is outside of appropriate time separation\n",
    "                        idx2 += 1 # increment index 2\n",
    "\n",
    "                        if idx2 > len(files_df)-1:\n",
    "                            print('No image pair with correct time separation found. Skip to next.')\n",
    "                            idx2 = idx1+1 # reset index 2 as the next item \n",
    "                            m = None; s = None\n",
    "                            break\n",
    "\n",
    "                        # Keep looking for the correct time separation:\n",
    "                        m = files_df.loc[idx1,'filename']; s = files_df.loc[idx2, 'filename']\n",
    "\n",
    "                        # grab the two dates and convert to datetime objects\n",
    "                        d1s = m.split('_')[date_split_idx]; d2s = s.split('_')[date_split_idx]\n",
    "                        if platform == 'S1': # S1 filenames need another split\n",
    "                            d1s = d1s[:8]; d2s = d2s[:8]\n",
    "                        d1 = datetime.datetime.strptime(d1s, '%Y%m%d'); d2 = datetime.datetime.strptime(d2s, '%Y%m%d')\n",
    "                        dt = d2-d1; dt = int(dt.days)\n",
    "\n",
    "                    # run geogrid and autoRIFT\n",
    "                    if m is not None and s is not None:\n",
    "                        print(m,s)\n",
    "                        indir_m = path+m; indir_s = path+s # path to the two images\n",
    "                        dem = dempath+'IfSAR_'+str(MINCHIPSIZE)+'m_DSM_clipped.tif'\n",
    "                        dhdx = dempath+'IfSAR_'+str(MINCHIPSIZE)+'m_DSM_clipped_dhdx.tif'\n",
    "                        dhdy = dempath+'IfSAR_'+str(MINCHIPSIZE)+'m_DSM_clipped_dhdy.tif'\n",
    "                        ###################### RUN GEOGRID ################################\n",
    "                        run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                                            dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                                            temp_dir)\n",
    "\n",
    "                        ##################### PREP AUTORIFT ##############################\n",
    "                        gp = out_path # identify files produced from geogrid\n",
    "                        # remove all empty grids\n",
    "                        for grid in os.listdir(gp): \n",
    "                            if grid.startswith('window') and grid.endswith('.tif'):\n",
    "                                reader = rio.open(gp+grid) # read dataset\n",
    "                                data_found = False \n",
    "                                for band in range(1,reader.count+1):\n",
    "                                    testband = reader.read(band) # read in the band\n",
    "                                    if np.count_nonzero(testband[testband != NO_DATA_VAL]) > 0:\n",
    "                                        data_found = True\n",
    "                                if not data_found:\n",
    "                                    print(grid, 'has no data. Removed.')\n",
    "                                    os.remove(gp+grid)\n",
    "\n",
    "                        # fill in AutoRIFT parameters using the files\n",
    "                        mpflag = 0 # leave multiprocessing off\n",
    "\n",
    "                        # GRID LOCATION (required) from window_location.tif\n",
    "                        grid_location = rio.open(gp+'window_location.tif')\n",
    "                        xGrid = grid_location.read(1) # 1st band in window location\n",
    "                        yGrid = grid_location.read(2) # 2nd band in window location\n",
    "\n",
    "                        # optional parameters (default None or zero until filled)\n",
    "                        init_offset = None; search_range = None\n",
    "                        chip_size_min = None; chip_size_max = None\n",
    "                        offset2vx = None; offset2vy = None; stable_surface_mask = None\n",
    "                        Dx0 = None; Dy0 = None; CSMINx0 = None\n",
    "                        SRx0 = None; SRy0 = None;\n",
    "                        CSMAXx0 = None; CSMAXy0 = None; SSM = None\n",
    "                        noDataMask = np.zeros(xGrid.shape).astype(int)\n",
    "\n",
    "                        if os.path.exists(gp+'window_offset.tif'): # Dx0 and Dy0 from window_offset.tif\n",
    "                            init_offset = rio.open(gp+'window_offset.tif')\n",
    "                            Dx0 = init_offset.read(1); Dy0 = init_offset.read(2)\n",
    "                        if os.path.exists(gp+'window_search_range.tif'): # SRx0 and SRy0 from window_search_range.tif\n",
    "                            search_range = rio.open(gp+'window_search_range.tif')\n",
    "                            SRx0 = search_range.read(1); SRy0 = search_range.read(2)\n",
    "                        if os.path.exists(gp+'window_chip_size_min.tif'): # CSMINx0 and CSMINy0 from window_chip_size_min.tif\n",
    "                            chip_size_min = rio.open(gp+'window_chip_size_min.tif')\n",
    "                            CSMINx0 = chip_size_min.read(1); CSMINy0 = chip_size_min.read(2)\n",
    "                        if os.path.exists(gp+'window_chip_size_max.tif'): # CSMAXx0 and CSMAXy0 from window_chip_size_max.tif\n",
    "                            chip_size_max = rio.open(gp+'window_chip_size_max.tif')\n",
    "                            CSMAXx0 = chip_size_max.read(1); CSMAXy0 = chip_size_max.read(2)\n",
    "                        if os.path.exists(gp+'window_rdr_off2vel_x_vec.tif'): # offset2vx from window_rdr_off2vel_x_vec.tif\n",
    "                            offset2vx = gp+'window_rdr_off2vel_x_vec.tif' # path to be read in with GDAL\n",
    "                        if os.path.exists(gp+'window_rdr_off2vel_y_vec.tif'): # offset2vy from window_rdr_off2vel_y_vec.tif\n",
    "                            offset2vy = gp+'window_rdr_off2vel_y_vec.tif' \n",
    "                        if os.path.exists(gp+'window_stable_surface_mask.tif'): # noDataMask from window_stable_surface_mask.tif\n",
    "                            stable_surface_mask = rio.open(gp+'window_stable_surface_mask.tif')\n",
    "                            noDataMask = stable_surface_mask.read(1)\n",
    "\n",
    "                        # other parameters\n",
    "                        nodata = NO_DATA_VAL # use same as in previous steps\n",
    "                        geogrid_run_info=None\n",
    "                        print('AutoRIFT parameters loaded.')\n",
    "\n",
    "                        ##################### RUN AUTORIFT ##############################\n",
    "                        # run autoRIFT with function\n",
    "                        run_autoRIFT_inhouse(out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                                                 FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATE, \n",
    "                                                 OVERSAMPLE_RATIO, MINCHIPSIZE,\n",
    "                                                 Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                                                 noDataMask, nodata, geogrid_run_info)\n",
    "\n",
    "                idx1 = idx2 # set the second image index as the new first image index\n",
    "                if idx2 >= len(files_df)-1:\n",
    "                    print('Finished searching.')\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newautoriftenv",
   "language": "python",
   "name": "newautoriftenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
